{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c37aafab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from model.wan.pipeline_wan_ttv import AutoencoderKLWan, WanPipeline\n",
    "from diffusers.utils import export_to_video\n",
    "from model.wan.wan_transformer_for_video import WanTransformer3DModel\n",
    "\n",
    "\n",
    "# Available models: Wan-AI/Wan2.1-T2V-14B-Diffusers, Wan-AI/Wan2.1-T2V-1.3B-Diffusers\n",
    "model_id = './assets/Wan2.1-T2V-1.3B-Diffusers'\n",
    "load_dtype = torch.float32\n",
    "transformer = WanTransformer3DModel.from_pretrained(\n",
    "        model_id, subfolder=\"transformer\", \n",
    "        torch_dtype=load_dtype, \n",
    "        local_files_only=True,\n",
    "        # low_cpu_mem_usage=False, \n",
    "        use_safetensors=True,\n",
    "        ignore_mismatched_sizes=True,      # Setting for model structure changes\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24e6617b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 5/5 [00:01<00:00,  3.48it/s]\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  8.21it/s]s/it]\n",
      "Loading pipeline components...: 100%|██████████| 5/5 [00:02<00:00,  2.30it/s]\n",
      "100%|██████████| 50/50 [02:30<00:00,  3.01s/it]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'output.mp4'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from diffusers.utils import export_to_video\n",
    "from diffusers import AutoencoderKLWan, WanPipeline\n",
    "from diffusers.schedulers.scheduling_unipc_multistep import UniPCMultistepScheduler\n",
    "\n",
    "# Available models: Wan-AI/Wan2.1-T2V-14B-Diffusers, Wan-AI/Wan2.1-T2V-1.3B-Diffusers\n",
    "model_id = \"./assets/Wan2.1-T2V-1.3B-Diffusers\"\n",
    "vae = AutoencoderKLWan.from_pretrained(model_id, subfolder=\"vae\", torch_dtype=torch.float16)\n",
    "flow_shift = 5.0 # 5.0 for 720P, 3.0 for 480P\n",
    "scheduler = UniPCMultistepScheduler(prediction_type='flow_prediction', use_flow_sigmas=True, num_train_timesteps=1000, flow_shift=flow_shift)\n",
    "pipe = WanPipeline.from_pretrained(model_id, vae=vae, torch_dtype=torch.float16)\n",
    "pipe.scheduler = scheduler\n",
    "pipe.to(\"cuda\")\n",
    "\n",
    "prompt = \"A cat and a dog baking a cake together in a kitchen. The cat is carefully measuring flour, while the dog is stirring the batter with a wooden spoon. The kitchen is cozy, with sunlight streaming through the window.\"\n",
    "negative_prompt = \"Bright tones, overexposed, static, blurred details, subtitles, style, works, paintings, images, static, overall gray, worst quality, low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn faces, deformed, disfigured, misshapen limbs, fused fingers, still picture, messy background, three legs, many people in the background, walking backwards\"\n",
    "\n",
    "output = pipe(\n",
    "     prompt=prompt,\n",
    "     negative_prompt=negative_prompt,\n",
    "     height=480,\n",
    "     width=832,\n",
    "     num_frames=81,\n",
    "     guidance_scale=5.0,\n",
    "    ).frames[0]\n",
    "export_to_video(output, \"output.mp4\", fps=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce27c133",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from diffusers import AutoModel\n",
    "from diffusers.training_utils import cast_training_params, free_memory\n",
    "import torch\n",
    "from torch.cuda.amp import autocast\n",
    "\n",
    "from diffusers.utils.torch_utils import randn_tensor\n",
    "from diffusers.video_processor import VideoProcessor\n",
    "from omegaconf import OmegaConf\n",
    "from model.wan.wan_transformer_for_video import WanTransformer3DModel\n",
    "from model.wan.pipeline_wan_ttv import FlowMatchEulerDiscreteScheduler, AutoencoderKLWan, WanPipeline\n",
    "from diffusers.schedulers.scheduling_unipc_multistep import UniPCMultistepScheduler\n",
    "from utils.text_encoding import get_t5_prompt_embeds, encode_prompt\n",
    "from diffusers.utils import check_min_version, convert_unet_state_dict_to_peft, export_to_video, is_wandb_available\n",
    "\n",
    "\n",
    "\n",
    "args = OmegaConf.load(\"/home/chengxin/chengxin/vagen/config/ttv_wan.yaml\")\n",
    "infer_dtype = torch.float16\n",
    "load_dtype = torch.float32\n",
    "device = \"cuda:1\"\n",
    "\n",
    "vae = AutoencoderKLWan.from_pretrained(\n",
    "    args.pretrained_model_name_or_path, subfolder=\"vae\", \n",
    "    torch_dtype=load_dtype\n",
    ").to(device)\n",
    "tokenizer = AutoModel.from_pretrained(\n",
    "    args.pretrained_model_name_or_path, subfolder=\"tokenizer\", \n",
    "    local_files_only=True,\n",
    "    use_safetensors=True,\n",
    ")\n",
    "transformer = WanTransformer3DModel.from_pretrained(\n",
    "    args.pretrained_model_name_or_path, subfolder=\"transformer\", \n",
    "    torch_dtype=load_dtype, \n",
    "    local_files_only=True,\n",
    "    low_cpu_mem_usage=False, \n",
    "    use_safetensors=True,\n",
    "    ignore_mismatched_sizes=True,      # Setting for model structure changes\n",
    ").to(device)\n",
    "text_encoder = AutoModel.from_pretrained(\n",
    "    args.pretrained_model_name_or_path, subfolder=\"text_encoder\", \n",
    "    torch_dtype=load_dtype, \n",
    "    local_files_only=True,\n",
    "    use_safetensors=True,\n",
    ").to(device)\n",
    "\n",
    "scheduler = UniPCMultistepScheduler(prediction_type='flow_prediction', use_flow_sigmas=True, num_train_timesteps=1000, flow_shift=args.validation.flow_shift)\n",
    "video_processor = VideoProcessor(vae_scale_factor=vae.config.scale_factor_spatial)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "config = args.validation\n",
    "with torch.no_grad():\n",
    "    print(\"Start log_validation\")\n",
    "    prompt_list = config.prompt.split(config.prompt_separator)\n",
    "    negative_prompt = config.negetive_prompt\n",
    "    output_dir = getattr(config, \"save_dir\", 5.0)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    for prompt_idx, prompt in enumerate(prompt_list):\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        free_memory()\n",
    "\n",
    "        prompt_embeds, negative_prompt_embeds = None, None\n",
    "        if config.negetive_prompt_embed is not None:\n",
    "            negative_prompt_embeds = torch.load(config.negetive_prompt_embed).to(device).unsqueeze(0)\n",
    "            \n",
    "        prompt_embeds, negative_prompt_embeds = encode_prompt(\n",
    "            prompt=prompt,\n",
    "            negative_prompt=negative_prompt,\n",
    "            tokenizer = tokenizer,\n",
    "            text_encoder = text_encoder,\n",
    "            do_classifier_free_guidance=config.guidance_scale > 1.0,\n",
    "            num_videos_per_prompt=config.num_videos_per_prompt,\n",
    "            prompt_embeds=prompt_embeds,\n",
    "            negative_prompt_embeds=negative_prompt_embeds,\n",
    "            max_sequence_length=512,\n",
    "            device=device,\n",
    "            dtype = load_dtype,\n",
    "        ) # type: ignore\n",
    "\n",
    "\n",
    "        with autocast(dtype=infer_dtype):\n",
    "            num_latent_frames = (config.num_frames - 1) // vae.config.scale_factor_temporal + 1\n",
    "            num_channels = transformer.module.config.in_channels if hasattr(transformer, \"module\") else transformer.config.in_channels\n",
    "            shape = (\n",
    "                    config.num_videos_per_prompt,\n",
    "                    num_channels,\n",
    "                    num_latent_frames,\n",
    "                    config.height // vae.config.scale_factor_spatial,\n",
    "                    config.width // vae.config.scale_factor_spatial,\n",
    "            )\n",
    "            latents = randn_tensor(shape, device=device, dtype=infer_dtype)\n",
    "            scheduler.set_timesteps(config.num_inference_steps, device=device)\n",
    "            timesteps = scheduler.timesteps\n",
    "\n",
    "            for i, t in tqdm(enumerate(timesteps)):\n",
    "                current_model = transformer.module if hasattr(transformer, \"module\") else transformer\n",
    "                current_guidance_scale = config.guidance_scale\n",
    "                latent_model_input = latents\n",
    "                timestep = t.expand(latents.shape[0])\n",
    "\n",
    "                with current_model.cache_context(\"cond\"):\n",
    "                    noise_pred = current_model(\n",
    "                        hidden_states=latent_model_input,\n",
    "                        timestep=timestep,\n",
    "                        encoder_hidden_states=prompt_embeds,\n",
    "                        attention_kwargs=None,\n",
    "                        return_dict=False,\n",
    "                    )[0]\n",
    "\n",
    "                if config.guidance_scale > 1.0:\n",
    "                    with current_model.cache_context(\"uncond\"):\n",
    "                        noise_uncond = current_model(\n",
    "                            hidden_states=latent_model_input,\n",
    "                            timestep=timestep,\n",
    "                            encoder_hidden_states=negative_prompt_embeds,\n",
    "                            attention_kwargs=None,\n",
    "                            return_dict=False,\n",
    "                        )[0]\n",
    "                    noise_pred = noise_uncond + current_guidance_scale * (noise_pred - noise_uncond)\n",
    "                latents = scheduler.step(noise_pred, t, latents, return_dict=False)[0]\n",
    "                                    \n",
    "\n",
    "            latents = latents.to(vae.dtype)\n",
    "            latents_mean = torch.tensor(vae.config.latents_mean).view(1, vae.config.z_dim, 1, 1, 1).to(vae.device, vae.dtype)\n",
    "            latents_std = 1 / torch.tensor(vae.config.latents_std).view(1, vae.config.z_dim, 1, 1, 1).to(vae.device, vae.dtype)\n",
    "            latents = latents / latents_std + latents_mean\n",
    "            video = vae.decode(latents, return_dict=False)[0]\n",
    "            video = video_processor.postprocess_video(video, output_type='np')\n",
    "            for i in range(config.num_videos_per_prompt):\n",
    "                export_to_video(video[i], f\"{output_dir}/output{prompt_idx}_{i}.mp4\", fps=config.fps if hasattr(config, \"fps\") else 16)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97e67628",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data-87-04/chengxin/envs/veo3/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  9.87it/s]\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from diffusers import AutoModel\n",
    "from diffusers.training_utils import cast_training_params, free_memory\n",
    "import torch\n",
    "from torch.cuda.amp import autocast\n",
    "\n",
    "from diffusers.utils.torch_utils import randn_tensor\n",
    "from diffusers.video_processor import VideoProcessor\n",
    "from omegaconf import OmegaConf\n",
    "from model.wan.wan_transformer_for_video import WanTransformer3DModel\n",
    "from model.wan.pipeline_wan_ttv import FlowMatchEulerDiscreteScheduler, AutoencoderKLWan, WanPipeline\n",
    "from diffusers.schedulers.scheduling_unipc_multistep import UniPCMultistepScheduler\n",
    "from utils.text_encoding import get_t5_prompt_embeds, encode_prompt\n",
    "from diffusers.utils import check_min_version, convert_unet_state_dict_to_peft, export_to_video, is_wandb_available\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "\n",
    "\n",
    "args = OmegaConf.load(\"/home/chengxin/chengxin/vagen/config/tta_tuning.yaml\")\n",
    "infer_dtype = torch.float16\n",
    "load_dtype = torch.float32\n",
    "device = \"cpu\"\n",
    "\n",
    "\n",
    "transformer = WanTransformer3DModel.from_pretrained(\n",
    "    args.pretrained_model_name_or_path, subfolder=\"transformer\", \n",
    "    torch_dtype=load_dtype, \n",
    "    local_files_only=True,\n",
    "    low_cpu_mem_usage=False, \n",
    "    use_safetensors=True,\n",
    "    ignore_mismatched_sizes=True,      # Setting for model structure changes\n",
    ").to(device)\n",
    "\n",
    "\n",
    "\n",
    "def set_requires_grad(transformer, target_params, print_param = False):\n",
    "    for name, param in transformer.named_parameters():\n",
    "        for target in target_params:\n",
    "            if target in name:\n",
    "                param.requires_grad = True  # 设置为需要梯度\n",
    "                if print_param:\n",
    "                    print(f\"{target}\", end = \" \")\n",
    "    print(\"\\n\")\n",
    "    return transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "349b6774",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data-87-04/chengxin/envs/veo3/lib/python3.10/site-packages/peft/mapping_func.py:73: UserWarning: You are trying to modify a model with PEFT for a second time. If you want to reload the model with a different config, make sure to call `.unload()` before.\n",
      "  warnings.warn(\n",
      "/data-87-04/chengxin/envs/veo3/lib/python3.10/site-packages/peft/tuners/tuners_utils.py:196: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scale_shift_table patch_embedding patch_embedding text_embedder text_embedder text_embedder text_embedder scale_shift_table scale_shift_table scale_shift_table scale_shift_table scale_shift_table scale_shift_table scale_shift_table scale_shift_table scale_shift_table scale_shift_table scale_shift_table scale_shift_table scale_shift_table scale_shift_table scale_shift_table scale_shift_table scale_shift_table scale_shift_table scale_shift_table scale_shift_table scale_shift_table scale_shift_table scale_shift_table scale_shift_table scale_shift_table scale_shift_table scale_shift_table scale_shift_table scale_shift_table scale_shift_table proj_out proj_out \n",
      "\n",
      "trainable params: 160,519,744 || all params: 1,570,384,960 || trainable%: 10.2217\n"
     ]
    }
   ],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=args.rank,  # LoRA 的秩（rank），通常设为 4、8、16 或 32\n",
    "    lora_alpha=args.lora_alpha,  # 缩放因子，控制 LoRA 更新的幅度\n",
    "    target_modules=[\"to_q\", \n",
    "                    \"to_k\", \n",
    "                    \"to_v\", \n",
    "                    \"ffn.net.0.proj\", \n",
    "                    \"ffn.net.2\"],  # 指定应用 LoRA 的模块（如 Transformer 的查询和值投影）\n",
    "    lora_dropout=0.1,  # Dropout 概率\n",
    "    bias=\"none\",  # 是否调整偏置\n",
    "    # task_type=\"CAUSAL_LM\"  # 任务类型，如 CAUSAL_LM 或 SEQ_CLS\n",
    ")\n",
    "\n",
    "\n",
    "transformer = get_peft_model(transformer, lora_config)\n",
    "transformer = set_requires_grad(transformer, ['patch_embedding', 'proj_out', 'scale_shift_table', 'text_embedder'], True)\n",
    "transformer.print_trainable_parameters()  # 查看可训练参数量\n",
    "\n",
    "\n",
    "# print(\"=== 可训练参数 ===\")\n",
    "# trainable_params = 0\n",
    "# total_params = 0\n",
    "# for name, param in transformer.named_parameters():\n",
    "#     if param.requires_grad:\n",
    "#         print(f\"Layer: {name}, Shape: {param.shape}\")\n",
    "#         trainable_params += param.numel()\n",
    "#     total_params += param.numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdfc376e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "324d32e9",
   "metadata": {},
   "source": [
    "# Stable Diffusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d84cd70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import AutoModel\n",
    "from diffusers.pipelines.stable_audio.modeling_stable_audio import StableAudioProjectionModel\n",
    "from model.stable_audio.stable_audio_transformer import StableAudioDiTModel\n",
    "import torch\n",
    "from diffusers.utils.torch_utils import randn_tensor\n",
    "from diffusers.models.embeddings import get_1d_rotary_pos_embed\n",
    "import inspect\n",
    "from tqdm import tqdm\n",
    "import soundfile as sf\n",
    "\n",
    "\n",
    "load_dtype = torch.float16\n",
    "device = \"cuda:0\"\n",
    "transformer = StableAudioDiTModel.from_pretrained(\n",
    "    '/home/chengxin/chengxin/vagen/assets/stable-audio-open-1.0', \n",
    "    subfolder=\"transformer\", \n",
    "    torch_dtype = load_dtype,\n",
    "    local_files_only=True,                  # From pretrained\n",
    "    low_cpu_mem_usage=False, \n",
    "    ignore_mismatched_sizes=True,      \n",
    "    # num_layers=10,                                 \n",
    "    use_safetensors=True,                          \n",
    ").to(device)\n",
    "\n",
    "\n",
    "projection_model = StableAudioProjectionModel.from_pretrained(\n",
    "    '/home/chengxin/chengxin/vagen/assets/stable-audio-open-1.0', \n",
    "    subfolder=\"projection_model\",\n",
    "    torch_dtype=load_dtype, \n",
    "    local_files_only=True,\n",
    "    use_safetensors=True,\n",
    ").to(device)\n",
    "\n",
    "vae = AutoModel.from_pretrained(\n",
    "    '/home/chengxin/chengxin/vagen/assets/stable-audio-open-1.0', \n",
    "    subfolder=\"vae\", \n",
    "    torch_dtype = load_dtype,\n",
    "    local_files_only=True,\n",
    "    use_safetensors=True,                      \n",
    ").to(device)\n",
    "\n",
    "\n",
    "text_encoder = AutoModel.from_pretrained(\n",
    "    '/home/chengxin/chengxin/vagen/assets/stable-audio-open-1.0', \n",
    "    subfolder=\"text_encoder\", \n",
    "    torch_dtype=load_dtype, \n",
    "    local_files_only=True,\n",
    "    use_safetensors=True,\n",
    ").to(device)\n",
    "\n",
    "tokenizer = AutoModel.from_pretrained(\n",
    "    '/home/chengxin/chengxin/vagen/assets/stable-audio-open-1.0', \n",
    "    subfolder=\"tokenizer\", \n",
    "    local_files_only=True,\n",
    "    use_safetensors=True,\n",
    ")\n",
    "\n",
    "scheduler = AutoModel.from_pretrained(\n",
    "    '/home/chengxin/chengxin/vagen/assets/stable-audio-open-1.0', \n",
    "    subfolder=\"scheduler\", \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb696101",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "200it [00:06, 28.58it/s]\n"
     ]
    }
   ],
   "source": [
    "from utils.text_encoding import encode_prompt_sd, encode_duration_sd, prepare_extra_step_kwargs\n",
    "\n",
    "\n",
    "prompt = [\"The sharp, resonant sound of a bowling ball striking the pins, followed by the clattering and scattering of the pins in a chaotic dance, fills the air with a mix of impact and rolling echoes.\"]\n",
    "negative_prompt = [\"Low quality.\"]\n",
    "batch_size = len(prompt)\n",
    "\n",
    "do_classifier_free_guidance = True\n",
    "num_waveforms_per_prompt = 1\n",
    "num_inference_steps = 200\n",
    "eta = 0.0\n",
    "guidance_scale = 7\n",
    "negative_prompt_embeds = None\n",
    "\n",
    "audio_start_in_s = 0.0\n",
    "audio_end_in_s   = 5.0\n",
    "\n",
    "with torch.no_grad():\n",
    "    prompt_embeds = encode_prompt_sd(\n",
    "        prompt,\n",
    "        tokenizer,\n",
    "        text_encoder,\n",
    "        projection_model,\n",
    "        device,\n",
    "        do_classifier_free_guidance,\n",
    "        negative_prompt,\n",
    "    )\n",
    "\n",
    "    # Encode duration\n",
    "    seconds_start_hidden_states, seconds_end_hidden_states = encode_duration_sd(\n",
    "        projection_model,\n",
    "        audio_start_in_s,\n",
    "        audio_end_in_s,\n",
    "        device,\n",
    "        do_classifier_free_guidance and (negative_prompt is not None or negative_prompt_embeds is not None),\n",
    "        batch_size,\n",
    "    )\n",
    "\n",
    "    # Create text_audio_duration_embeds and audio_duration_embeds\n",
    "    text_audio_duration_embeds = torch.cat([prompt_embeds, seconds_start_hidden_states, seconds_end_hidden_states], dim=1)\n",
    "    audio_duration_embeds = torch.cat([seconds_start_hidden_states, seconds_end_hidden_states], dim=2)\n",
    "\n",
    "    # In case of classifier free guidance without negative prompt, we need to create unconditional embeddings and\n",
    "    if do_classifier_free_guidance and negative_prompt_embeds is None and negative_prompt is None:\n",
    "        negative_text_audio_duration_embeds = torch.zeros_like(text_audio_duration_embeds, device=text_audio_duration_embeds.device)\n",
    "        text_audio_duration_embeds = torch.cat([negative_text_audio_duration_embeds, text_audio_duration_embeds], dim=0)\n",
    "        audio_duration_embeds = torch.cat([audio_duration_embeds, audio_duration_embeds], dim=0)\n",
    "\n",
    "    bs_embed, seq_len, hidden_size = text_audio_duration_embeds.shape\n",
    "    # duplicate audio_duration_embeds and text_audio_duration_embeds for each generation per prompt, using mps friendly method\n",
    "    text_audio_duration_embeds = text_audio_duration_embeds.repeat(1, num_waveforms_per_prompt, 1)\n",
    "    text_audio_duration_embeds = text_audio_duration_embeds.view(bs_embed * num_waveforms_per_prompt, seq_len, hidden_size)\n",
    "\n",
    "    # # print(audio_duration_embeds.shape)\n",
    "    audio_duration_embeds = audio_duration_embeds.repeat(1, num_waveforms_per_prompt, 1)\n",
    "    audio_duration_embeds = audio_duration_embeds.view(bs_embed * num_waveforms_per_prompt, -1, audio_duration_embeds.shape[-1])\n",
    "    # # print(audio_duration_embeds.shape)\n",
    "\n",
    "    # 4. Prepare timesteps\n",
    "    scheduler.set_timesteps(num_inference_steps, device=device)\n",
    "    timesteps = scheduler.timesteps\n",
    "\n",
    "    # 5. Prepare latent variables\n",
    "    num_channels_vae = transformer.config.in_channels\n",
    "    waveform_length = int(transformer.config.sample_size)\n",
    "    # waveform_length = int(audio_end_in_s * 22.5)\n",
    "    shape = (batch_size * num_waveforms_per_prompt, num_channels_vae, waveform_length)\n",
    "    generator = torch.Generator(\"cuda\").manual_seed(0)\n",
    "    # generator = None\n",
    "    latents = randn_tensor(shape, generator=generator, device=device, dtype=load_dtype)\n",
    "\n",
    "    # 6. Prepare extra step kwargs and rotary_embed_dim\n",
    "    extra_step_kwargs = prepare_extra_step_kwargs(generator, eta, scheduler)\n",
    "    rotary_embed_dim = transformer.config.attention_head_dim // 2\n",
    "    rotary_embedding = get_1d_rotary_pos_embed(\n",
    "        rotary_embed_dim,\n",
    "        latents.shape[2] + audio_duration_embeds.shape[1],\n",
    "        use_real=True,\n",
    "        repeat_interleave_real=False,\n",
    "    )\n",
    "\n",
    "    # 8. Denoising loop\n",
    "    for i, t in tqdm(enumerate(timesteps)):\n",
    "        # expand the latents if we are doing classifier free guidance\n",
    "        latent_model_input = torch.cat([latents] * 2) if do_classifier_free_guidance else latents\n",
    "        latent_model_input = scheduler.scale_model_input(latent_model_input, t)\n",
    "        t = torch.stack([t for _ in range(latent_model_input.shape[0])])\n",
    "\n",
    "        # predict the noise residual\n",
    "        noise_pred = transformer(\n",
    "            latent_model_input,\n",
    "            t,\n",
    "            encoder_hidden_states=text_audio_duration_embeds,\n",
    "            global_hidden_states=audio_duration_embeds,\n",
    "            rotary_embedding=rotary_embedding,\n",
    "            return_dict=False,\n",
    "        )[0]\n",
    "\n",
    "        # perform guidance\n",
    "        if do_classifier_free_guidance:\n",
    "            noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
    "            noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
    "        latents = scheduler.step(noise_pred, t, latents, **extra_step_kwargs).prev_sample\n",
    "\n",
    "    audio = vae.decode(latents).sample\n",
    "\n",
    "import torchaudio\n",
    "\n",
    "for i in range(len(audio)):\n",
    "    torchaudio.save(f'test{i}.wav', audio[i].to(torch.float32).cpu(), 44100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68454c7a",
   "metadata": {},
   "source": [
    "# Stable Diffusion FT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05ff6b0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data-87-04/chengxin/envs/veo3/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/data-87-04/chengxin/envs/veo3/lib/python3.10/site-packages/torch/nn/utils/weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    }
   ],
   "source": [
    "from diffusers import AutoModel\n",
    "from diffusers.pipelines.stable_audio.modeling_stable_audio import StableAudioProjectionModel\n",
    "from model.stable_audio.stable_audio_transformer import StableAudioDiTModel\n",
    "import torch\n",
    "from diffusers.utils.torch_utils import randn_tensor\n",
    "from diffusers.models.embeddings import get_1d_rotary_pos_embed\n",
    "import inspect\n",
    "from tqdm import tqdm\n",
    "import soundfile as sf\n",
    "from diffusers.schedulers.scheduling_unipc_multistep import UniPCMultistepScheduler\n",
    "\n",
    "\n",
    "load_dtype = torch.float16\n",
    "device = \"cuda:0\"\n",
    "transformer = StableAudioDiTModel.from_pretrained(\n",
    "    '/home/chengxin/chengxin/vagen/assets/stable-audio-open-1.0', \n",
    "    subfolder=\"transformer_ft\", \n",
    "    torch_dtype = load_dtype,\n",
    "    local_files_only=True,                  # From pretrained\n",
    "    low_cpu_mem_usage=False, \n",
    "    ignore_mismatched_sizes=True,      \n",
    "    # num_layers=10,                                 \n",
    "    use_safetensors=True,                          \n",
    ").to(device)\n",
    "\n",
    "# sd_tta_pt_16 ckpt1\n",
    "projection_model = StableAudioProjectionModel.from_pretrained(\n",
    "    '/home/chengxin/chengxin/vagen/assets/stable-audio-open-1.0', \n",
    "    subfolder=\"projection_model\",\n",
    "    torch_dtype=load_dtype, \n",
    "    local_files_only=True,\n",
    "    use_safetensors=True,\n",
    ").to(device)\n",
    "\n",
    "vae = AutoModel.from_pretrained(\n",
    "    '/home/chengxin/chengxin/vagen/assets/stable-audio-open-1.0', \n",
    "    subfolder=\"vae\", \n",
    "    torch_dtype = load_dtype,\n",
    "    local_files_only=True,\n",
    "    use_safetensors=True,                      \n",
    ").to(device)\n",
    "\n",
    "\n",
    "text_encoder = AutoModel.from_pretrained(\n",
    "    '/home/chengxin/chengxin/vagen/assets/stable-audio-open-1.0', \n",
    "    subfolder=\"text_encoder\", \n",
    "    torch_dtype=load_dtype, \n",
    "    local_files_only=True,\n",
    "    use_safetensors=True,\n",
    ").to(device)\n",
    "\n",
    "tokenizer = AutoModel.from_pretrained(\n",
    "    '/home/chengxin/chengxin/vagen/assets/stable-audio-open-1.0', \n",
    "    subfolder=\"tokenizer\", \n",
    "    local_files_only=True,\n",
    "    use_safetensors=True,\n",
    ")\n",
    "\n",
    "step_scheduler = UniPCMultistepScheduler(prediction_type='flow_prediction', use_flow_sigmas=True, num_train_timesteps=1000) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "46c45cf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "200it [00:55,  3.58it/s]\n"
     ]
    }
   ],
   "source": [
    "from utils.text_encoding import encode_prompt_sd, encode_duration_sd, prepare_extra_step_kwargs\n",
    "import json\n",
    "import torchaudio\n",
    "\n",
    "\n",
    "do_classifier_free_guidance = True\n",
    "num_waveforms_per_prompt = 1\n",
    "num_inference_steps = 200\n",
    "eta = 0.0\n",
    "guidance_scale = 7\n",
    "negative_prompt_embeds = None\n",
    "audio_start_in_s = 0.0\n",
    "audio_end_in_s   = 10.0\n",
    "\n",
    "\n",
    "with open('/home/chengxin/chengxin/vagen/data/tta/test_avsync_recap.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "for path, info in data.items():\n",
    "    prompt = [info['label']] \n",
    "    # Lions Roaring Deeply, Bowling striking\n",
    "    prompt = [ \"Sharpen knife\", \"chicken crowing\", \"Bowling rolling and striking\", \"hammering\", \"Lions Roaring Deeply\", 'Frog Croaking'] # ['Lions Roaring Deeply', 'Frog Croaking', 'Dog barking', 'Bowling striking']\n",
    "            #  [ \"Sharpen knife\", \"chicken crowing\", \"toilet flushing\", \"hammering\"]\n",
    "    negative_prompt = [\"\" for _ in range(len(prompt))]\n",
    "    batch_size = len(prompt)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        prompt_embeds = encode_prompt_sd(\n",
    "            prompt,\n",
    "            tokenizer,\n",
    "            text_encoder,\n",
    "            projection_model,\n",
    "            device,\n",
    "            do_classifier_free_guidance,\n",
    "            negative_prompt,\n",
    "        )\n",
    "\n",
    "        # 5. Prepare latent variables\n",
    "        num_channels_vae = transformer.config.in_channels\n",
    "        # waveform_length = int(transformer.config.sample_size)\n",
    "        waveform_length = int(audio_end_in_s * 22.5)\n",
    "        shape = (batch_size * num_waveforms_per_prompt, num_channels_vae, waveform_length)\n",
    "        # generator = torch.Generator(\"cuda\").manual_seed(0)\n",
    "        latents = randn_tensor(shape, device=device, dtype=load_dtype)\n",
    "\n",
    "        # 6. Prepare extra step kwargs and rotary_embed_dim\n",
    "        rotary_embed_dim = transformer.config.attention_head_dim // 2\n",
    "        rotary_embedding = get_1d_rotary_pos_embed(\n",
    "            rotary_embed_dim,\n",
    "            latents.shape[2] + 1,\n",
    "            use_real=True,\n",
    "            repeat_interleave_real=False,\n",
    "        )\n",
    "\n",
    "        # 8. Denoising loop\n",
    "        # TODO: Check scheduler, t = 1 for noise;  UniPCMultistepScheduler timesteps [1-1000]\n",
    "        step_scheduler.set_timesteps(num_inference_steps, device=device)\n",
    "        timesteps = step_scheduler.timesteps\n",
    "        for i, t in tqdm(enumerate(timesteps)):\n",
    "            latent_model_input = torch.cat([latents] * 2) if do_classifier_free_guidance else latents\n",
    "            latent_model_input = step_scheduler.scale_model_input(latent_model_input, t)\n",
    "            t = torch.stack([t for _ in range(latent_model_input.shape[0])])\n",
    "\n",
    "            # predict the noise residual\n",
    "            noise_pred = transformer(\n",
    "                latent_model_input,\n",
    "                t,\n",
    "                encoder_hidden_states=prompt_embeds, # text_audio_duration_embeds,\n",
    "                # global_hidden_states=audio_duration_embeds,\n",
    "                rotary_embedding=rotary_embedding,\n",
    "                return_dict=False,\n",
    "            )[0]\n",
    "\n",
    "            # perform guidance\n",
    "            if do_classifier_free_guidance:\n",
    "                t = t[:len(t)//2]\n",
    "                noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
    "                noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
    "            latents = step_scheduler.step(noise_pred, t[0], latents).prev_sample\n",
    "\n",
    "        audio = vae.decode(latents).sample\n",
    "\n",
    "\n",
    "    for i in range(len(audio)):\n",
    "        # torchaudio.save(f\"/home/chengxin/chengxin/vagen/log/predict/{path.split('/')[-1][:-4]}.wav\", audio[i].to(torch.float32).cpu(), 44100)\n",
    "        torchaudio.save(f\"./test{i}.wav\", audio[i].to(torch.float32).cpu(), 44100)\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25eb4b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, shutil\n",
    "import torchaudio\n",
    "\n",
    "i = 0\n",
    "gen_path = '/home/chengxin/chengxin/vagen/log/sd_tta_pt_16/logging/10000'    # /home/chengxin/chengxin/vagen/log/sd_tta_pt_16/checkpoints/checkpoint_1\n",
    "gen_path = '/home/chengxin/chengxin/vagen/log/sd_tta_ft_16/logging/6500'     # /home/chengxin/chengxin/vagen/log/sd_tta_ft_16/checkpoints/checkpoint_13   6500 7000 7500 \n",
    "# gen_path = '/home/chengxin/chengxin/vagen/log_backup/sd_tta_ft_recap_16/logging/5500'     # /home/chengxin/chengxin/vagen/log/sd_tta_ft_recap_16/checkpoints/   5500 4500   7000 8000\n",
    "# gen_path = '/home/chengxin/chengxin/vagen/log/wan_sd_ttva_55_bi_new/logging/1500'     \n",
    "\n",
    "target_path = '/home/chengxin/chengxin/Dataset_Sound/VGGSound/generated_audios/veo3/avsync'\n",
    "\n",
    "\n",
    "for file in os.listdir(target_path):\n",
    "    try:\n",
    "\n",
    "        input_file = f'{gen_path}/{file}'\n",
    "        output_file = f'{target_path}/{file}'\n",
    "        duration = 5.4  # 截取时长（秒）\n",
    "\n",
    "        waveform, sample_rate = torchaudio.load(input_file)\n",
    "        num_samples = int(duration * sample_rate)\n",
    "        waveform = waveform[:, :num_samples]\n",
    "        # waveform[:, num_samples:] = 0\n",
    "\n",
    "        torchaudio.save(output_file, waveform, sample_rate)\n",
    "        # os.remove(input_file)\n",
    "        i += 1\n",
    "\n",
    "    except:\n",
    "        pass\n",
    "        # print(file)\n",
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc14bdbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.va_processing import add_audio_to_video\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "input_dir = '/home/chengxin/chengxin/vagen/log/wan_sd_ttva_55_bi_new/logging/0'\n",
    "output_dir = '/home/chengxin/chengxin/Dataset_Sound/VGGSound/generated_audios/veo3/avsync_video'\n",
    "for path in tqdm(glob(f'{input_dir}/*.mp4')):\n",
    "    v_path =  f\"{input_dir}/{path.split('/')[-1][:-4]}.mp4\"\n",
    "    a_path =  f\"{input_dir}/{path.split('/')[-1][:-4]}.wav\"\n",
    "    o_path = f\"{output_dir}/{path.split('/')[-1][:-4]}.mp4\"\n",
    "    if os.path.exists(a_path):\n",
    "        add_audio_to_video(video_path = v_path, audio_path = a_path, output_path = o_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a842661a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chengxin/anaconda3/envs/vagen/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded fusion checkpoint from /home/chengxin/chengxin/Ovi/ckpts/Ovi/model.safetensors\n",
      "video_model.patch_embedding.weight\n",
      "video_model.patch_embedding.bias\n",
      "video_model.text_embedding.0.weight\n",
      "video_model.text_embedding.0.bias\n",
      "video_model.text_embedding.2.weight\n",
      "video_model.text_embedding.2.bias\n",
      "video_model.time_embedding.0.weight\n",
      "video_model.time_embedding.0.bias\n",
      "video_model.time_embedding.2.weight\n",
      "video_model.time_embedding.2.bias\n",
      "video_model.time_projection.1.weight\n",
      "video_model.time_projection.1.bias\n",
      "video_model.blocks.0.self_attn.q.weight\n",
      "video_model.blocks.0.self_attn.q.bias\n",
      "video_model.blocks.0.self_attn.k.weight\n",
      "video_model.blocks.0.self_attn.k.bias\n",
      "video_model.blocks.0.self_attn.v.weight\n",
      "video_model.blocks.0.self_attn.v.bias\n",
      "video_model.blocks.0.self_attn.o.weight\n",
      "video_model.blocks.0.self_attn.o.bias\n",
      "video_model.blocks.0.self_attn.norm_q.weight\n",
      "video_model.blocks.0.self_attn.norm_k.weight\n",
      "video_model.blocks.0.norm3.weight\n",
      "video_model.blocks.0.norm3.bias\n",
      "video_model.blocks.0.cross_attn.q.weight\n",
      "video_model.blocks.0.cross_attn.q.bias\n",
      "video_model.blocks.0.cross_attn.k.weight\n",
      "video_model.blocks.0.cross_attn.k.bias\n",
      "video_model.blocks.0.cross_attn.v.weight\n",
      "video_model.blocks.0.cross_attn.v.bias\n",
      "video_model.blocks.0.cross_attn.o.weight\n",
      "video_model.blocks.0.cross_attn.o.bias\n",
      "video_model.blocks.0.cross_attn.norm_q.weight\n",
      "video_model.blocks.0.cross_attn.norm_k.weight\n",
      "video_model.blocks.0.cross_attn.k_fusion.weight\n",
      "video_model.blocks.0.cross_attn.k_fusion.bias\n",
      "video_model.blocks.0.cross_attn.v_fusion.weight\n",
      "video_model.blocks.0.cross_attn.v_fusion.bias\n",
      "video_model.blocks.0.cross_attn.pre_attn_norm_fusion.weight\n",
      "video_model.blocks.0.cross_attn.pre_attn_norm_fusion.bias\n",
      "video_model.blocks.0.cross_attn.norm_k_fusion.weight\n",
      "video_model.blocks.0.ffn.0.weight\n",
      "video_model.blocks.0.ffn.0.bias\n",
      "video_model.blocks.0.ffn.2.weight\n",
      "video_model.blocks.0.ffn.2.bias\n",
      "video_model.blocks.0.modulation.modulation\n",
      "video_model.blocks.1.self_attn.q.weight\n",
      "video_model.blocks.1.self_attn.q.bias\n",
      "video_model.blocks.1.self_attn.k.weight\n",
      "video_model.blocks.1.self_attn.k.bias\n",
      "video_model.blocks.1.self_attn.v.weight\n",
      "video_model.blocks.1.self_attn.v.bias\n",
      "video_model.blocks.1.self_attn.o.weight\n",
      "video_model.blocks.1.self_attn.o.bias\n",
      "video_model.blocks.1.self_attn.norm_q.weight\n",
      "video_model.blocks.1.self_attn.norm_k.weight\n",
      "video_model.blocks.1.norm3.weight\n",
      "video_model.blocks.1.norm3.bias\n",
      "video_model.blocks.1.cross_attn.q.weight\n",
      "video_model.blocks.1.cross_attn.q.bias\n",
      "video_model.blocks.1.cross_attn.k.weight\n",
      "video_model.blocks.1.cross_attn.k.bias\n",
      "video_model.blocks.1.cross_attn.v.weight\n",
      "video_model.blocks.1.cross_attn.v.bias\n",
      "video_model.blocks.1.cross_attn.o.weight\n",
      "video_model.blocks.1.cross_attn.o.bias\n",
      "video_model.blocks.1.cross_attn.norm_q.weight\n",
      "video_model.blocks.1.cross_attn.norm_k.weight\n",
      "video_model.blocks.1.cross_attn.k_fusion.weight\n",
      "video_model.blocks.1.cross_attn.k_fusion.bias\n",
      "video_model.blocks.1.cross_attn.v_fusion.weight\n",
      "video_model.blocks.1.cross_attn.v_fusion.bias\n",
      "video_model.blocks.1.cross_attn.pre_attn_norm_fusion.weight\n",
      "video_model.blocks.1.cross_attn.pre_attn_norm_fusion.bias\n",
      "video_model.blocks.1.cross_attn.norm_k_fusion.weight\n",
      "video_model.blocks.1.ffn.0.weight\n",
      "video_model.blocks.1.ffn.0.bias\n",
      "video_model.blocks.1.ffn.2.weight\n",
      "video_model.blocks.1.ffn.2.bias\n",
      "video_model.blocks.1.modulation.modulation\n",
      "video_model.blocks.2.self_attn.q.weight\n",
      "video_model.blocks.2.self_attn.q.bias\n",
      "video_model.blocks.2.self_attn.k.weight\n",
      "video_model.blocks.2.self_attn.k.bias\n",
      "video_model.blocks.2.self_attn.v.weight\n",
      "video_model.blocks.2.self_attn.v.bias\n",
      "video_model.blocks.2.self_attn.o.weight\n",
      "video_model.blocks.2.self_attn.o.bias\n",
      "video_model.blocks.2.self_attn.norm_q.weight\n",
      "video_model.blocks.2.self_attn.norm_k.weight\n",
      "video_model.blocks.2.norm3.weight\n",
      "video_model.blocks.2.norm3.bias\n",
      "video_model.blocks.2.cross_attn.q.weight\n",
      "video_model.blocks.2.cross_attn.q.bias\n",
      "video_model.blocks.2.cross_attn.k.weight\n",
      "video_model.blocks.2.cross_attn.k.bias\n",
      "video_model.blocks.2.cross_attn.v.weight\n",
      "video_model.blocks.2.cross_attn.v.bias\n",
      "video_model.blocks.2.cross_attn.o.weight\n",
      "video_model.blocks.2.cross_attn.o.bias\n",
      "video_model.blocks.2.cross_attn.norm_q.weight\n",
      "video_model.blocks.2.cross_attn.norm_k.weight\n",
      "video_model.blocks.2.cross_attn.k_fusion.weight\n",
      "video_model.blocks.2.cross_attn.k_fusion.bias\n",
      "video_model.blocks.2.cross_attn.v_fusion.weight\n",
      "video_model.blocks.2.cross_attn.v_fusion.bias\n",
      "video_model.blocks.2.cross_attn.pre_attn_norm_fusion.weight\n",
      "video_model.blocks.2.cross_attn.pre_attn_norm_fusion.bias\n",
      "video_model.blocks.2.cross_attn.norm_k_fusion.weight\n",
      "video_model.blocks.2.ffn.0.weight\n",
      "video_model.blocks.2.ffn.0.bias\n",
      "video_model.blocks.2.ffn.2.weight\n",
      "video_model.blocks.2.ffn.2.bias\n",
      "video_model.blocks.2.modulation.modulation\n",
      "video_model.blocks.3.self_attn.q.weight\n",
      "video_model.blocks.3.self_attn.q.bias\n",
      "video_model.blocks.3.self_attn.k.weight\n",
      "video_model.blocks.3.self_attn.k.bias\n",
      "video_model.blocks.3.self_attn.v.weight\n",
      "video_model.blocks.3.self_attn.v.bias\n",
      "video_model.blocks.3.self_attn.o.weight\n",
      "video_model.blocks.3.self_attn.o.bias\n",
      "video_model.blocks.3.self_attn.norm_q.weight\n",
      "video_model.blocks.3.self_attn.norm_k.weight\n",
      "video_model.blocks.3.norm3.weight\n",
      "video_model.blocks.3.norm3.bias\n",
      "video_model.blocks.3.cross_attn.q.weight\n",
      "video_model.blocks.3.cross_attn.q.bias\n",
      "video_model.blocks.3.cross_attn.k.weight\n",
      "video_model.blocks.3.cross_attn.k.bias\n",
      "video_model.blocks.3.cross_attn.v.weight\n",
      "video_model.blocks.3.cross_attn.v.bias\n",
      "video_model.blocks.3.cross_attn.o.weight\n",
      "video_model.blocks.3.cross_attn.o.bias\n",
      "video_model.blocks.3.cross_attn.norm_q.weight\n",
      "video_model.blocks.3.cross_attn.norm_k.weight\n",
      "video_model.blocks.3.cross_attn.k_fusion.weight\n",
      "video_model.blocks.3.cross_attn.k_fusion.bias\n",
      "video_model.blocks.3.cross_attn.v_fusion.weight\n",
      "video_model.blocks.3.cross_attn.v_fusion.bias\n",
      "video_model.blocks.3.cross_attn.pre_attn_norm_fusion.weight\n",
      "video_model.blocks.3.cross_attn.pre_attn_norm_fusion.bias\n",
      "video_model.blocks.3.cross_attn.norm_k_fusion.weight\n",
      "video_model.blocks.3.ffn.0.weight\n",
      "video_model.blocks.3.ffn.0.bias\n",
      "video_model.blocks.3.ffn.2.weight\n",
      "video_model.blocks.3.ffn.2.bias\n",
      "video_model.blocks.3.modulation.modulation\n",
      "video_model.blocks.4.self_attn.q.weight\n",
      "video_model.blocks.4.self_attn.q.bias\n",
      "video_model.blocks.4.self_attn.k.weight\n",
      "video_model.blocks.4.self_attn.k.bias\n",
      "video_model.blocks.4.self_attn.v.weight\n",
      "video_model.blocks.4.self_attn.v.bias\n",
      "video_model.blocks.4.self_attn.o.weight\n",
      "video_model.blocks.4.self_attn.o.bias\n",
      "video_model.blocks.4.self_attn.norm_q.weight\n",
      "video_model.blocks.4.self_attn.norm_k.weight\n",
      "video_model.blocks.4.norm3.weight\n",
      "video_model.blocks.4.norm3.bias\n",
      "video_model.blocks.4.cross_attn.q.weight\n",
      "video_model.blocks.4.cross_attn.q.bias\n",
      "video_model.blocks.4.cross_attn.k.weight\n",
      "video_model.blocks.4.cross_attn.k.bias\n",
      "video_model.blocks.4.cross_attn.v.weight\n",
      "video_model.blocks.4.cross_attn.v.bias\n",
      "video_model.blocks.4.cross_attn.o.weight\n",
      "video_model.blocks.4.cross_attn.o.bias\n",
      "video_model.blocks.4.cross_attn.norm_q.weight\n",
      "video_model.blocks.4.cross_attn.norm_k.weight\n",
      "video_model.blocks.4.cross_attn.k_fusion.weight\n",
      "video_model.blocks.4.cross_attn.k_fusion.bias\n",
      "video_model.blocks.4.cross_attn.v_fusion.weight\n",
      "video_model.blocks.4.cross_attn.v_fusion.bias\n",
      "video_model.blocks.4.cross_attn.pre_attn_norm_fusion.weight\n",
      "video_model.blocks.4.cross_attn.pre_attn_norm_fusion.bias\n",
      "video_model.blocks.4.cross_attn.norm_k_fusion.weight\n",
      "video_model.blocks.4.ffn.0.weight\n",
      "video_model.blocks.4.ffn.0.bias\n",
      "video_model.blocks.4.ffn.2.weight\n",
      "video_model.blocks.4.ffn.2.bias\n",
      "video_model.blocks.4.modulation.modulation\n",
      "video_model.blocks.5.self_attn.q.weight\n",
      "video_model.blocks.5.self_attn.q.bias\n",
      "video_model.blocks.5.self_attn.k.weight\n",
      "video_model.blocks.5.self_attn.k.bias\n",
      "video_model.blocks.5.self_attn.v.weight\n",
      "video_model.blocks.5.self_attn.v.bias\n",
      "video_model.blocks.5.self_attn.o.weight\n",
      "video_model.blocks.5.self_attn.o.bias\n",
      "video_model.blocks.5.self_attn.norm_q.weight\n",
      "video_model.blocks.5.self_attn.norm_k.weight\n",
      "video_model.blocks.5.norm3.weight\n",
      "video_model.blocks.5.norm3.bias\n",
      "video_model.blocks.5.cross_attn.q.weight\n",
      "video_model.blocks.5.cross_attn.q.bias\n",
      "video_model.blocks.5.cross_attn.k.weight\n",
      "video_model.blocks.5.cross_attn.k.bias\n",
      "video_model.blocks.5.cross_attn.v.weight\n",
      "video_model.blocks.5.cross_attn.v.bias\n",
      "video_model.blocks.5.cross_attn.o.weight\n",
      "video_model.blocks.5.cross_attn.o.bias\n",
      "video_model.blocks.5.cross_attn.norm_q.weight\n",
      "video_model.blocks.5.cross_attn.norm_k.weight\n",
      "video_model.blocks.5.cross_attn.k_fusion.weight\n",
      "video_model.blocks.5.cross_attn.k_fusion.bias\n",
      "video_model.blocks.5.cross_attn.v_fusion.weight\n",
      "video_model.blocks.5.cross_attn.v_fusion.bias\n",
      "video_model.blocks.5.cross_attn.pre_attn_norm_fusion.weight\n",
      "video_model.blocks.5.cross_attn.pre_attn_norm_fusion.bias\n",
      "video_model.blocks.5.cross_attn.norm_k_fusion.weight\n",
      "video_model.blocks.5.ffn.0.weight\n",
      "video_model.blocks.5.ffn.0.bias\n",
      "video_model.blocks.5.ffn.2.weight\n",
      "video_model.blocks.5.ffn.2.bias\n",
      "video_model.blocks.5.modulation.modulation\n",
      "video_model.blocks.6.self_attn.q.weight\n",
      "video_model.blocks.6.self_attn.q.bias\n",
      "video_model.blocks.6.self_attn.k.weight\n",
      "video_model.blocks.6.self_attn.k.bias\n",
      "video_model.blocks.6.self_attn.v.weight\n",
      "video_model.blocks.6.self_attn.v.bias\n",
      "video_model.blocks.6.self_attn.o.weight\n",
      "video_model.blocks.6.self_attn.o.bias\n",
      "video_model.blocks.6.self_attn.norm_q.weight\n",
      "video_model.blocks.6.self_attn.norm_k.weight\n",
      "video_model.blocks.6.norm3.weight\n",
      "video_model.blocks.6.norm3.bias\n",
      "video_model.blocks.6.cross_attn.q.weight\n",
      "video_model.blocks.6.cross_attn.q.bias\n",
      "video_model.blocks.6.cross_attn.k.weight\n",
      "video_model.blocks.6.cross_attn.k.bias\n",
      "video_model.blocks.6.cross_attn.v.weight\n",
      "video_model.blocks.6.cross_attn.v.bias\n",
      "video_model.blocks.6.cross_attn.o.weight\n",
      "video_model.blocks.6.cross_attn.o.bias\n",
      "video_model.blocks.6.cross_attn.norm_q.weight\n",
      "video_model.blocks.6.cross_attn.norm_k.weight\n",
      "video_model.blocks.6.cross_attn.k_fusion.weight\n",
      "video_model.blocks.6.cross_attn.k_fusion.bias\n",
      "video_model.blocks.6.cross_attn.v_fusion.weight\n",
      "video_model.blocks.6.cross_attn.v_fusion.bias\n",
      "video_model.blocks.6.cross_attn.pre_attn_norm_fusion.weight\n",
      "video_model.blocks.6.cross_attn.pre_attn_norm_fusion.bias\n",
      "video_model.blocks.6.cross_attn.norm_k_fusion.weight\n",
      "video_model.blocks.6.ffn.0.weight\n",
      "video_model.blocks.6.ffn.0.bias\n",
      "video_model.blocks.6.ffn.2.weight\n",
      "video_model.blocks.6.ffn.2.bias\n",
      "video_model.blocks.6.modulation.modulation\n",
      "video_model.blocks.7.self_attn.q.weight\n",
      "video_model.blocks.7.self_attn.q.bias\n",
      "video_model.blocks.7.self_attn.k.weight\n",
      "video_model.blocks.7.self_attn.k.bias\n",
      "video_model.blocks.7.self_attn.v.weight\n",
      "video_model.blocks.7.self_attn.v.bias\n",
      "video_model.blocks.7.self_attn.o.weight\n",
      "video_model.blocks.7.self_attn.o.bias\n",
      "video_model.blocks.7.self_attn.norm_q.weight\n",
      "video_model.blocks.7.self_attn.norm_k.weight\n",
      "video_model.blocks.7.norm3.weight\n",
      "video_model.blocks.7.norm3.bias\n",
      "video_model.blocks.7.cross_attn.q.weight\n",
      "video_model.blocks.7.cross_attn.q.bias\n",
      "video_model.blocks.7.cross_attn.k.weight\n",
      "video_model.blocks.7.cross_attn.k.bias\n",
      "video_model.blocks.7.cross_attn.v.weight\n",
      "video_model.blocks.7.cross_attn.v.bias\n",
      "video_model.blocks.7.cross_attn.o.weight\n",
      "video_model.blocks.7.cross_attn.o.bias\n",
      "video_model.blocks.7.cross_attn.norm_q.weight\n",
      "video_model.blocks.7.cross_attn.norm_k.weight\n",
      "video_model.blocks.7.cross_attn.k_fusion.weight\n",
      "video_model.blocks.7.cross_attn.k_fusion.bias\n",
      "video_model.blocks.7.cross_attn.v_fusion.weight\n",
      "video_model.blocks.7.cross_attn.v_fusion.bias\n",
      "video_model.blocks.7.cross_attn.pre_attn_norm_fusion.weight\n",
      "video_model.blocks.7.cross_attn.pre_attn_norm_fusion.bias\n",
      "video_model.blocks.7.cross_attn.norm_k_fusion.weight\n",
      "video_model.blocks.7.ffn.0.weight\n",
      "video_model.blocks.7.ffn.0.bias\n",
      "video_model.blocks.7.ffn.2.weight\n",
      "video_model.blocks.7.ffn.2.bias\n",
      "video_model.blocks.7.modulation.modulation\n",
      "video_model.blocks.8.self_attn.q.weight\n",
      "video_model.blocks.8.self_attn.q.bias\n",
      "video_model.blocks.8.self_attn.k.weight\n",
      "video_model.blocks.8.self_attn.k.bias\n",
      "video_model.blocks.8.self_attn.v.weight\n",
      "video_model.blocks.8.self_attn.v.bias\n",
      "video_model.blocks.8.self_attn.o.weight\n",
      "video_model.blocks.8.self_attn.o.bias\n",
      "video_model.blocks.8.self_attn.norm_q.weight\n",
      "video_model.blocks.8.self_attn.norm_k.weight\n",
      "video_model.blocks.8.norm3.weight\n",
      "video_model.blocks.8.norm3.bias\n",
      "video_model.blocks.8.cross_attn.q.weight\n",
      "video_model.blocks.8.cross_attn.q.bias\n",
      "video_model.blocks.8.cross_attn.k.weight\n",
      "video_model.blocks.8.cross_attn.k.bias\n",
      "video_model.blocks.8.cross_attn.v.weight\n",
      "video_model.blocks.8.cross_attn.v.bias\n",
      "video_model.blocks.8.cross_attn.o.weight\n",
      "video_model.blocks.8.cross_attn.o.bias\n",
      "video_model.blocks.8.cross_attn.norm_q.weight\n",
      "video_model.blocks.8.cross_attn.norm_k.weight\n",
      "video_model.blocks.8.cross_attn.k_fusion.weight\n",
      "video_model.blocks.8.cross_attn.k_fusion.bias\n",
      "video_model.blocks.8.cross_attn.v_fusion.weight\n",
      "video_model.blocks.8.cross_attn.v_fusion.bias\n",
      "video_model.blocks.8.cross_attn.pre_attn_norm_fusion.weight\n",
      "video_model.blocks.8.cross_attn.pre_attn_norm_fusion.bias\n",
      "video_model.blocks.8.cross_attn.norm_k_fusion.weight\n",
      "video_model.blocks.8.ffn.0.weight\n",
      "video_model.blocks.8.ffn.0.bias\n",
      "video_model.blocks.8.ffn.2.weight\n",
      "video_model.blocks.8.ffn.2.bias\n",
      "video_model.blocks.8.modulation.modulation\n",
      "video_model.blocks.9.self_attn.q.weight\n",
      "video_model.blocks.9.self_attn.q.bias\n",
      "video_model.blocks.9.self_attn.k.weight\n",
      "video_model.blocks.9.self_attn.k.bias\n",
      "video_model.blocks.9.self_attn.v.weight\n",
      "video_model.blocks.9.self_attn.v.bias\n",
      "video_model.blocks.9.self_attn.o.weight\n",
      "video_model.blocks.9.self_attn.o.bias\n",
      "video_model.blocks.9.self_attn.norm_q.weight\n",
      "video_model.blocks.9.self_attn.norm_k.weight\n",
      "video_model.blocks.9.norm3.weight\n",
      "video_model.blocks.9.norm3.bias\n",
      "video_model.blocks.9.cross_attn.q.weight\n",
      "video_model.blocks.9.cross_attn.q.bias\n",
      "video_model.blocks.9.cross_attn.k.weight\n",
      "video_model.blocks.9.cross_attn.k.bias\n",
      "video_model.blocks.9.cross_attn.v.weight\n",
      "video_model.blocks.9.cross_attn.v.bias\n",
      "video_model.blocks.9.cross_attn.o.weight\n",
      "video_model.blocks.9.cross_attn.o.bias\n",
      "video_model.blocks.9.cross_attn.norm_q.weight\n",
      "video_model.blocks.9.cross_attn.norm_k.weight\n",
      "video_model.blocks.9.cross_attn.k_fusion.weight\n",
      "video_model.blocks.9.cross_attn.k_fusion.bias\n",
      "video_model.blocks.9.cross_attn.v_fusion.weight\n",
      "video_model.blocks.9.cross_attn.v_fusion.bias\n",
      "video_model.blocks.9.cross_attn.pre_attn_norm_fusion.weight\n",
      "video_model.blocks.9.cross_attn.pre_attn_norm_fusion.bias\n",
      "video_model.blocks.9.cross_attn.norm_k_fusion.weight\n",
      "video_model.blocks.9.ffn.0.weight\n",
      "video_model.blocks.9.ffn.0.bias\n",
      "video_model.blocks.9.ffn.2.weight\n",
      "video_model.blocks.9.ffn.2.bias\n",
      "video_model.blocks.9.modulation.modulation\n",
      "video_model.blocks.10.self_attn.q.weight\n",
      "video_model.blocks.10.self_attn.q.bias\n",
      "video_model.blocks.10.self_attn.k.weight\n",
      "video_model.blocks.10.self_attn.k.bias\n",
      "video_model.blocks.10.self_attn.v.weight\n",
      "video_model.blocks.10.self_attn.v.bias\n",
      "video_model.blocks.10.self_attn.o.weight\n",
      "video_model.blocks.10.self_attn.o.bias\n",
      "video_model.blocks.10.self_attn.norm_q.weight\n",
      "video_model.blocks.10.self_attn.norm_k.weight\n",
      "video_model.blocks.10.norm3.weight\n",
      "video_model.blocks.10.norm3.bias\n",
      "video_model.blocks.10.cross_attn.q.weight\n",
      "video_model.blocks.10.cross_attn.q.bias\n",
      "video_model.blocks.10.cross_attn.k.weight\n",
      "video_model.blocks.10.cross_attn.k.bias\n",
      "video_model.blocks.10.cross_attn.v.weight\n",
      "video_model.blocks.10.cross_attn.v.bias\n",
      "video_model.blocks.10.cross_attn.o.weight\n",
      "video_model.blocks.10.cross_attn.o.bias\n",
      "video_model.blocks.10.cross_attn.norm_q.weight\n",
      "video_model.blocks.10.cross_attn.norm_k.weight\n",
      "video_model.blocks.10.cross_attn.k_fusion.weight\n",
      "video_model.blocks.10.cross_attn.k_fusion.bias\n",
      "video_model.blocks.10.cross_attn.v_fusion.weight\n",
      "video_model.blocks.10.cross_attn.v_fusion.bias\n",
      "video_model.blocks.10.cross_attn.pre_attn_norm_fusion.weight\n",
      "video_model.blocks.10.cross_attn.pre_attn_norm_fusion.bias\n",
      "video_model.blocks.10.cross_attn.norm_k_fusion.weight\n",
      "video_model.blocks.10.ffn.0.weight\n",
      "video_model.blocks.10.ffn.0.bias\n",
      "video_model.blocks.10.ffn.2.weight\n",
      "video_model.blocks.10.ffn.2.bias\n",
      "video_model.blocks.10.modulation.modulation\n",
      "video_model.blocks.11.self_attn.q.weight\n",
      "video_model.blocks.11.self_attn.q.bias\n",
      "video_model.blocks.11.self_attn.k.weight\n",
      "video_model.blocks.11.self_attn.k.bias\n",
      "video_model.blocks.11.self_attn.v.weight\n",
      "video_model.blocks.11.self_attn.v.bias\n",
      "video_model.blocks.11.self_attn.o.weight\n",
      "video_model.blocks.11.self_attn.o.bias\n",
      "video_model.blocks.11.self_attn.norm_q.weight\n",
      "video_model.blocks.11.self_attn.norm_k.weight\n",
      "video_model.blocks.11.norm3.weight\n",
      "video_model.blocks.11.norm3.bias\n",
      "video_model.blocks.11.cross_attn.q.weight\n",
      "video_model.blocks.11.cross_attn.q.bias\n",
      "video_model.blocks.11.cross_attn.k.weight\n",
      "video_model.blocks.11.cross_attn.k.bias\n",
      "video_model.blocks.11.cross_attn.v.weight\n",
      "video_model.blocks.11.cross_attn.v.bias\n",
      "video_model.blocks.11.cross_attn.o.weight\n",
      "video_model.blocks.11.cross_attn.o.bias\n",
      "video_model.blocks.11.cross_attn.norm_q.weight\n",
      "video_model.blocks.11.cross_attn.norm_k.weight\n",
      "video_model.blocks.11.cross_attn.k_fusion.weight\n",
      "video_model.blocks.11.cross_attn.k_fusion.bias\n",
      "video_model.blocks.11.cross_attn.v_fusion.weight\n",
      "video_model.blocks.11.cross_attn.v_fusion.bias\n",
      "video_model.blocks.11.cross_attn.pre_attn_norm_fusion.weight\n",
      "video_model.blocks.11.cross_attn.pre_attn_norm_fusion.bias\n",
      "video_model.blocks.11.cross_attn.norm_k_fusion.weight\n",
      "video_model.blocks.11.ffn.0.weight\n",
      "video_model.blocks.11.ffn.0.bias\n",
      "video_model.blocks.11.ffn.2.weight\n",
      "video_model.blocks.11.ffn.2.bias\n",
      "video_model.blocks.11.modulation.modulation\n",
      "video_model.blocks.12.self_attn.q.weight\n",
      "video_model.blocks.12.self_attn.q.bias\n",
      "video_model.blocks.12.self_attn.k.weight\n",
      "video_model.blocks.12.self_attn.k.bias\n",
      "video_model.blocks.12.self_attn.v.weight\n",
      "video_model.blocks.12.self_attn.v.bias\n",
      "video_model.blocks.12.self_attn.o.weight\n",
      "video_model.blocks.12.self_attn.o.bias\n",
      "video_model.blocks.12.self_attn.norm_q.weight\n",
      "video_model.blocks.12.self_attn.norm_k.weight\n",
      "video_model.blocks.12.norm3.weight\n",
      "video_model.blocks.12.norm3.bias\n",
      "video_model.blocks.12.cross_attn.q.weight\n",
      "video_model.blocks.12.cross_attn.q.bias\n",
      "video_model.blocks.12.cross_attn.k.weight\n",
      "video_model.blocks.12.cross_attn.k.bias\n",
      "video_model.blocks.12.cross_attn.v.weight\n",
      "video_model.blocks.12.cross_attn.v.bias\n",
      "video_model.blocks.12.cross_attn.o.weight\n",
      "video_model.blocks.12.cross_attn.o.bias\n",
      "video_model.blocks.12.cross_attn.norm_q.weight\n",
      "video_model.blocks.12.cross_attn.norm_k.weight\n",
      "video_model.blocks.12.cross_attn.k_fusion.weight\n",
      "video_model.blocks.12.cross_attn.k_fusion.bias\n",
      "video_model.blocks.12.cross_attn.v_fusion.weight\n",
      "video_model.blocks.12.cross_attn.v_fusion.bias\n",
      "video_model.blocks.12.cross_attn.pre_attn_norm_fusion.weight\n",
      "video_model.blocks.12.cross_attn.pre_attn_norm_fusion.bias\n",
      "video_model.blocks.12.cross_attn.norm_k_fusion.weight\n",
      "video_model.blocks.12.ffn.0.weight\n",
      "video_model.blocks.12.ffn.0.bias\n",
      "video_model.blocks.12.ffn.2.weight\n",
      "video_model.blocks.12.ffn.2.bias\n",
      "video_model.blocks.12.modulation.modulation\n",
      "video_model.blocks.13.self_attn.q.weight\n",
      "video_model.blocks.13.self_attn.q.bias\n",
      "video_model.blocks.13.self_attn.k.weight\n",
      "video_model.blocks.13.self_attn.k.bias\n",
      "video_model.blocks.13.self_attn.v.weight\n",
      "video_model.blocks.13.self_attn.v.bias\n",
      "video_model.blocks.13.self_attn.o.weight\n",
      "video_model.blocks.13.self_attn.o.bias\n",
      "video_model.blocks.13.self_attn.norm_q.weight\n",
      "video_model.blocks.13.self_attn.norm_k.weight\n",
      "video_model.blocks.13.norm3.weight\n",
      "video_model.blocks.13.norm3.bias\n",
      "video_model.blocks.13.cross_attn.q.weight\n",
      "video_model.blocks.13.cross_attn.q.bias\n",
      "video_model.blocks.13.cross_attn.k.weight\n",
      "video_model.blocks.13.cross_attn.k.bias\n",
      "video_model.blocks.13.cross_attn.v.weight\n",
      "video_model.blocks.13.cross_attn.v.bias\n",
      "video_model.blocks.13.cross_attn.o.weight\n",
      "video_model.blocks.13.cross_attn.o.bias\n",
      "video_model.blocks.13.cross_attn.norm_q.weight\n",
      "video_model.blocks.13.cross_attn.norm_k.weight\n",
      "video_model.blocks.13.cross_attn.k_fusion.weight\n",
      "video_model.blocks.13.cross_attn.k_fusion.bias\n",
      "video_model.blocks.13.cross_attn.v_fusion.weight\n",
      "video_model.blocks.13.cross_attn.v_fusion.bias\n",
      "video_model.blocks.13.cross_attn.pre_attn_norm_fusion.weight\n",
      "video_model.blocks.13.cross_attn.pre_attn_norm_fusion.bias\n",
      "video_model.blocks.13.cross_attn.norm_k_fusion.weight\n",
      "video_model.blocks.13.ffn.0.weight\n",
      "video_model.blocks.13.ffn.0.bias\n",
      "video_model.blocks.13.ffn.2.weight\n",
      "video_model.blocks.13.ffn.2.bias\n",
      "video_model.blocks.13.modulation.modulation\n",
      "video_model.blocks.14.self_attn.q.weight\n",
      "video_model.blocks.14.self_attn.q.bias\n",
      "video_model.blocks.14.self_attn.k.weight\n",
      "video_model.blocks.14.self_attn.k.bias\n",
      "video_model.blocks.14.self_attn.v.weight\n",
      "video_model.blocks.14.self_attn.v.bias\n",
      "video_model.blocks.14.self_attn.o.weight\n",
      "video_model.blocks.14.self_attn.o.bias\n",
      "video_model.blocks.14.self_attn.norm_q.weight\n",
      "video_model.blocks.14.self_attn.norm_k.weight\n",
      "video_model.blocks.14.norm3.weight\n",
      "video_model.blocks.14.norm3.bias\n",
      "video_model.blocks.14.cross_attn.q.weight\n",
      "video_model.blocks.14.cross_attn.q.bias\n",
      "video_model.blocks.14.cross_attn.k.weight\n",
      "video_model.blocks.14.cross_attn.k.bias\n",
      "video_model.blocks.14.cross_attn.v.weight\n",
      "video_model.blocks.14.cross_attn.v.bias\n",
      "video_model.blocks.14.cross_attn.o.weight\n",
      "video_model.blocks.14.cross_attn.o.bias\n",
      "video_model.blocks.14.cross_attn.norm_q.weight\n",
      "video_model.blocks.14.cross_attn.norm_k.weight\n",
      "video_model.blocks.14.cross_attn.k_fusion.weight\n",
      "video_model.blocks.14.cross_attn.k_fusion.bias\n",
      "video_model.blocks.14.cross_attn.v_fusion.weight\n",
      "video_model.blocks.14.cross_attn.v_fusion.bias\n",
      "video_model.blocks.14.cross_attn.pre_attn_norm_fusion.weight\n",
      "video_model.blocks.14.cross_attn.pre_attn_norm_fusion.bias\n",
      "video_model.blocks.14.cross_attn.norm_k_fusion.weight\n",
      "video_model.blocks.14.ffn.0.weight\n",
      "video_model.blocks.14.ffn.0.bias\n",
      "video_model.blocks.14.ffn.2.weight\n",
      "video_model.blocks.14.ffn.2.bias\n",
      "video_model.blocks.14.modulation.modulation\n",
      "video_model.blocks.15.self_attn.q.weight\n",
      "video_model.blocks.15.self_attn.q.bias\n",
      "video_model.blocks.15.self_attn.k.weight\n",
      "video_model.blocks.15.self_attn.k.bias\n",
      "video_model.blocks.15.self_attn.v.weight\n",
      "video_model.blocks.15.self_attn.v.bias\n",
      "video_model.blocks.15.self_attn.o.weight\n",
      "video_model.blocks.15.self_attn.o.bias\n",
      "video_model.blocks.15.self_attn.norm_q.weight\n",
      "video_model.blocks.15.self_attn.norm_k.weight\n",
      "video_model.blocks.15.norm3.weight\n",
      "video_model.blocks.15.norm3.bias\n",
      "video_model.blocks.15.cross_attn.q.weight\n",
      "video_model.blocks.15.cross_attn.q.bias\n",
      "video_model.blocks.15.cross_attn.k.weight\n",
      "video_model.blocks.15.cross_attn.k.bias\n",
      "video_model.blocks.15.cross_attn.v.weight\n",
      "video_model.blocks.15.cross_attn.v.bias\n",
      "video_model.blocks.15.cross_attn.o.weight\n",
      "video_model.blocks.15.cross_attn.o.bias\n",
      "video_model.blocks.15.cross_attn.norm_q.weight\n",
      "video_model.blocks.15.cross_attn.norm_k.weight\n",
      "video_model.blocks.15.cross_attn.k_fusion.weight\n",
      "video_model.blocks.15.cross_attn.k_fusion.bias\n",
      "video_model.blocks.15.cross_attn.v_fusion.weight\n",
      "video_model.blocks.15.cross_attn.v_fusion.bias\n",
      "video_model.blocks.15.cross_attn.pre_attn_norm_fusion.weight\n",
      "video_model.blocks.15.cross_attn.pre_attn_norm_fusion.bias\n",
      "video_model.blocks.15.cross_attn.norm_k_fusion.weight\n",
      "video_model.blocks.15.ffn.0.weight\n",
      "video_model.blocks.15.ffn.0.bias\n",
      "video_model.blocks.15.ffn.2.weight\n",
      "video_model.blocks.15.ffn.2.bias\n",
      "video_model.blocks.15.modulation.modulation\n",
      "video_model.blocks.16.self_attn.q.weight\n",
      "video_model.blocks.16.self_attn.q.bias\n",
      "video_model.blocks.16.self_attn.k.weight\n",
      "video_model.blocks.16.self_attn.k.bias\n",
      "video_model.blocks.16.self_attn.v.weight\n",
      "video_model.blocks.16.self_attn.v.bias\n",
      "video_model.blocks.16.self_attn.o.weight\n",
      "video_model.blocks.16.self_attn.o.bias\n",
      "video_model.blocks.16.self_attn.norm_q.weight\n",
      "video_model.blocks.16.self_attn.norm_k.weight\n",
      "video_model.blocks.16.norm3.weight\n",
      "video_model.blocks.16.norm3.bias\n",
      "video_model.blocks.16.cross_attn.q.weight\n",
      "video_model.blocks.16.cross_attn.q.bias\n",
      "video_model.blocks.16.cross_attn.k.weight\n",
      "video_model.blocks.16.cross_attn.k.bias\n",
      "video_model.blocks.16.cross_attn.v.weight\n",
      "video_model.blocks.16.cross_attn.v.bias\n",
      "video_model.blocks.16.cross_attn.o.weight\n",
      "video_model.blocks.16.cross_attn.o.bias\n",
      "video_model.blocks.16.cross_attn.norm_q.weight\n",
      "video_model.blocks.16.cross_attn.norm_k.weight\n",
      "video_model.blocks.16.cross_attn.k_fusion.weight\n",
      "video_model.blocks.16.cross_attn.k_fusion.bias\n",
      "video_model.blocks.16.cross_attn.v_fusion.weight\n",
      "video_model.blocks.16.cross_attn.v_fusion.bias\n",
      "video_model.blocks.16.cross_attn.pre_attn_norm_fusion.weight\n",
      "video_model.blocks.16.cross_attn.pre_attn_norm_fusion.bias\n",
      "video_model.blocks.16.cross_attn.norm_k_fusion.weight\n",
      "video_model.blocks.16.ffn.0.weight\n",
      "video_model.blocks.16.ffn.0.bias\n",
      "video_model.blocks.16.ffn.2.weight\n",
      "video_model.blocks.16.ffn.2.bias\n",
      "video_model.blocks.16.modulation.modulation\n",
      "video_model.blocks.17.self_attn.q.weight\n",
      "video_model.blocks.17.self_attn.q.bias\n",
      "video_model.blocks.17.self_attn.k.weight\n",
      "video_model.blocks.17.self_attn.k.bias\n",
      "video_model.blocks.17.self_attn.v.weight\n",
      "video_model.blocks.17.self_attn.v.bias\n",
      "video_model.blocks.17.self_attn.o.weight\n",
      "video_model.blocks.17.self_attn.o.bias\n",
      "video_model.blocks.17.self_attn.norm_q.weight\n",
      "video_model.blocks.17.self_attn.norm_k.weight\n",
      "video_model.blocks.17.norm3.weight\n",
      "video_model.blocks.17.norm3.bias\n",
      "video_model.blocks.17.cross_attn.q.weight\n",
      "video_model.blocks.17.cross_attn.q.bias\n",
      "video_model.blocks.17.cross_attn.k.weight\n",
      "video_model.blocks.17.cross_attn.k.bias\n",
      "video_model.blocks.17.cross_attn.v.weight\n",
      "video_model.blocks.17.cross_attn.v.bias\n",
      "video_model.blocks.17.cross_attn.o.weight\n",
      "video_model.blocks.17.cross_attn.o.bias\n",
      "video_model.blocks.17.cross_attn.norm_q.weight\n",
      "video_model.blocks.17.cross_attn.norm_k.weight\n",
      "video_model.blocks.17.cross_attn.k_fusion.weight\n",
      "video_model.blocks.17.cross_attn.k_fusion.bias\n",
      "video_model.blocks.17.cross_attn.v_fusion.weight\n",
      "video_model.blocks.17.cross_attn.v_fusion.bias\n",
      "video_model.blocks.17.cross_attn.pre_attn_norm_fusion.weight\n",
      "video_model.blocks.17.cross_attn.pre_attn_norm_fusion.bias\n",
      "video_model.blocks.17.cross_attn.norm_k_fusion.weight\n",
      "video_model.blocks.17.ffn.0.weight\n",
      "video_model.blocks.17.ffn.0.bias\n",
      "video_model.blocks.17.ffn.2.weight\n",
      "video_model.blocks.17.ffn.2.bias\n",
      "video_model.blocks.17.modulation.modulation\n",
      "video_model.blocks.18.self_attn.q.weight\n",
      "video_model.blocks.18.self_attn.q.bias\n",
      "video_model.blocks.18.self_attn.k.weight\n",
      "video_model.blocks.18.self_attn.k.bias\n",
      "video_model.blocks.18.self_attn.v.weight\n",
      "video_model.blocks.18.self_attn.v.bias\n",
      "video_model.blocks.18.self_attn.o.weight\n",
      "video_model.blocks.18.self_attn.o.bias\n",
      "video_model.blocks.18.self_attn.norm_q.weight\n",
      "video_model.blocks.18.self_attn.norm_k.weight\n",
      "video_model.blocks.18.norm3.weight\n",
      "video_model.blocks.18.norm3.bias\n",
      "video_model.blocks.18.cross_attn.q.weight\n",
      "video_model.blocks.18.cross_attn.q.bias\n",
      "video_model.blocks.18.cross_attn.k.weight\n",
      "video_model.blocks.18.cross_attn.k.bias\n",
      "video_model.blocks.18.cross_attn.v.weight\n",
      "video_model.blocks.18.cross_attn.v.bias\n",
      "video_model.blocks.18.cross_attn.o.weight\n",
      "video_model.blocks.18.cross_attn.o.bias\n",
      "video_model.blocks.18.cross_attn.norm_q.weight\n",
      "video_model.blocks.18.cross_attn.norm_k.weight\n",
      "video_model.blocks.18.cross_attn.k_fusion.weight\n",
      "video_model.blocks.18.cross_attn.k_fusion.bias\n",
      "video_model.blocks.18.cross_attn.v_fusion.weight\n",
      "video_model.blocks.18.cross_attn.v_fusion.bias\n",
      "video_model.blocks.18.cross_attn.pre_attn_norm_fusion.weight\n",
      "video_model.blocks.18.cross_attn.pre_attn_norm_fusion.bias\n",
      "video_model.blocks.18.cross_attn.norm_k_fusion.weight\n",
      "video_model.blocks.18.ffn.0.weight\n",
      "video_model.blocks.18.ffn.0.bias\n",
      "video_model.blocks.18.ffn.2.weight\n",
      "video_model.blocks.18.ffn.2.bias\n",
      "video_model.blocks.18.modulation.modulation\n",
      "video_model.blocks.19.self_attn.q.weight\n",
      "video_model.blocks.19.self_attn.q.bias\n",
      "video_model.blocks.19.self_attn.k.weight\n",
      "video_model.blocks.19.self_attn.k.bias\n",
      "video_model.blocks.19.self_attn.v.weight\n",
      "video_model.blocks.19.self_attn.v.bias\n",
      "video_model.blocks.19.self_attn.o.weight\n",
      "video_model.blocks.19.self_attn.o.bias\n",
      "video_model.blocks.19.self_attn.norm_q.weight\n",
      "video_model.blocks.19.self_attn.norm_k.weight\n",
      "video_model.blocks.19.norm3.weight\n",
      "video_model.blocks.19.norm3.bias\n",
      "video_model.blocks.19.cross_attn.q.weight\n",
      "video_model.blocks.19.cross_attn.q.bias\n",
      "video_model.blocks.19.cross_attn.k.weight\n",
      "video_model.blocks.19.cross_attn.k.bias\n",
      "video_model.blocks.19.cross_attn.v.weight\n",
      "video_model.blocks.19.cross_attn.v.bias\n",
      "video_model.blocks.19.cross_attn.o.weight\n",
      "video_model.blocks.19.cross_attn.o.bias\n",
      "video_model.blocks.19.cross_attn.norm_q.weight\n",
      "video_model.blocks.19.cross_attn.norm_k.weight\n",
      "video_model.blocks.19.cross_attn.k_fusion.weight\n",
      "video_model.blocks.19.cross_attn.k_fusion.bias\n",
      "video_model.blocks.19.cross_attn.v_fusion.weight\n",
      "video_model.blocks.19.cross_attn.v_fusion.bias\n",
      "video_model.blocks.19.cross_attn.pre_attn_norm_fusion.weight\n",
      "video_model.blocks.19.cross_attn.pre_attn_norm_fusion.bias\n",
      "video_model.blocks.19.cross_attn.norm_k_fusion.weight\n",
      "video_model.blocks.19.ffn.0.weight\n",
      "video_model.blocks.19.ffn.0.bias\n",
      "video_model.blocks.19.ffn.2.weight\n",
      "video_model.blocks.19.ffn.2.bias\n",
      "video_model.blocks.19.modulation.modulation\n",
      "video_model.blocks.20.self_attn.q.weight\n",
      "video_model.blocks.20.self_attn.q.bias\n",
      "video_model.blocks.20.self_attn.k.weight\n",
      "video_model.blocks.20.self_attn.k.bias\n",
      "video_model.blocks.20.self_attn.v.weight\n",
      "video_model.blocks.20.self_attn.v.bias\n",
      "video_model.blocks.20.self_attn.o.weight\n",
      "video_model.blocks.20.self_attn.o.bias\n",
      "video_model.blocks.20.self_attn.norm_q.weight\n",
      "video_model.blocks.20.self_attn.norm_k.weight\n",
      "video_model.blocks.20.norm3.weight\n",
      "video_model.blocks.20.norm3.bias\n",
      "video_model.blocks.20.cross_attn.q.weight\n",
      "video_model.blocks.20.cross_attn.q.bias\n",
      "video_model.blocks.20.cross_attn.k.weight\n",
      "video_model.blocks.20.cross_attn.k.bias\n",
      "video_model.blocks.20.cross_attn.v.weight\n",
      "video_model.blocks.20.cross_attn.v.bias\n",
      "video_model.blocks.20.cross_attn.o.weight\n",
      "video_model.blocks.20.cross_attn.o.bias\n",
      "video_model.blocks.20.cross_attn.norm_q.weight\n",
      "video_model.blocks.20.cross_attn.norm_k.weight\n",
      "video_model.blocks.20.cross_attn.k_fusion.weight\n",
      "video_model.blocks.20.cross_attn.k_fusion.bias\n",
      "video_model.blocks.20.cross_attn.v_fusion.weight\n",
      "video_model.blocks.20.cross_attn.v_fusion.bias\n",
      "video_model.blocks.20.cross_attn.pre_attn_norm_fusion.weight\n",
      "video_model.blocks.20.cross_attn.pre_attn_norm_fusion.bias\n",
      "video_model.blocks.20.cross_attn.norm_k_fusion.weight\n",
      "video_model.blocks.20.ffn.0.weight\n",
      "video_model.blocks.20.ffn.0.bias\n",
      "video_model.blocks.20.ffn.2.weight\n",
      "video_model.blocks.20.ffn.2.bias\n",
      "video_model.blocks.20.modulation.modulation\n",
      "video_model.blocks.21.self_attn.q.weight\n",
      "video_model.blocks.21.self_attn.q.bias\n",
      "video_model.blocks.21.self_attn.k.weight\n",
      "video_model.blocks.21.self_attn.k.bias\n",
      "video_model.blocks.21.self_attn.v.weight\n",
      "video_model.blocks.21.self_attn.v.bias\n",
      "video_model.blocks.21.self_attn.o.weight\n",
      "video_model.blocks.21.self_attn.o.bias\n",
      "video_model.blocks.21.self_attn.norm_q.weight\n",
      "video_model.blocks.21.self_attn.norm_k.weight\n",
      "video_model.blocks.21.norm3.weight\n",
      "video_model.blocks.21.norm3.bias\n",
      "video_model.blocks.21.cross_attn.q.weight\n",
      "video_model.blocks.21.cross_attn.q.bias\n",
      "video_model.blocks.21.cross_attn.k.weight\n",
      "video_model.blocks.21.cross_attn.k.bias\n",
      "video_model.blocks.21.cross_attn.v.weight\n",
      "video_model.blocks.21.cross_attn.v.bias\n",
      "video_model.blocks.21.cross_attn.o.weight\n",
      "video_model.blocks.21.cross_attn.o.bias\n",
      "video_model.blocks.21.cross_attn.norm_q.weight\n",
      "video_model.blocks.21.cross_attn.norm_k.weight\n",
      "video_model.blocks.21.cross_attn.k_fusion.weight\n",
      "video_model.blocks.21.cross_attn.k_fusion.bias\n",
      "video_model.blocks.21.cross_attn.v_fusion.weight\n",
      "video_model.blocks.21.cross_attn.v_fusion.bias\n",
      "video_model.blocks.21.cross_attn.pre_attn_norm_fusion.weight\n",
      "video_model.blocks.21.cross_attn.pre_attn_norm_fusion.bias\n",
      "video_model.blocks.21.cross_attn.norm_k_fusion.weight\n",
      "video_model.blocks.21.ffn.0.weight\n",
      "video_model.blocks.21.ffn.0.bias\n",
      "video_model.blocks.21.ffn.2.weight\n",
      "video_model.blocks.21.ffn.2.bias\n",
      "video_model.blocks.21.modulation.modulation\n",
      "video_model.blocks.22.self_attn.q.weight\n",
      "video_model.blocks.22.self_attn.q.bias\n",
      "video_model.blocks.22.self_attn.k.weight\n",
      "video_model.blocks.22.self_attn.k.bias\n",
      "video_model.blocks.22.self_attn.v.weight\n",
      "video_model.blocks.22.self_attn.v.bias\n",
      "video_model.blocks.22.self_attn.o.weight\n",
      "video_model.blocks.22.self_attn.o.bias\n",
      "video_model.blocks.22.self_attn.norm_q.weight\n",
      "video_model.blocks.22.self_attn.norm_k.weight\n",
      "video_model.blocks.22.norm3.weight\n",
      "video_model.blocks.22.norm3.bias\n",
      "video_model.blocks.22.cross_attn.q.weight\n",
      "video_model.blocks.22.cross_attn.q.bias\n",
      "video_model.blocks.22.cross_attn.k.weight\n",
      "video_model.blocks.22.cross_attn.k.bias\n",
      "video_model.blocks.22.cross_attn.v.weight\n",
      "video_model.blocks.22.cross_attn.v.bias\n",
      "video_model.blocks.22.cross_attn.o.weight\n",
      "video_model.blocks.22.cross_attn.o.bias\n",
      "video_model.blocks.22.cross_attn.norm_q.weight\n",
      "video_model.blocks.22.cross_attn.norm_k.weight\n",
      "video_model.blocks.22.cross_attn.k_fusion.weight\n",
      "video_model.blocks.22.cross_attn.k_fusion.bias\n",
      "video_model.blocks.22.cross_attn.v_fusion.weight\n",
      "video_model.blocks.22.cross_attn.v_fusion.bias\n",
      "video_model.blocks.22.cross_attn.pre_attn_norm_fusion.weight\n",
      "video_model.blocks.22.cross_attn.pre_attn_norm_fusion.bias\n",
      "video_model.blocks.22.cross_attn.norm_k_fusion.weight\n",
      "video_model.blocks.22.ffn.0.weight\n",
      "video_model.blocks.22.ffn.0.bias\n",
      "video_model.blocks.22.ffn.2.weight\n",
      "video_model.blocks.22.ffn.2.bias\n",
      "video_model.blocks.22.modulation.modulation\n",
      "video_model.blocks.23.self_attn.q.weight\n",
      "video_model.blocks.23.self_attn.q.bias\n",
      "video_model.blocks.23.self_attn.k.weight\n",
      "video_model.blocks.23.self_attn.k.bias\n",
      "video_model.blocks.23.self_attn.v.weight\n",
      "video_model.blocks.23.self_attn.v.bias\n",
      "video_model.blocks.23.self_attn.o.weight\n",
      "video_model.blocks.23.self_attn.o.bias\n",
      "video_model.blocks.23.self_attn.norm_q.weight\n",
      "video_model.blocks.23.self_attn.norm_k.weight\n",
      "video_model.blocks.23.norm3.weight\n",
      "video_model.blocks.23.norm3.bias\n",
      "video_model.blocks.23.cross_attn.q.weight\n",
      "video_model.blocks.23.cross_attn.q.bias\n",
      "video_model.blocks.23.cross_attn.k.weight\n",
      "video_model.blocks.23.cross_attn.k.bias\n",
      "video_model.blocks.23.cross_attn.v.weight\n",
      "video_model.blocks.23.cross_attn.v.bias\n",
      "video_model.blocks.23.cross_attn.o.weight\n",
      "video_model.blocks.23.cross_attn.o.bias\n",
      "video_model.blocks.23.cross_attn.norm_q.weight\n",
      "video_model.blocks.23.cross_attn.norm_k.weight\n",
      "video_model.blocks.23.cross_attn.k_fusion.weight\n",
      "video_model.blocks.23.cross_attn.k_fusion.bias\n",
      "video_model.blocks.23.cross_attn.v_fusion.weight\n",
      "video_model.blocks.23.cross_attn.v_fusion.bias\n",
      "video_model.blocks.23.cross_attn.pre_attn_norm_fusion.weight\n",
      "video_model.blocks.23.cross_attn.pre_attn_norm_fusion.bias\n",
      "video_model.blocks.23.cross_attn.norm_k_fusion.weight\n",
      "video_model.blocks.23.ffn.0.weight\n",
      "video_model.blocks.23.ffn.0.bias\n",
      "video_model.blocks.23.ffn.2.weight\n",
      "video_model.blocks.23.ffn.2.bias\n",
      "video_model.blocks.23.modulation.modulation\n",
      "video_model.blocks.24.self_attn.q.weight\n",
      "video_model.blocks.24.self_attn.q.bias\n",
      "video_model.blocks.24.self_attn.k.weight\n",
      "video_model.blocks.24.self_attn.k.bias\n",
      "video_model.blocks.24.self_attn.v.weight\n",
      "video_model.blocks.24.self_attn.v.bias\n",
      "video_model.blocks.24.self_attn.o.weight\n",
      "video_model.blocks.24.self_attn.o.bias\n",
      "video_model.blocks.24.self_attn.norm_q.weight\n",
      "video_model.blocks.24.self_attn.norm_k.weight\n",
      "video_model.blocks.24.norm3.weight\n",
      "video_model.blocks.24.norm3.bias\n",
      "video_model.blocks.24.cross_attn.q.weight\n",
      "video_model.blocks.24.cross_attn.q.bias\n",
      "video_model.blocks.24.cross_attn.k.weight\n",
      "video_model.blocks.24.cross_attn.k.bias\n",
      "video_model.blocks.24.cross_attn.v.weight\n",
      "video_model.blocks.24.cross_attn.v.bias\n",
      "video_model.blocks.24.cross_attn.o.weight\n",
      "video_model.blocks.24.cross_attn.o.bias\n",
      "video_model.blocks.24.cross_attn.norm_q.weight\n",
      "video_model.blocks.24.cross_attn.norm_k.weight\n",
      "video_model.blocks.24.cross_attn.k_fusion.weight\n",
      "video_model.blocks.24.cross_attn.k_fusion.bias\n",
      "video_model.blocks.24.cross_attn.v_fusion.weight\n",
      "video_model.blocks.24.cross_attn.v_fusion.bias\n",
      "video_model.blocks.24.cross_attn.pre_attn_norm_fusion.weight\n",
      "video_model.blocks.24.cross_attn.pre_attn_norm_fusion.bias\n",
      "video_model.blocks.24.cross_attn.norm_k_fusion.weight\n",
      "video_model.blocks.24.ffn.0.weight\n",
      "video_model.blocks.24.ffn.0.bias\n",
      "video_model.blocks.24.ffn.2.weight\n",
      "video_model.blocks.24.ffn.2.bias\n",
      "video_model.blocks.24.modulation.modulation\n",
      "video_model.blocks.25.self_attn.q.weight\n",
      "video_model.blocks.25.self_attn.q.bias\n",
      "video_model.blocks.25.self_attn.k.weight\n",
      "video_model.blocks.25.self_attn.k.bias\n",
      "video_model.blocks.25.self_attn.v.weight\n",
      "video_model.blocks.25.self_attn.v.bias\n",
      "video_model.blocks.25.self_attn.o.weight\n",
      "video_model.blocks.25.self_attn.o.bias\n",
      "video_model.blocks.25.self_attn.norm_q.weight\n",
      "video_model.blocks.25.self_attn.norm_k.weight\n",
      "video_model.blocks.25.norm3.weight\n",
      "video_model.blocks.25.norm3.bias\n",
      "video_model.blocks.25.cross_attn.q.weight\n",
      "video_model.blocks.25.cross_attn.q.bias\n",
      "video_model.blocks.25.cross_attn.k.weight\n",
      "video_model.blocks.25.cross_attn.k.bias\n",
      "video_model.blocks.25.cross_attn.v.weight\n",
      "video_model.blocks.25.cross_attn.v.bias\n",
      "video_model.blocks.25.cross_attn.o.weight\n",
      "video_model.blocks.25.cross_attn.o.bias\n",
      "video_model.blocks.25.cross_attn.norm_q.weight\n",
      "video_model.blocks.25.cross_attn.norm_k.weight\n",
      "video_model.blocks.25.cross_attn.k_fusion.weight\n",
      "video_model.blocks.25.cross_attn.k_fusion.bias\n",
      "video_model.blocks.25.cross_attn.v_fusion.weight\n",
      "video_model.blocks.25.cross_attn.v_fusion.bias\n",
      "video_model.blocks.25.cross_attn.pre_attn_norm_fusion.weight\n",
      "video_model.blocks.25.cross_attn.pre_attn_norm_fusion.bias\n",
      "video_model.blocks.25.cross_attn.norm_k_fusion.weight\n",
      "video_model.blocks.25.ffn.0.weight\n",
      "video_model.blocks.25.ffn.0.bias\n",
      "video_model.blocks.25.ffn.2.weight\n",
      "video_model.blocks.25.ffn.2.bias\n",
      "video_model.blocks.25.modulation.modulation\n",
      "video_model.blocks.26.self_attn.q.weight\n",
      "video_model.blocks.26.self_attn.q.bias\n",
      "video_model.blocks.26.self_attn.k.weight\n",
      "video_model.blocks.26.self_attn.k.bias\n",
      "video_model.blocks.26.self_attn.v.weight\n",
      "video_model.blocks.26.self_attn.v.bias\n",
      "video_model.blocks.26.self_attn.o.weight\n",
      "video_model.blocks.26.self_attn.o.bias\n",
      "video_model.blocks.26.self_attn.norm_q.weight\n",
      "video_model.blocks.26.self_attn.norm_k.weight\n",
      "video_model.blocks.26.norm3.weight\n",
      "video_model.blocks.26.norm3.bias\n",
      "video_model.blocks.26.cross_attn.q.weight\n",
      "video_model.blocks.26.cross_attn.q.bias\n",
      "video_model.blocks.26.cross_attn.k.weight\n",
      "video_model.blocks.26.cross_attn.k.bias\n",
      "video_model.blocks.26.cross_attn.v.weight\n",
      "video_model.blocks.26.cross_attn.v.bias\n",
      "video_model.blocks.26.cross_attn.o.weight\n",
      "video_model.blocks.26.cross_attn.o.bias\n",
      "video_model.blocks.26.cross_attn.norm_q.weight\n",
      "video_model.blocks.26.cross_attn.norm_k.weight\n",
      "video_model.blocks.26.cross_attn.k_fusion.weight\n",
      "video_model.blocks.26.cross_attn.k_fusion.bias\n",
      "video_model.blocks.26.cross_attn.v_fusion.weight\n",
      "video_model.blocks.26.cross_attn.v_fusion.bias\n",
      "video_model.blocks.26.cross_attn.pre_attn_norm_fusion.weight\n",
      "video_model.blocks.26.cross_attn.pre_attn_norm_fusion.bias\n",
      "video_model.blocks.26.cross_attn.norm_k_fusion.weight\n",
      "video_model.blocks.26.ffn.0.weight\n",
      "video_model.blocks.26.ffn.0.bias\n",
      "video_model.blocks.26.ffn.2.weight\n",
      "video_model.blocks.26.ffn.2.bias\n",
      "video_model.blocks.26.modulation.modulation\n",
      "video_model.blocks.27.self_attn.q.weight\n",
      "video_model.blocks.27.self_attn.q.bias\n",
      "video_model.blocks.27.self_attn.k.weight\n",
      "video_model.blocks.27.self_attn.k.bias\n",
      "video_model.blocks.27.self_attn.v.weight\n",
      "video_model.blocks.27.self_attn.v.bias\n",
      "video_model.blocks.27.self_attn.o.weight\n",
      "video_model.blocks.27.self_attn.o.bias\n",
      "video_model.blocks.27.self_attn.norm_q.weight\n",
      "video_model.blocks.27.self_attn.norm_k.weight\n",
      "video_model.blocks.27.norm3.weight\n",
      "video_model.blocks.27.norm3.bias\n",
      "video_model.blocks.27.cross_attn.q.weight\n",
      "video_model.blocks.27.cross_attn.q.bias\n",
      "video_model.blocks.27.cross_attn.k.weight\n",
      "video_model.blocks.27.cross_attn.k.bias\n",
      "video_model.blocks.27.cross_attn.v.weight\n",
      "video_model.blocks.27.cross_attn.v.bias\n",
      "video_model.blocks.27.cross_attn.o.weight\n",
      "video_model.blocks.27.cross_attn.o.bias\n",
      "video_model.blocks.27.cross_attn.norm_q.weight\n",
      "video_model.blocks.27.cross_attn.norm_k.weight\n",
      "video_model.blocks.27.cross_attn.k_fusion.weight\n",
      "video_model.blocks.27.cross_attn.k_fusion.bias\n",
      "video_model.blocks.27.cross_attn.v_fusion.weight\n",
      "video_model.blocks.27.cross_attn.v_fusion.bias\n",
      "video_model.blocks.27.cross_attn.pre_attn_norm_fusion.weight\n",
      "video_model.blocks.27.cross_attn.pre_attn_norm_fusion.bias\n",
      "video_model.blocks.27.cross_attn.norm_k_fusion.weight\n",
      "video_model.blocks.27.ffn.0.weight\n",
      "video_model.blocks.27.ffn.0.bias\n",
      "video_model.blocks.27.ffn.2.weight\n",
      "video_model.blocks.27.ffn.2.bias\n",
      "video_model.blocks.27.modulation.modulation\n",
      "video_model.blocks.28.self_attn.q.weight\n",
      "video_model.blocks.28.self_attn.q.bias\n",
      "video_model.blocks.28.self_attn.k.weight\n",
      "video_model.blocks.28.self_attn.k.bias\n",
      "video_model.blocks.28.self_attn.v.weight\n",
      "video_model.blocks.28.self_attn.v.bias\n",
      "video_model.blocks.28.self_attn.o.weight\n",
      "video_model.blocks.28.self_attn.o.bias\n",
      "video_model.blocks.28.self_attn.norm_q.weight\n",
      "video_model.blocks.28.self_attn.norm_k.weight\n",
      "video_model.blocks.28.norm3.weight\n",
      "video_model.blocks.28.norm3.bias\n",
      "video_model.blocks.28.cross_attn.q.weight\n",
      "video_model.blocks.28.cross_attn.q.bias\n",
      "video_model.blocks.28.cross_attn.k.weight\n",
      "video_model.blocks.28.cross_attn.k.bias\n",
      "video_model.blocks.28.cross_attn.v.weight\n",
      "video_model.blocks.28.cross_attn.v.bias\n",
      "video_model.blocks.28.cross_attn.o.weight\n",
      "video_model.blocks.28.cross_attn.o.bias\n",
      "video_model.blocks.28.cross_attn.norm_q.weight\n",
      "video_model.blocks.28.cross_attn.norm_k.weight\n",
      "video_model.blocks.28.cross_attn.k_fusion.weight\n",
      "video_model.blocks.28.cross_attn.k_fusion.bias\n",
      "video_model.blocks.28.cross_attn.v_fusion.weight\n",
      "video_model.blocks.28.cross_attn.v_fusion.bias\n",
      "video_model.blocks.28.cross_attn.pre_attn_norm_fusion.weight\n",
      "video_model.blocks.28.cross_attn.pre_attn_norm_fusion.bias\n",
      "video_model.blocks.28.cross_attn.norm_k_fusion.weight\n",
      "video_model.blocks.28.ffn.0.weight\n",
      "video_model.blocks.28.ffn.0.bias\n",
      "video_model.blocks.28.ffn.2.weight\n",
      "video_model.blocks.28.ffn.2.bias\n",
      "video_model.blocks.28.modulation.modulation\n",
      "video_model.blocks.29.self_attn.q.weight\n",
      "video_model.blocks.29.self_attn.q.bias\n",
      "video_model.blocks.29.self_attn.k.weight\n",
      "video_model.blocks.29.self_attn.k.bias\n",
      "video_model.blocks.29.self_attn.v.weight\n",
      "video_model.blocks.29.self_attn.v.bias\n",
      "video_model.blocks.29.self_attn.o.weight\n",
      "video_model.blocks.29.self_attn.o.bias\n",
      "video_model.blocks.29.self_attn.norm_q.weight\n",
      "video_model.blocks.29.self_attn.norm_k.weight\n",
      "video_model.blocks.29.norm3.weight\n",
      "video_model.blocks.29.norm3.bias\n",
      "video_model.blocks.29.cross_attn.q.weight\n",
      "video_model.blocks.29.cross_attn.q.bias\n",
      "video_model.blocks.29.cross_attn.k.weight\n",
      "video_model.blocks.29.cross_attn.k.bias\n",
      "video_model.blocks.29.cross_attn.v.weight\n",
      "video_model.blocks.29.cross_attn.v.bias\n",
      "video_model.blocks.29.cross_attn.o.weight\n",
      "video_model.blocks.29.cross_attn.o.bias\n",
      "video_model.blocks.29.cross_attn.norm_q.weight\n",
      "video_model.blocks.29.cross_attn.norm_k.weight\n",
      "video_model.blocks.29.cross_attn.k_fusion.weight\n",
      "video_model.blocks.29.cross_attn.k_fusion.bias\n",
      "video_model.blocks.29.cross_attn.v_fusion.weight\n",
      "video_model.blocks.29.cross_attn.v_fusion.bias\n",
      "video_model.blocks.29.cross_attn.pre_attn_norm_fusion.weight\n",
      "video_model.blocks.29.cross_attn.pre_attn_norm_fusion.bias\n",
      "video_model.blocks.29.cross_attn.norm_k_fusion.weight\n",
      "video_model.blocks.29.ffn.0.weight\n",
      "video_model.blocks.29.ffn.0.bias\n",
      "video_model.blocks.29.ffn.2.weight\n",
      "video_model.blocks.29.ffn.2.bias\n",
      "video_model.blocks.29.modulation.modulation\n",
      "video_model.head.modulation\n",
      "video_model.head.head.weight\n",
      "video_model.head.head.bias\n",
      "audio_model.patch_embedding.0.weight\n",
      "audio_model.patch_embedding.0.bias\n",
      "audio_model.patch_embedding.2.w1.weight\n",
      "audio_model.patch_embedding.2.w2.weight\n",
      "audio_model.patch_embedding.2.w3.weight\n",
      "audio_model.text_embedding.0.weight\n",
      "audio_model.text_embedding.0.bias\n",
      "audio_model.text_embedding.2.weight\n",
      "audio_model.text_embedding.2.bias\n",
      "audio_model.time_embedding.0.weight\n",
      "audio_model.time_embedding.0.bias\n",
      "audio_model.time_embedding.2.weight\n",
      "audio_model.time_embedding.2.bias\n",
      "audio_model.time_projection.1.weight\n",
      "audio_model.time_projection.1.bias\n",
      "audio_model.blocks.0.self_attn.q.weight\n",
      "audio_model.blocks.0.self_attn.q.bias\n",
      "audio_model.blocks.0.self_attn.k.weight\n",
      "audio_model.blocks.0.self_attn.k.bias\n",
      "audio_model.blocks.0.self_attn.v.weight\n",
      "audio_model.blocks.0.self_attn.v.bias\n",
      "audio_model.blocks.0.self_attn.o.weight\n",
      "audio_model.blocks.0.self_attn.o.bias\n",
      "audio_model.blocks.0.self_attn.norm_q.weight\n",
      "audio_model.blocks.0.self_attn.norm_k.weight\n",
      "audio_model.blocks.0.norm3.weight\n",
      "audio_model.blocks.0.norm3.bias\n",
      "audio_model.blocks.0.cross_attn.q.weight\n",
      "audio_model.blocks.0.cross_attn.q.bias\n",
      "audio_model.blocks.0.cross_attn.k.weight\n",
      "audio_model.blocks.0.cross_attn.k.bias\n",
      "audio_model.blocks.0.cross_attn.v.weight\n",
      "audio_model.blocks.0.cross_attn.v.bias\n",
      "audio_model.blocks.0.cross_attn.o.weight\n",
      "audio_model.blocks.0.cross_attn.o.bias\n",
      "audio_model.blocks.0.cross_attn.norm_q.weight\n",
      "audio_model.blocks.0.cross_attn.norm_k.weight\n",
      "audio_model.blocks.0.cross_attn.k_fusion.weight\n",
      "audio_model.blocks.0.cross_attn.k_fusion.bias\n",
      "audio_model.blocks.0.cross_attn.v_fusion.weight\n",
      "audio_model.blocks.0.cross_attn.v_fusion.bias\n",
      "audio_model.blocks.0.cross_attn.pre_attn_norm_fusion.weight\n",
      "audio_model.blocks.0.cross_attn.pre_attn_norm_fusion.bias\n",
      "audio_model.blocks.0.cross_attn.norm_k_fusion.weight\n",
      "audio_model.blocks.0.ffn.0.weight\n",
      "audio_model.blocks.0.ffn.0.bias\n",
      "audio_model.blocks.0.ffn.2.weight\n",
      "audio_model.blocks.0.ffn.2.bias\n",
      "audio_model.blocks.0.modulation.modulation\n",
      "audio_model.blocks.1.self_attn.q.weight\n",
      "audio_model.blocks.1.self_attn.q.bias\n",
      "audio_model.blocks.1.self_attn.k.weight\n",
      "audio_model.blocks.1.self_attn.k.bias\n",
      "audio_model.blocks.1.self_attn.v.weight\n",
      "audio_model.blocks.1.self_attn.v.bias\n",
      "audio_model.blocks.1.self_attn.o.weight\n",
      "audio_model.blocks.1.self_attn.o.bias\n",
      "audio_model.blocks.1.self_attn.norm_q.weight\n",
      "audio_model.blocks.1.self_attn.norm_k.weight\n",
      "audio_model.blocks.1.norm3.weight\n",
      "audio_model.blocks.1.norm3.bias\n",
      "audio_model.blocks.1.cross_attn.q.weight\n",
      "audio_model.blocks.1.cross_attn.q.bias\n",
      "audio_model.blocks.1.cross_attn.k.weight\n",
      "audio_model.blocks.1.cross_attn.k.bias\n",
      "audio_model.blocks.1.cross_attn.v.weight\n",
      "audio_model.blocks.1.cross_attn.v.bias\n",
      "audio_model.blocks.1.cross_attn.o.weight\n",
      "audio_model.blocks.1.cross_attn.o.bias\n",
      "audio_model.blocks.1.cross_attn.norm_q.weight\n",
      "audio_model.blocks.1.cross_attn.norm_k.weight\n",
      "audio_model.blocks.1.cross_attn.k_fusion.weight\n",
      "audio_model.blocks.1.cross_attn.k_fusion.bias\n",
      "audio_model.blocks.1.cross_attn.v_fusion.weight\n",
      "audio_model.blocks.1.cross_attn.v_fusion.bias\n",
      "audio_model.blocks.1.cross_attn.pre_attn_norm_fusion.weight\n",
      "audio_model.blocks.1.cross_attn.pre_attn_norm_fusion.bias\n",
      "audio_model.blocks.1.cross_attn.norm_k_fusion.weight\n",
      "audio_model.blocks.1.ffn.0.weight\n",
      "audio_model.blocks.1.ffn.0.bias\n",
      "audio_model.blocks.1.ffn.2.weight\n",
      "audio_model.blocks.1.ffn.2.bias\n",
      "audio_model.blocks.1.modulation.modulation\n",
      "audio_model.blocks.2.self_attn.q.weight\n",
      "audio_model.blocks.2.self_attn.q.bias\n",
      "audio_model.blocks.2.self_attn.k.weight\n",
      "audio_model.blocks.2.self_attn.k.bias\n",
      "audio_model.blocks.2.self_attn.v.weight\n",
      "audio_model.blocks.2.self_attn.v.bias\n",
      "audio_model.blocks.2.self_attn.o.weight\n",
      "audio_model.blocks.2.self_attn.o.bias\n",
      "audio_model.blocks.2.self_attn.norm_q.weight\n",
      "audio_model.blocks.2.self_attn.norm_k.weight\n",
      "audio_model.blocks.2.norm3.weight\n",
      "audio_model.blocks.2.norm3.bias\n",
      "audio_model.blocks.2.cross_attn.q.weight\n",
      "audio_model.blocks.2.cross_attn.q.bias\n",
      "audio_model.blocks.2.cross_attn.k.weight\n",
      "audio_model.blocks.2.cross_attn.k.bias\n",
      "audio_model.blocks.2.cross_attn.v.weight\n",
      "audio_model.blocks.2.cross_attn.v.bias\n",
      "audio_model.blocks.2.cross_attn.o.weight\n",
      "audio_model.blocks.2.cross_attn.o.bias\n",
      "audio_model.blocks.2.cross_attn.norm_q.weight\n",
      "audio_model.blocks.2.cross_attn.norm_k.weight\n",
      "audio_model.blocks.2.cross_attn.k_fusion.weight\n",
      "audio_model.blocks.2.cross_attn.k_fusion.bias\n",
      "audio_model.blocks.2.cross_attn.v_fusion.weight\n",
      "audio_model.blocks.2.cross_attn.v_fusion.bias\n",
      "audio_model.blocks.2.cross_attn.pre_attn_norm_fusion.weight\n",
      "audio_model.blocks.2.cross_attn.pre_attn_norm_fusion.bias\n",
      "audio_model.blocks.2.cross_attn.norm_k_fusion.weight\n",
      "audio_model.blocks.2.ffn.0.weight\n",
      "audio_model.blocks.2.ffn.0.bias\n",
      "audio_model.blocks.2.ffn.2.weight\n",
      "audio_model.blocks.2.ffn.2.bias\n",
      "audio_model.blocks.2.modulation.modulation\n",
      "audio_model.blocks.3.self_attn.q.weight\n",
      "audio_model.blocks.3.self_attn.q.bias\n",
      "audio_model.blocks.3.self_attn.k.weight\n",
      "audio_model.blocks.3.self_attn.k.bias\n",
      "audio_model.blocks.3.self_attn.v.weight\n",
      "audio_model.blocks.3.self_attn.v.bias\n",
      "audio_model.blocks.3.self_attn.o.weight\n",
      "audio_model.blocks.3.self_attn.o.bias\n",
      "audio_model.blocks.3.self_attn.norm_q.weight\n",
      "audio_model.blocks.3.self_attn.norm_k.weight\n",
      "audio_model.blocks.3.norm3.weight\n",
      "audio_model.blocks.3.norm3.bias\n",
      "audio_model.blocks.3.cross_attn.q.weight\n",
      "audio_model.blocks.3.cross_attn.q.bias\n",
      "audio_model.blocks.3.cross_attn.k.weight\n",
      "audio_model.blocks.3.cross_attn.k.bias\n",
      "audio_model.blocks.3.cross_attn.v.weight\n",
      "audio_model.blocks.3.cross_attn.v.bias\n",
      "audio_model.blocks.3.cross_attn.o.weight\n",
      "audio_model.blocks.3.cross_attn.o.bias\n",
      "audio_model.blocks.3.cross_attn.norm_q.weight\n",
      "audio_model.blocks.3.cross_attn.norm_k.weight\n",
      "audio_model.blocks.3.cross_attn.k_fusion.weight\n",
      "audio_model.blocks.3.cross_attn.k_fusion.bias\n",
      "audio_model.blocks.3.cross_attn.v_fusion.weight\n",
      "audio_model.blocks.3.cross_attn.v_fusion.bias\n",
      "audio_model.blocks.3.cross_attn.pre_attn_norm_fusion.weight\n",
      "audio_model.blocks.3.cross_attn.pre_attn_norm_fusion.bias\n",
      "audio_model.blocks.3.cross_attn.norm_k_fusion.weight\n",
      "audio_model.blocks.3.ffn.0.weight\n",
      "audio_model.blocks.3.ffn.0.bias\n",
      "audio_model.blocks.3.ffn.2.weight\n",
      "audio_model.blocks.3.ffn.2.bias\n",
      "audio_model.blocks.3.modulation.modulation\n",
      "audio_model.blocks.4.self_attn.q.weight\n",
      "audio_model.blocks.4.self_attn.q.bias\n",
      "audio_model.blocks.4.self_attn.k.weight\n",
      "audio_model.blocks.4.self_attn.k.bias\n",
      "audio_model.blocks.4.self_attn.v.weight\n",
      "audio_model.blocks.4.self_attn.v.bias\n",
      "audio_model.blocks.4.self_attn.o.weight\n",
      "audio_model.blocks.4.self_attn.o.bias\n",
      "audio_model.blocks.4.self_attn.norm_q.weight\n",
      "audio_model.blocks.4.self_attn.norm_k.weight\n",
      "audio_model.blocks.4.norm3.weight\n",
      "audio_model.blocks.4.norm3.bias\n",
      "audio_model.blocks.4.cross_attn.q.weight\n",
      "audio_model.blocks.4.cross_attn.q.bias\n",
      "audio_model.blocks.4.cross_attn.k.weight\n",
      "audio_model.blocks.4.cross_attn.k.bias\n",
      "audio_model.blocks.4.cross_attn.v.weight\n",
      "audio_model.blocks.4.cross_attn.v.bias\n",
      "audio_model.blocks.4.cross_attn.o.weight\n",
      "audio_model.blocks.4.cross_attn.o.bias\n",
      "audio_model.blocks.4.cross_attn.norm_q.weight\n",
      "audio_model.blocks.4.cross_attn.norm_k.weight\n",
      "audio_model.blocks.4.cross_attn.k_fusion.weight\n",
      "audio_model.blocks.4.cross_attn.k_fusion.bias\n",
      "audio_model.blocks.4.cross_attn.v_fusion.weight\n",
      "audio_model.blocks.4.cross_attn.v_fusion.bias\n",
      "audio_model.blocks.4.cross_attn.pre_attn_norm_fusion.weight\n",
      "audio_model.blocks.4.cross_attn.pre_attn_norm_fusion.bias\n",
      "audio_model.blocks.4.cross_attn.norm_k_fusion.weight\n",
      "audio_model.blocks.4.ffn.0.weight\n",
      "audio_model.blocks.4.ffn.0.bias\n",
      "audio_model.blocks.4.ffn.2.weight\n",
      "audio_model.blocks.4.ffn.2.bias\n",
      "audio_model.blocks.4.modulation.modulation\n",
      "audio_model.blocks.5.self_attn.q.weight\n",
      "audio_model.blocks.5.self_attn.q.bias\n",
      "audio_model.blocks.5.self_attn.k.weight\n",
      "audio_model.blocks.5.self_attn.k.bias\n",
      "audio_model.blocks.5.self_attn.v.weight\n",
      "audio_model.blocks.5.self_attn.v.bias\n",
      "audio_model.blocks.5.self_attn.o.weight\n",
      "audio_model.blocks.5.self_attn.o.bias\n",
      "audio_model.blocks.5.self_attn.norm_q.weight\n",
      "audio_model.blocks.5.self_attn.norm_k.weight\n",
      "audio_model.blocks.5.norm3.weight\n",
      "audio_model.blocks.5.norm3.bias\n",
      "audio_model.blocks.5.cross_attn.q.weight\n",
      "audio_model.blocks.5.cross_attn.q.bias\n",
      "audio_model.blocks.5.cross_attn.k.weight\n",
      "audio_model.blocks.5.cross_attn.k.bias\n",
      "audio_model.blocks.5.cross_attn.v.weight\n",
      "audio_model.blocks.5.cross_attn.v.bias\n",
      "audio_model.blocks.5.cross_attn.o.weight\n",
      "audio_model.blocks.5.cross_attn.o.bias\n",
      "audio_model.blocks.5.cross_attn.norm_q.weight\n",
      "audio_model.blocks.5.cross_attn.norm_k.weight\n",
      "audio_model.blocks.5.cross_attn.k_fusion.weight\n",
      "audio_model.blocks.5.cross_attn.k_fusion.bias\n",
      "audio_model.blocks.5.cross_attn.v_fusion.weight\n",
      "audio_model.blocks.5.cross_attn.v_fusion.bias\n",
      "audio_model.blocks.5.cross_attn.pre_attn_norm_fusion.weight\n",
      "audio_model.blocks.5.cross_attn.pre_attn_norm_fusion.bias\n",
      "audio_model.blocks.5.cross_attn.norm_k_fusion.weight\n",
      "audio_model.blocks.5.ffn.0.weight\n",
      "audio_model.blocks.5.ffn.0.bias\n",
      "audio_model.blocks.5.ffn.2.weight\n",
      "audio_model.blocks.5.ffn.2.bias\n",
      "audio_model.blocks.5.modulation.modulation\n",
      "audio_model.blocks.6.self_attn.q.weight\n",
      "audio_model.blocks.6.self_attn.q.bias\n",
      "audio_model.blocks.6.self_attn.k.weight\n",
      "audio_model.blocks.6.self_attn.k.bias\n",
      "audio_model.blocks.6.self_attn.v.weight\n",
      "audio_model.blocks.6.self_attn.v.bias\n",
      "audio_model.blocks.6.self_attn.o.weight\n",
      "audio_model.blocks.6.self_attn.o.bias\n",
      "audio_model.blocks.6.self_attn.norm_q.weight\n",
      "audio_model.blocks.6.self_attn.norm_k.weight\n",
      "audio_model.blocks.6.norm3.weight\n",
      "audio_model.blocks.6.norm3.bias\n",
      "audio_model.blocks.6.cross_attn.q.weight\n",
      "audio_model.blocks.6.cross_attn.q.bias\n",
      "audio_model.blocks.6.cross_attn.k.weight\n",
      "audio_model.blocks.6.cross_attn.k.bias\n",
      "audio_model.blocks.6.cross_attn.v.weight\n",
      "audio_model.blocks.6.cross_attn.v.bias\n",
      "audio_model.blocks.6.cross_attn.o.weight\n",
      "audio_model.blocks.6.cross_attn.o.bias\n",
      "audio_model.blocks.6.cross_attn.norm_q.weight\n",
      "audio_model.blocks.6.cross_attn.norm_k.weight\n",
      "audio_model.blocks.6.cross_attn.k_fusion.weight\n",
      "audio_model.blocks.6.cross_attn.k_fusion.bias\n",
      "audio_model.blocks.6.cross_attn.v_fusion.weight\n",
      "audio_model.blocks.6.cross_attn.v_fusion.bias\n",
      "audio_model.blocks.6.cross_attn.pre_attn_norm_fusion.weight\n",
      "audio_model.blocks.6.cross_attn.pre_attn_norm_fusion.bias\n",
      "audio_model.blocks.6.cross_attn.norm_k_fusion.weight\n",
      "audio_model.blocks.6.ffn.0.weight\n",
      "audio_model.blocks.6.ffn.0.bias\n",
      "audio_model.blocks.6.ffn.2.weight\n",
      "audio_model.blocks.6.ffn.2.bias\n",
      "audio_model.blocks.6.modulation.modulation\n",
      "audio_model.blocks.7.self_attn.q.weight\n",
      "audio_model.blocks.7.self_attn.q.bias\n",
      "audio_model.blocks.7.self_attn.k.weight\n",
      "audio_model.blocks.7.self_attn.k.bias\n",
      "audio_model.blocks.7.self_attn.v.weight\n",
      "audio_model.blocks.7.self_attn.v.bias\n",
      "audio_model.blocks.7.self_attn.o.weight\n",
      "audio_model.blocks.7.self_attn.o.bias\n",
      "audio_model.blocks.7.self_attn.norm_q.weight\n",
      "audio_model.blocks.7.self_attn.norm_k.weight\n",
      "audio_model.blocks.7.norm3.weight\n",
      "audio_model.blocks.7.norm3.bias\n",
      "audio_model.blocks.7.cross_attn.q.weight\n",
      "audio_model.blocks.7.cross_attn.q.bias\n",
      "audio_model.blocks.7.cross_attn.k.weight\n",
      "audio_model.blocks.7.cross_attn.k.bias\n",
      "audio_model.blocks.7.cross_attn.v.weight\n",
      "audio_model.blocks.7.cross_attn.v.bias\n",
      "audio_model.blocks.7.cross_attn.o.weight\n",
      "audio_model.blocks.7.cross_attn.o.bias\n",
      "audio_model.blocks.7.cross_attn.norm_q.weight\n",
      "audio_model.blocks.7.cross_attn.norm_k.weight\n",
      "audio_model.blocks.7.cross_attn.k_fusion.weight\n",
      "audio_model.blocks.7.cross_attn.k_fusion.bias\n",
      "audio_model.blocks.7.cross_attn.v_fusion.weight\n",
      "audio_model.blocks.7.cross_attn.v_fusion.bias\n",
      "audio_model.blocks.7.cross_attn.pre_attn_norm_fusion.weight\n",
      "audio_model.blocks.7.cross_attn.pre_attn_norm_fusion.bias\n",
      "audio_model.blocks.7.cross_attn.norm_k_fusion.weight\n",
      "audio_model.blocks.7.ffn.0.weight\n",
      "audio_model.blocks.7.ffn.0.bias\n",
      "audio_model.blocks.7.ffn.2.weight\n",
      "audio_model.blocks.7.ffn.2.bias\n",
      "audio_model.blocks.7.modulation.modulation\n",
      "audio_model.blocks.8.self_attn.q.weight\n",
      "audio_model.blocks.8.self_attn.q.bias\n",
      "audio_model.blocks.8.self_attn.k.weight\n",
      "audio_model.blocks.8.self_attn.k.bias\n",
      "audio_model.blocks.8.self_attn.v.weight\n",
      "audio_model.blocks.8.self_attn.v.bias\n",
      "audio_model.blocks.8.self_attn.o.weight\n",
      "audio_model.blocks.8.self_attn.o.bias\n",
      "audio_model.blocks.8.self_attn.norm_q.weight\n",
      "audio_model.blocks.8.self_attn.norm_k.weight\n",
      "audio_model.blocks.8.norm3.weight\n",
      "audio_model.blocks.8.norm3.bias\n",
      "audio_model.blocks.8.cross_attn.q.weight\n",
      "audio_model.blocks.8.cross_attn.q.bias\n",
      "audio_model.blocks.8.cross_attn.k.weight\n",
      "audio_model.blocks.8.cross_attn.k.bias\n",
      "audio_model.blocks.8.cross_attn.v.weight\n",
      "audio_model.blocks.8.cross_attn.v.bias\n",
      "audio_model.blocks.8.cross_attn.o.weight\n",
      "audio_model.blocks.8.cross_attn.o.bias\n",
      "audio_model.blocks.8.cross_attn.norm_q.weight\n",
      "audio_model.blocks.8.cross_attn.norm_k.weight\n",
      "audio_model.blocks.8.cross_attn.k_fusion.weight\n",
      "audio_model.blocks.8.cross_attn.k_fusion.bias\n",
      "audio_model.blocks.8.cross_attn.v_fusion.weight\n",
      "audio_model.blocks.8.cross_attn.v_fusion.bias\n",
      "audio_model.blocks.8.cross_attn.pre_attn_norm_fusion.weight\n",
      "audio_model.blocks.8.cross_attn.pre_attn_norm_fusion.bias\n",
      "audio_model.blocks.8.cross_attn.norm_k_fusion.weight\n",
      "audio_model.blocks.8.ffn.0.weight\n",
      "audio_model.blocks.8.ffn.0.bias\n",
      "audio_model.blocks.8.ffn.2.weight\n",
      "audio_model.blocks.8.ffn.2.bias\n",
      "audio_model.blocks.8.modulation.modulation\n",
      "audio_model.blocks.9.self_attn.q.weight\n",
      "audio_model.blocks.9.self_attn.q.bias\n",
      "audio_model.blocks.9.self_attn.k.weight\n",
      "audio_model.blocks.9.self_attn.k.bias\n",
      "audio_model.blocks.9.self_attn.v.weight\n",
      "audio_model.blocks.9.self_attn.v.bias\n",
      "audio_model.blocks.9.self_attn.o.weight\n",
      "audio_model.blocks.9.self_attn.o.bias\n",
      "audio_model.blocks.9.self_attn.norm_q.weight\n",
      "audio_model.blocks.9.self_attn.norm_k.weight\n",
      "audio_model.blocks.9.norm3.weight\n",
      "audio_model.blocks.9.norm3.bias\n",
      "audio_model.blocks.9.cross_attn.q.weight\n",
      "audio_model.blocks.9.cross_attn.q.bias\n",
      "audio_model.blocks.9.cross_attn.k.weight\n",
      "audio_model.blocks.9.cross_attn.k.bias\n",
      "audio_model.blocks.9.cross_attn.v.weight\n",
      "audio_model.blocks.9.cross_attn.v.bias\n",
      "audio_model.blocks.9.cross_attn.o.weight\n",
      "audio_model.blocks.9.cross_attn.o.bias\n",
      "audio_model.blocks.9.cross_attn.norm_q.weight\n",
      "audio_model.blocks.9.cross_attn.norm_k.weight\n",
      "audio_model.blocks.9.cross_attn.k_fusion.weight\n",
      "audio_model.blocks.9.cross_attn.k_fusion.bias\n",
      "audio_model.blocks.9.cross_attn.v_fusion.weight\n",
      "audio_model.blocks.9.cross_attn.v_fusion.bias\n",
      "audio_model.blocks.9.cross_attn.pre_attn_norm_fusion.weight\n",
      "audio_model.blocks.9.cross_attn.pre_attn_norm_fusion.bias\n",
      "audio_model.blocks.9.cross_attn.norm_k_fusion.weight\n",
      "audio_model.blocks.9.ffn.0.weight\n",
      "audio_model.blocks.9.ffn.0.bias\n",
      "audio_model.blocks.9.ffn.2.weight\n",
      "audio_model.blocks.9.ffn.2.bias\n",
      "audio_model.blocks.9.modulation.modulation\n",
      "audio_model.blocks.10.self_attn.q.weight\n",
      "audio_model.blocks.10.self_attn.q.bias\n",
      "audio_model.blocks.10.self_attn.k.weight\n",
      "audio_model.blocks.10.self_attn.k.bias\n",
      "audio_model.blocks.10.self_attn.v.weight\n",
      "audio_model.blocks.10.self_attn.v.bias\n",
      "audio_model.blocks.10.self_attn.o.weight\n",
      "audio_model.blocks.10.self_attn.o.bias\n",
      "audio_model.blocks.10.self_attn.norm_q.weight\n",
      "audio_model.blocks.10.self_attn.norm_k.weight\n",
      "audio_model.blocks.10.norm3.weight\n",
      "audio_model.blocks.10.norm3.bias\n",
      "audio_model.blocks.10.cross_attn.q.weight\n",
      "audio_model.blocks.10.cross_attn.q.bias\n",
      "audio_model.blocks.10.cross_attn.k.weight\n",
      "audio_model.blocks.10.cross_attn.k.bias\n",
      "audio_model.blocks.10.cross_attn.v.weight\n",
      "audio_model.blocks.10.cross_attn.v.bias\n",
      "audio_model.blocks.10.cross_attn.o.weight\n",
      "audio_model.blocks.10.cross_attn.o.bias\n",
      "audio_model.blocks.10.cross_attn.norm_q.weight\n",
      "audio_model.blocks.10.cross_attn.norm_k.weight\n",
      "audio_model.blocks.10.cross_attn.k_fusion.weight\n",
      "audio_model.blocks.10.cross_attn.k_fusion.bias\n",
      "audio_model.blocks.10.cross_attn.v_fusion.weight\n",
      "audio_model.blocks.10.cross_attn.v_fusion.bias\n",
      "audio_model.blocks.10.cross_attn.pre_attn_norm_fusion.weight\n",
      "audio_model.blocks.10.cross_attn.pre_attn_norm_fusion.bias\n",
      "audio_model.blocks.10.cross_attn.norm_k_fusion.weight\n",
      "audio_model.blocks.10.ffn.0.weight\n",
      "audio_model.blocks.10.ffn.0.bias\n",
      "audio_model.blocks.10.ffn.2.weight\n",
      "audio_model.blocks.10.ffn.2.bias\n",
      "audio_model.blocks.10.modulation.modulation\n",
      "audio_model.blocks.11.self_attn.q.weight\n",
      "audio_model.blocks.11.self_attn.q.bias\n",
      "audio_model.blocks.11.self_attn.k.weight\n",
      "audio_model.blocks.11.self_attn.k.bias\n",
      "audio_model.blocks.11.self_attn.v.weight\n",
      "audio_model.blocks.11.self_attn.v.bias\n",
      "audio_model.blocks.11.self_attn.o.weight\n",
      "audio_model.blocks.11.self_attn.o.bias\n",
      "audio_model.blocks.11.self_attn.norm_q.weight\n",
      "audio_model.blocks.11.self_attn.norm_k.weight\n",
      "audio_model.blocks.11.norm3.weight\n",
      "audio_model.blocks.11.norm3.bias\n",
      "audio_model.blocks.11.cross_attn.q.weight\n",
      "audio_model.blocks.11.cross_attn.q.bias\n",
      "audio_model.blocks.11.cross_attn.k.weight\n",
      "audio_model.blocks.11.cross_attn.k.bias\n",
      "audio_model.blocks.11.cross_attn.v.weight\n",
      "audio_model.blocks.11.cross_attn.v.bias\n",
      "audio_model.blocks.11.cross_attn.o.weight\n",
      "audio_model.blocks.11.cross_attn.o.bias\n",
      "audio_model.blocks.11.cross_attn.norm_q.weight\n",
      "audio_model.blocks.11.cross_attn.norm_k.weight\n",
      "audio_model.blocks.11.cross_attn.k_fusion.weight\n",
      "audio_model.blocks.11.cross_attn.k_fusion.bias\n",
      "audio_model.blocks.11.cross_attn.v_fusion.weight\n",
      "audio_model.blocks.11.cross_attn.v_fusion.bias\n",
      "audio_model.blocks.11.cross_attn.pre_attn_norm_fusion.weight\n",
      "audio_model.blocks.11.cross_attn.pre_attn_norm_fusion.bias\n",
      "audio_model.blocks.11.cross_attn.norm_k_fusion.weight\n",
      "audio_model.blocks.11.ffn.0.weight\n",
      "audio_model.blocks.11.ffn.0.bias\n",
      "audio_model.blocks.11.ffn.2.weight\n",
      "audio_model.blocks.11.ffn.2.bias\n",
      "audio_model.blocks.11.modulation.modulation\n",
      "audio_model.blocks.12.self_attn.q.weight\n",
      "audio_model.blocks.12.self_attn.q.bias\n",
      "audio_model.blocks.12.self_attn.k.weight\n",
      "audio_model.blocks.12.self_attn.k.bias\n",
      "audio_model.blocks.12.self_attn.v.weight\n",
      "audio_model.blocks.12.self_attn.v.bias\n",
      "audio_model.blocks.12.self_attn.o.weight\n",
      "audio_model.blocks.12.self_attn.o.bias\n",
      "audio_model.blocks.12.self_attn.norm_q.weight\n",
      "audio_model.blocks.12.self_attn.norm_k.weight\n",
      "audio_model.blocks.12.norm3.weight\n",
      "audio_model.blocks.12.norm3.bias\n",
      "audio_model.blocks.12.cross_attn.q.weight\n",
      "audio_model.blocks.12.cross_attn.q.bias\n",
      "audio_model.blocks.12.cross_attn.k.weight\n",
      "audio_model.blocks.12.cross_attn.k.bias\n",
      "audio_model.blocks.12.cross_attn.v.weight\n",
      "audio_model.blocks.12.cross_attn.v.bias\n",
      "audio_model.blocks.12.cross_attn.o.weight\n",
      "audio_model.blocks.12.cross_attn.o.bias\n",
      "audio_model.blocks.12.cross_attn.norm_q.weight\n",
      "audio_model.blocks.12.cross_attn.norm_k.weight\n",
      "audio_model.blocks.12.cross_attn.k_fusion.weight\n",
      "audio_model.blocks.12.cross_attn.k_fusion.bias\n",
      "audio_model.blocks.12.cross_attn.v_fusion.weight\n",
      "audio_model.blocks.12.cross_attn.v_fusion.bias\n",
      "audio_model.blocks.12.cross_attn.pre_attn_norm_fusion.weight\n",
      "audio_model.blocks.12.cross_attn.pre_attn_norm_fusion.bias\n",
      "audio_model.blocks.12.cross_attn.norm_k_fusion.weight\n",
      "audio_model.blocks.12.ffn.0.weight\n",
      "audio_model.blocks.12.ffn.0.bias\n",
      "audio_model.blocks.12.ffn.2.weight\n",
      "audio_model.blocks.12.ffn.2.bias\n",
      "audio_model.blocks.12.modulation.modulation\n",
      "audio_model.blocks.13.self_attn.q.weight\n",
      "audio_model.blocks.13.self_attn.q.bias\n",
      "audio_model.blocks.13.self_attn.k.weight\n",
      "audio_model.blocks.13.self_attn.k.bias\n",
      "audio_model.blocks.13.self_attn.v.weight\n",
      "audio_model.blocks.13.self_attn.v.bias\n",
      "audio_model.blocks.13.self_attn.o.weight\n",
      "audio_model.blocks.13.self_attn.o.bias\n",
      "audio_model.blocks.13.self_attn.norm_q.weight\n",
      "audio_model.blocks.13.self_attn.norm_k.weight\n",
      "audio_model.blocks.13.norm3.weight\n",
      "audio_model.blocks.13.norm3.bias\n",
      "audio_model.blocks.13.cross_attn.q.weight\n",
      "audio_model.blocks.13.cross_attn.q.bias\n",
      "audio_model.blocks.13.cross_attn.k.weight\n",
      "audio_model.blocks.13.cross_attn.k.bias\n",
      "audio_model.blocks.13.cross_attn.v.weight\n",
      "audio_model.blocks.13.cross_attn.v.bias\n",
      "audio_model.blocks.13.cross_attn.o.weight\n",
      "audio_model.blocks.13.cross_attn.o.bias\n",
      "audio_model.blocks.13.cross_attn.norm_q.weight\n",
      "audio_model.blocks.13.cross_attn.norm_k.weight\n",
      "audio_model.blocks.13.cross_attn.k_fusion.weight\n",
      "audio_model.blocks.13.cross_attn.k_fusion.bias\n",
      "audio_model.blocks.13.cross_attn.v_fusion.weight\n",
      "audio_model.blocks.13.cross_attn.v_fusion.bias\n",
      "audio_model.blocks.13.cross_attn.pre_attn_norm_fusion.weight\n",
      "audio_model.blocks.13.cross_attn.pre_attn_norm_fusion.bias\n",
      "audio_model.blocks.13.cross_attn.norm_k_fusion.weight\n",
      "audio_model.blocks.13.ffn.0.weight\n",
      "audio_model.blocks.13.ffn.0.bias\n",
      "audio_model.blocks.13.ffn.2.weight\n",
      "audio_model.blocks.13.ffn.2.bias\n",
      "audio_model.blocks.13.modulation.modulation\n",
      "audio_model.blocks.14.self_attn.q.weight\n",
      "audio_model.blocks.14.self_attn.q.bias\n",
      "audio_model.blocks.14.self_attn.k.weight\n",
      "audio_model.blocks.14.self_attn.k.bias\n",
      "audio_model.blocks.14.self_attn.v.weight\n",
      "audio_model.blocks.14.self_attn.v.bias\n",
      "audio_model.blocks.14.self_attn.o.weight\n",
      "audio_model.blocks.14.self_attn.o.bias\n",
      "audio_model.blocks.14.self_attn.norm_q.weight\n",
      "audio_model.blocks.14.self_attn.norm_k.weight\n",
      "audio_model.blocks.14.norm3.weight\n",
      "audio_model.blocks.14.norm3.bias\n",
      "audio_model.blocks.14.cross_attn.q.weight\n",
      "audio_model.blocks.14.cross_attn.q.bias\n",
      "audio_model.blocks.14.cross_attn.k.weight\n",
      "audio_model.blocks.14.cross_attn.k.bias\n",
      "audio_model.blocks.14.cross_attn.v.weight\n",
      "audio_model.blocks.14.cross_attn.v.bias\n",
      "audio_model.blocks.14.cross_attn.o.weight\n",
      "audio_model.blocks.14.cross_attn.o.bias\n",
      "audio_model.blocks.14.cross_attn.norm_q.weight\n",
      "audio_model.blocks.14.cross_attn.norm_k.weight\n",
      "audio_model.blocks.14.cross_attn.k_fusion.weight\n",
      "audio_model.blocks.14.cross_attn.k_fusion.bias\n",
      "audio_model.blocks.14.cross_attn.v_fusion.weight\n",
      "audio_model.blocks.14.cross_attn.v_fusion.bias\n",
      "audio_model.blocks.14.cross_attn.pre_attn_norm_fusion.weight\n",
      "audio_model.blocks.14.cross_attn.pre_attn_norm_fusion.bias\n",
      "audio_model.blocks.14.cross_attn.norm_k_fusion.weight\n",
      "audio_model.blocks.14.ffn.0.weight\n",
      "audio_model.blocks.14.ffn.0.bias\n",
      "audio_model.blocks.14.ffn.2.weight\n",
      "audio_model.blocks.14.ffn.2.bias\n",
      "audio_model.blocks.14.modulation.modulation\n",
      "audio_model.blocks.15.self_attn.q.weight\n",
      "audio_model.blocks.15.self_attn.q.bias\n",
      "audio_model.blocks.15.self_attn.k.weight\n",
      "audio_model.blocks.15.self_attn.k.bias\n",
      "audio_model.blocks.15.self_attn.v.weight\n",
      "audio_model.blocks.15.self_attn.v.bias\n",
      "audio_model.blocks.15.self_attn.o.weight\n",
      "audio_model.blocks.15.self_attn.o.bias\n",
      "audio_model.blocks.15.self_attn.norm_q.weight\n",
      "audio_model.blocks.15.self_attn.norm_k.weight\n",
      "audio_model.blocks.15.norm3.weight\n",
      "audio_model.blocks.15.norm3.bias\n",
      "audio_model.blocks.15.cross_attn.q.weight\n",
      "audio_model.blocks.15.cross_attn.q.bias\n",
      "audio_model.blocks.15.cross_attn.k.weight\n",
      "audio_model.blocks.15.cross_attn.k.bias\n",
      "audio_model.blocks.15.cross_attn.v.weight\n",
      "audio_model.blocks.15.cross_attn.v.bias\n",
      "audio_model.blocks.15.cross_attn.o.weight\n",
      "audio_model.blocks.15.cross_attn.o.bias\n",
      "audio_model.blocks.15.cross_attn.norm_q.weight\n",
      "audio_model.blocks.15.cross_attn.norm_k.weight\n",
      "audio_model.blocks.15.cross_attn.k_fusion.weight\n",
      "audio_model.blocks.15.cross_attn.k_fusion.bias\n",
      "audio_model.blocks.15.cross_attn.v_fusion.weight\n",
      "audio_model.blocks.15.cross_attn.v_fusion.bias\n",
      "audio_model.blocks.15.cross_attn.pre_attn_norm_fusion.weight\n",
      "audio_model.blocks.15.cross_attn.pre_attn_norm_fusion.bias\n",
      "audio_model.blocks.15.cross_attn.norm_k_fusion.weight\n",
      "audio_model.blocks.15.ffn.0.weight\n",
      "audio_model.blocks.15.ffn.0.bias\n",
      "audio_model.blocks.15.ffn.2.weight\n",
      "audio_model.blocks.15.ffn.2.bias\n",
      "audio_model.blocks.15.modulation.modulation\n",
      "audio_model.blocks.16.self_attn.q.weight\n",
      "audio_model.blocks.16.self_attn.q.bias\n",
      "audio_model.blocks.16.self_attn.k.weight\n",
      "audio_model.blocks.16.self_attn.k.bias\n",
      "audio_model.blocks.16.self_attn.v.weight\n",
      "audio_model.blocks.16.self_attn.v.bias\n",
      "audio_model.blocks.16.self_attn.o.weight\n",
      "audio_model.blocks.16.self_attn.o.bias\n",
      "audio_model.blocks.16.self_attn.norm_q.weight\n",
      "audio_model.blocks.16.self_attn.norm_k.weight\n",
      "audio_model.blocks.16.norm3.weight\n",
      "audio_model.blocks.16.norm3.bias\n",
      "audio_model.blocks.16.cross_attn.q.weight\n",
      "audio_model.blocks.16.cross_attn.q.bias\n",
      "audio_model.blocks.16.cross_attn.k.weight\n",
      "audio_model.blocks.16.cross_attn.k.bias\n",
      "audio_model.blocks.16.cross_attn.v.weight\n",
      "audio_model.blocks.16.cross_attn.v.bias\n",
      "audio_model.blocks.16.cross_attn.o.weight\n",
      "audio_model.blocks.16.cross_attn.o.bias\n",
      "audio_model.blocks.16.cross_attn.norm_q.weight\n",
      "audio_model.blocks.16.cross_attn.norm_k.weight\n",
      "audio_model.blocks.16.cross_attn.k_fusion.weight\n",
      "audio_model.blocks.16.cross_attn.k_fusion.bias\n",
      "audio_model.blocks.16.cross_attn.v_fusion.weight\n",
      "audio_model.blocks.16.cross_attn.v_fusion.bias\n",
      "audio_model.blocks.16.cross_attn.pre_attn_norm_fusion.weight\n",
      "audio_model.blocks.16.cross_attn.pre_attn_norm_fusion.bias\n",
      "audio_model.blocks.16.cross_attn.norm_k_fusion.weight\n",
      "audio_model.blocks.16.ffn.0.weight\n",
      "audio_model.blocks.16.ffn.0.bias\n",
      "audio_model.blocks.16.ffn.2.weight\n",
      "audio_model.blocks.16.ffn.2.bias\n",
      "audio_model.blocks.16.modulation.modulation\n",
      "audio_model.blocks.17.self_attn.q.weight\n",
      "audio_model.blocks.17.self_attn.q.bias\n",
      "audio_model.blocks.17.self_attn.k.weight\n",
      "audio_model.blocks.17.self_attn.k.bias\n",
      "audio_model.blocks.17.self_attn.v.weight\n",
      "audio_model.blocks.17.self_attn.v.bias\n",
      "audio_model.blocks.17.self_attn.o.weight\n",
      "audio_model.blocks.17.self_attn.o.bias\n",
      "audio_model.blocks.17.self_attn.norm_q.weight\n",
      "audio_model.blocks.17.self_attn.norm_k.weight\n",
      "audio_model.blocks.17.norm3.weight\n",
      "audio_model.blocks.17.norm3.bias\n",
      "audio_model.blocks.17.cross_attn.q.weight\n",
      "audio_model.blocks.17.cross_attn.q.bias\n",
      "audio_model.blocks.17.cross_attn.k.weight\n",
      "audio_model.blocks.17.cross_attn.k.bias\n",
      "audio_model.blocks.17.cross_attn.v.weight\n",
      "audio_model.blocks.17.cross_attn.v.bias\n",
      "audio_model.blocks.17.cross_attn.o.weight\n",
      "audio_model.blocks.17.cross_attn.o.bias\n",
      "audio_model.blocks.17.cross_attn.norm_q.weight\n",
      "audio_model.blocks.17.cross_attn.norm_k.weight\n",
      "audio_model.blocks.17.cross_attn.k_fusion.weight\n",
      "audio_model.blocks.17.cross_attn.k_fusion.bias\n",
      "audio_model.blocks.17.cross_attn.v_fusion.weight\n",
      "audio_model.blocks.17.cross_attn.v_fusion.bias\n",
      "audio_model.blocks.17.cross_attn.pre_attn_norm_fusion.weight\n",
      "audio_model.blocks.17.cross_attn.pre_attn_norm_fusion.bias\n",
      "audio_model.blocks.17.cross_attn.norm_k_fusion.weight\n",
      "audio_model.blocks.17.ffn.0.weight\n",
      "audio_model.blocks.17.ffn.0.bias\n",
      "audio_model.blocks.17.ffn.2.weight\n",
      "audio_model.blocks.17.ffn.2.bias\n",
      "audio_model.blocks.17.modulation.modulation\n",
      "audio_model.blocks.18.self_attn.q.weight\n",
      "audio_model.blocks.18.self_attn.q.bias\n",
      "audio_model.blocks.18.self_attn.k.weight\n",
      "audio_model.blocks.18.self_attn.k.bias\n",
      "audio_model.blocks.18.self_attn.v.weight\n",
      "audio_model.blocks.18.self_attn.v.bias\n",
      "audio_model.blocks.18.self_attn.o.weight\n",
      "audio_model.blocks.18.self_attn.o.bias\n",
      "audio_model.blocks.18.self_attn.norm_q.weight\n",
      "audio_model.blocks.18.self_attn.norm_k.weight\n",
      "audio_model.blocks.18.norm3.weight\n",
      "audio_model.blocks.18.norm3.bias\n",
      "audio_model.blocks.18.cross_attn.q.weight\n",
      "audio_model.blocks.18.cross_attn.q.bias\n",
      "audio_model.blocks.18.cross_attn.k.weight\n",
      "audio_model.blocks.18.cross_attn.k.bias\n",
      "audio_model.blocks.18.cross_attn.v.weight\n",
      "audio_model.blocks.18.cross_attn.v.bias\n",
      "audio_model.blocks.18.cross_attn.o.weight\n",
      "audio_model.blocks.18.cross_attn.o.bias\n",
      "audio_model.blocks.18.cross_attn.norm_q.weight\n",
      "audio_model.blocks.18.cross_attn.norm_k.weight\n",
      "audio_model.blocks.18.cross_attn.k_fusion.weight\n",
      "audio_model.blocks.18.cross_attn.k_fusion.bias\n",
      "audio_model.blocks.18.cross_attn.v_fusion.weight\n",
      "audio_model.blocks.18.cross_attn.v_fusion.bias\n",
      "audio_model.blocks.18.cross_attn.pre_attn_norm_fusion.weight\n",
      "audio_model.blocks.18.cross_attn.pre_attn_norm_fusion.bias\n",
      "audio_model.blocks.18.cross_attn.norm_k_fusion.weight\n",
      "audio_model.blocks.18.ffn.0.weight\n",
      "audio_model.blocks.18.ffn.0.bias\n",
      "audio_model.blocks.18.ffn.2.weight\n",
      "audio_model.blocks.18.ffn.2.bias\n",
      "audio_model.blocks.18.modulation.modulation\n",
      "audio_model.blocks.19.self_attn.q.weight\n",
      "audio_model.blocks.19.self_attn.q.bias\n",
      "audio_model.blocks.19.self_attn.k.weight\n",
      "audio_model.blocks.19.self_attn.k.bias\n",
      "audio_model.blocks.19.self_attn.v.weight\n",
      "audio_model.blocks.19.self_attn.v.bias\n",
      "audio_model.blocks.19.self_attn.o.weight\n",
      "audio_model.blocks.19.self_attn.o.bias\n",
      "audio_model.blocks.19.self_attn.norm_q.weight\n",
      "audio_model.blocks.19.self_attn.norm_k.weight\n",
      "audio_model.blocks.19.norm3.weight\n",
      "audio_model.blocks.19.norm3.bias\n",
      "audio_model.blocks.19.cross_attn.q.weight\n",
      "audio_model.blocks.19.cross_attn.q.bias\n",
      "audio_model.blocks.19.cross_attn.k.weight\n",
      "audio_model.blocks.19.cross_attn.k.bias\n",
      "audio_model.blocks.19.cross_attn.v.weight\n",
      "audio_model.blocks.19.cross_attn.v.bias\n",
      "audio_model.blocks.19.cross_attn.o.weight\n",
      "audio_model.blocks.19.cross_attn.o.bias\n",
      "audio_model.blocks.19.cross_attn.norm_q.weight\n",
      "audio_model.blocks.19.cross_attn.norm_k.weight\n",
      "audio_model.blocks.19.cross_attn.k_fusion.weight\n",
      "audio_model.blocks.19.cross_attn.k_fusion.bias\n",
      "audio_model.blocks.19.cross_attn.v_fusion.weight\n",
      "audio_model.blocks.19.cross_attn.v_fusion.bias\n",
      "audio_model.blocks.19.cross_attn.pre_attn_norm_fusion.weight\n",
      "audio_model.blocks.19.cross_attn.pre_attn_norm_fusion.bias\n",
      "audio_model.blocks.19.cross_attn.norm_k_fusion.weight\n",
      "audio_model.blocks.19.ffn.0.weight\n",
      "audio_model.blocks.19.ffn.0.bias\n",
      "audio_model.blocks.19.ffn.2.weight\n",
      "audio_model.blocks.19.ffn.2.bias\n",
      "audio_model.blocks.19.modulation.modulation\n",
      "audio_model.blocks.20.self_attn.q.weight\n",
      "audio_model.blocks.20.self_attn.q.bias\n",
      "audio_model.blocks.20.self_attn.k.weight\n",
      "audio_model.blocks.20.self_attn.k.bias\n",
      "audio_model.blocks.20.self_attn.v.weight\n",
      "audio_model.blocks.20.self_attn.v.bias\n",
      "audio_model.blocks.20.self_attn.o.weight\n",
      "audio_model.blocks.20.self_attn.o.bias\n",
      "audio_model.blocks.20.self_attn.norm_q.weight\n",
      "audio_model.blocks.20.self_attn.norm_k.weight\n",
      "audio_model.blocks.20.norm3.weight\n",
      "audio_model.blocks.20.norm3.bias\n",
      "audio_model.blocks.20.cross_attn.q.weight\n",
      "audio_model.blocks.20.cross_attn.q.bias\n",
      "audio_model.blocks.20.cross_attn.k.weight\n",
      "audio_model.blocks.20.cross_attn.k.bias\n",
      "audio_model.blocks.20.cross_attn.v.weight\n",
      "audio_model.blocks.20.cross_attn.v.bias\n",
      "audio_model.blocks.20.cross_attn.o.weight\n",
      "audio_model.blocks.20.cross_attn.o.bias\n",
      "audio_model.blocks.20.cross_attn.norm_q.weight\n",
      "audio_model.blocks.20.cross_attn.norm_k.weight\n",
      "audio_model.blocks.20.cross_attn.k_fusion.weight\n",
      "audio_model.blocks.20.cross_attn.k_fusion.bias\n",
      "audio_model.blocks.20.cross_attn.v_fusion.weight\n",
      "audio_model.blocks.20.cross_attn.v_fusion.bias\n",
      "audio_model.blocks.20.cross_attn.pre_attn_norm_fusion.weight\n",
      "audio_model.blocks.20.cross_attn.pre_attn_norm_fusion.bias\n",
      "audio_model.blocks.20.cross_attn.norm_k_fusion.weight\n",
      "audio_model.blocks.20.ffn.0.weight\n",
      "audio_model.blocks.20.ffn.0.bias\n",
      "audio_model.blocks.20.ffn.2.weight\n",
      "audio_model.blocks.20.ffn.2.bias\n",
      "audio_model.blocks.20.modulation.modulation\n",
      "audio_model.blocks.21.self_attn.q.weight\n",
      "audio_model.blocks.21.self_attn.q.bias\n",
      "audio_model.blocks.21.self_attn.k.weight\n",
      "audio_model.blocks.21.self_attn.k.bias\n",
      "audio_model.blocks.21.self_attn.v.weight\n",
      "audio_model.blocks.21.self_attn.v.bias\n",
      "audio_model.blocks.21.self_attn.o.weight\n",
      "audio_model.blocks.21.self_attn.o.bias\n",
      "audio_model.blocks.21.self_attn.norm_q.weight\n",
      "audio_model.blocks.21.self_attn.norm_k.weight\n",
      "audio_model.blocks.21.norm3.weight\n",
      "audio_model.blocks.21.norm3.bias\n",
      "audio_model.blocks.21.cross_attn.q.weight\n",
      "audio_model.blocks.21.cross_attn.q.bias\n",
      "audio_model.blocks.21.cross_attn.k.weight\n",
      "audio_model.blocks.21.cross_attn.k.bias\n",
      "audio_model.blocks.21.cross_attn.v.weight\n",
      "audio_model.blocks.21.cross_attn.v.bias\n",
      "audio_model.blocks.21.cross_attn.o.weight\n",
      "audio_model.blocks.21.cross_attn.o.bias\n",
      "audio_model.blocks.21.cross_attn.norm_q.weight\n",
      "audio_model.blocks.21.cross_attn.norm_k.weight\n",
      "audio_model.blocks.21.cross_attn.k_fusion.weight\n",
      "audio_model.blocks.21.cross_attn.k_fusion.bias\n",
      "audio_model.blocks.21.cross_attn.v_fusion.weight\n",
      "audio_model.blocks.21.cross_attn.v_fusion.bias\n",
      "audio_model.blocks.21.cross_attn.pre_attn_norm_fusion.weight\n",
      "audio_model.blocks.21.cross_attn.pre_attn_norm_fusion.bias\n",
      "audio_model.blocks.21.cross_attn.norm_k_fusion.weight\n",
      "audio_model.blocks.21.ffn.0.weight\n",
      "audio_model.blocks.21.ffn.0.bias\n",
      "audio_model.blocks.21.ffn.2.weight\n",
      "audio_model.blocks.21.ffn.2.bias\n",
      "audio_model.blocks.21.modulation.modulation\n",
      "audio_model.blocks.22.self_attn.q.weight\n",
      "audio_model.blocks.22.self_attn.q.bias\n",
      "audio_model.blocks.22.self_attn.k.weight\n",
      "audio_model.blocks.22.self_attn.k.bias\n",
      "audio_model.blocks.22.self_attn.v.weight\n",
      "audio_model.blocks.22.self_attn.v.bias\n",
      "audio_model.blocks.22.self_attn.o.weight\n",
      "audio_model.blocks.22.self_attn.o.bias\n",
      "audio_model.blocks.22.self_attn.norm_q.weight\n",
      "audio_model.blocks.22.self_attn.norm_k.weight\n",
      "audio_model.blocks.22.norm3.weight\n",
      "audio_model.blocks.22.norm3.bias\n",
      "audio_model.blocks.22.cross_attn.q.weight\n",
      "audio_model.blocks.22.cross_attn.q.bias\n",
      "audio_model.blocks.22.cross_attn.k.weight\n",
      "audio_model.blocks.22.cross_attn.k.bias\n",
      "audio_model.blocks.22.cross_attn.v.weight\n",
      "audio_model.blocks.22.cross_attn.v.bias\n",
      "audio_model.blocks.22.cross_attn.o.weight\n",
      "audio_model.blocks.22.cross_attn.o.bias\n",
      "audio_model.blocks.22.cross_attn.norm_q.weight\n",
      "audio_model.blocks.22.cross_attn.norm_k.weight\n",
      "audio_model.blocks.22.cross_attn.k_fusion.weight\n",
      "audio_model.blocks.22.cross_attn.k_fusion.bias\n",
      "audio_model.blocks.22.cross_attn.v_fusion.weight\n",
      "audio_model.blocks.22.cross_attn.v_fusion.bias\n",
      "audio_model.blocks.22.cross_attn.pre_attn_norm_fusion.weight\n",
      "audio_model.blocks.22.cross_attn.pre_attn_norm_fusion.bias\n",
      "audio_model.blocks.22.cross_attn.norm_k_fusion.weight\n",
      "audio_model.blocks.22.ffn.0.weight\n",
      "audio_model.blocks.22.ffn.0.bias\n",
      "audio_model.blocks.22.ffn.2.weight\n",
      "audio_model.blocks.22.ffn.2.bias\n",
      "audio_model.blocks.22.modulation.modulation\n",
      "audio_model.blocks.23.self_attn.q.weight\n",
      "audio_model.blocks.23.self_attn.q.bias\n",
      "audio_model.blocks.23.self_attn.k.weight\n",
      "audio_model.blocks.23.self_attn.k.bias\n",
      "audio_model.blocks.23.self_attn.v.weight\n",
      "audio_model.blocks.23.self_attn.v.bias\n",
      "audio_model.blocks.23.self_attn.o.weight\n",
      "audio_model.blocks.23.self_attn.o.bias\n",
      "audio_model.blocks.23.self_attn.norm_q.weight\n",
      "audio_model.blocks.23.self_attn.norm_k.weight\n",
      "audio_model.blocks.23.norm3.weight\n",
      "audio_model.blocks.23.norm3.bias\n",
      "audio_model.blocks.23.cross_attn.q.weight\n",
      "audio_model.blocks.23.cross_attn.q.bias\n",
      "audio_model.blocks.23.cross_attn.k.weight\n",
      "audio_model.blocks.23.cross_attn.k.bias\n",
      "audio_model.blocks.23.cross_attn.v.weight\n",
      "audio_model.blocks.23.cross_attn.v.bias\n",
      "audio_model.blocks.23.cross_attn.o.weight\n",
      "audio_model.blocks.23.cross_attn.o.bias\n",
      "audio_model.blocks.23.cross_attn.norm_q.weight\n",
      "audio_model.blocks.23.cross_attn.norm_k.weight\n",
      "audio_model.blocks.23.cross_attn.k_fusion.weight\n",
      "audio_model.blocks.23.cross_attn.k_fusion.bias\n",
      "audio_model.blocks.23.cross_attn.v_fusion.weight\n",
      "audio_model.blocks.23.cross_attn.v_fusion.bias\n",
      "audio_model.blocks.23.cross_attn.pre_attn_norm_fusion.weight\n",
      "audio_model.blocks.23.cross_attn.pre_attn_norm_fusion.bias\n",
      "audio_model.blocks.23.cross_attn.norm_k_fusion.weight\n",
      "audio_model.blocks.23.ffn.0.weight\n",
      "audio_model.blocks.23.ffn.0.bias\n",
      "audio_model.blocks.23.ffn.2.weight\n",
      "audio_model.blocks.23.ffn.2.bias\n",
      "audio_model.blocks.23.modulation.modulation\n",
      "audio_model.blocks.24.self_attn.q.weight\n",
      "audio_model.blocks.24.self_attn.q.bias\n",
      "audio_model.blocks.24.self_attn.k.weight\n",
      "audio_model.blocks.24.self_attn.k.bias\n",
      "audio_model.blocks.24.self_attn.v.weight\n",
      "audio_model.blocks.24.self_attn.v.bias\n",
      "audio_model.blocks.24.self_attn.o.weight\n",
      "audio_model.blocks.24.self_attn.o.bias\n",
      "audio_model.blocks.24.self_attn.norm_q.weight\n",
      "audio_model.blocks.24.self_attn.norm_k.weight\n",
      "audio_model.blocks.24.norm3.weight\n",
      "audio_model.blocks.24.norm3.bias\n",
      "audio_model.blocks.24.cross_attn.q.weight\n",
      "audio_model.blocks.24.cross_attn.q.bias\n",
      "audio_model.blocks.24.cross_attn.k.weight\n",
      "audio_model.blocks.24.cross_attn.k.bias\n",
      "audio_model.blocks.24.cross_attn.v.weight\n",
      "audio_model.blocks.24.cross_attn.v.bias\n",
      "audio_model.blocks.24.cross_attn.o.weight\n",
      "audio_model.blocks.24.cross_attn.o.bias\n",
      "audio_model.blocks.24.cross_attn.norm_q.weight\n",
      "audio_model.blocks.24.cross_attn.norm_k.weight\n",
      "audio_model.blocks.24.cross_attn.k_fusion.weight\n",
      "audio_model.blocks.24.cross_attn.k_fusion.bias\n",
      "audio_model.blocks.24.cross_attn.v_fusion.weight\n",
      "audio_model.blocks.24.cross_attn.v_fusion.bias\n",
      "audio_model.blocks.24.cross_attn.pre_attn_norm_fusion.weight\n",
      "audio_model.blocks.24.cross_attn.pre_attn_norm_fusion.bias\n",
      "audio_model.blocks.24.cross_attn.norm_k_fusion.weight\n",
      "audio_model.blocks.24.ffn.0.weight\n",
      "audio_model.blocks.24.ffn.0.bias\n",
      "audio_model.blocks.24.ffn.2.weight\n",
      "audio_model.blocks.24.ffn.2.bias\n",
      "audio_model.blocks.24.modulation.modulation\n",
      "audio_model.blocks.25.self_attn.q.weight\n",
      "audio_model.blocks.25.self_attn.q.bias\n",
      "audio_model.blocks.25.self_attn.k.weight\n",
      "audio_model.blocks.25.self_attn.k.bias\n",
      "audio_model.blocks.25.self_attn.v.weight\n",
      "audio_model.blocks.25.self_attn.v.bias\n",
      "audio_model.blocks.25.self_attn.o.weight\n",
      "audio_model.blocks.25.self_attn.o.bias\n",
      "audio_model.blocks.25.self_attn.norm_q.weight\n",
      "audio_model.blocks.25.self_attn.norm_k.weight\n",
      "audio_model.blocks.25.norm3.weight\n",
      "audio_model.blocks.25.norm3.bias\n",
      "audio_model.blocks.25.cross_attn.q.weight\n",
      "audio_model.blocks.25.cross_attn.q.bias\n",
      "audio_model.blocks.25.cross_attn.k.weight\n",
      "audio_model.blocks.25.cross_attn.k.bias\n",
      "audio_model.blocks.25.cross_attn.v.weight\n",
      "audio_model.blocks.25.cross_attn.v.bias\n",
      "audio_model.blocks.25.cross_attn.o.weight\n",
      "audio_model.blocks.25.cross_attn.o.bias\n",
      "audio_model.blocks.25.cross_attn.norm_q.weight\n",
      "audio_model.blocks.25.cross_attn.norm_k.weight\n",
      "audio_model.blocks.25.cross_attn.k_fusion.weight\n",
      "audio_model.blocks.25.cross_attn.k_fusion.bias\n",
      "audio_model.blocks.25.cross_attn.v_fusion.weight\n",
      "audio_model.blocks.25.cross_attn.v_fusion.bias\n",
      "audio_model.blocks.25.cross_attn.pre_attn_norm_fusion.weight\n",
      "audio_model.blocks.25.cross_attn.pre_attn_norm_fusion.bias\n",
      "audio_model.blocks.25.cross_attn.norm_k_fusion.weight\n",
      "audio_model.blocks.25.ffn.0.weight\n",
      "audio_model.blocks.25.ffn.0.bias\n",
      "audio_model.blocks.25.ffn.2.weight\n",
      "audio_model.blocks.25.ffn.2.bias\n",
      "audio_model.blocks.25.modulation.modulation\n",
      "audio_model.blocks.26.self_attn.q.weight\n",
      "audio_model.blocks.26.self_attn.q.bias\n",
      "audio_model.blocks.26.self_attn.k.weight\n",
      "audio_model.blocks.26.self_attn.k.bias\n",
      "audio_model.blocks.26.self_attn.v.weight\n",
      "audio_model.blocks.26.self_attn.v.bias\n",
      "audio_model.blocks.26.self_attn.o.weight\n",
      "audio_model.blocks.26.self_attn.o.bias\n",
      "audio_model.blocks.26.self_attn.norm_q.weight\n",
      "audio_model.blocks.26.self_attn.norm_k.weight\n",
      "audio_model.blocks.26.norm3.weight\n",
      "audio_model.blocks.26.norm3.bias\n",
      "audio_model.blocks.26.cross_attn.q.weight\n",
      "audio_model.blocks.26.cross_attn.q.bias\n",
      "audio_model.blocks.26.cross_attn.k.weight\n",
      "audio_model.blocks.26.cross_attn.k.bias\n",
      "audio_model.blocks.26.cross_attn.v.weight\n",
      "audio_model.blocks.26.cross_attn.v.bias\n",
      "audio_model.blocks.26.cross_attn.o.weight\n",
      "audio_model.blocks.26.cross_attn.o.bias\n",
      "audio_model.blocks.26.cross_attn.norm_q.weight\n",
      "audio_model.blocks.26.cross_attn.norm_k.weight\n",
      "audio_model.blocks.26.cross_attn.k_fusion.weight\n",
      "audio_model.blocks.26.cross_attn.k_fusion.bias\n",
      "audio_model.blocks.26.cross_attn.v_fusion.weight\n",
      "audio_model.blocks.26.cross_attn.v_fusion.bias\n",
      "audio_model.blocks.26.cross_attn.pre_attn_norm_fusion.weight\n",
      "audio_model.blocks.26.cross_attn.pre_attn_norm_fusion.bias\n",
      "audio_model.blocks.26.cross_attn.norm_k_fusion.weight\n",
      "audio_model.blocks.26.ffn.0.weight\n",
      "audio_model.blocks.26.ffn.0.bias\n",
      "audio_model.blocks.26.ffn.2.weight\n",
      "audio_model.blocks.26.ffn.2.bias\n",
      "audio_model.blocks.26.modulation.modulation\n",
      "audio_model.blocks.27.self_attn.q.weight\n",
      "audio_model.blocks.27.self_attn.q.bias\n",
      "audio_model.blocks.27.self_attn.k.weight\n",
      "audio_model.blocks.27.self_attn.k.bias\n",
      "audio_model.blocks.27.self_attn.v.weight\n",
      "audio_model.blocks.27.self_attn.v.bias\n",
      "audio_model.blocks.27.self_attn.o.weight\n",
      "audio_model.blocks.27.self_attn.o.bias\n",
      "audio_model.blocks.27.self_attn.norm_q.weight\n",
      "audio_model.blocks.27.self_attn.norm_k.weight\n",
      "audio_model.blocks.27.norm3.weight\n",
      "audio_model.blocks.27.norm3.bias\n",
      "audio_model.blocks.27.cross_attn.q.weight\n",
      "audio_model.blocks.27.cross_attn.q.bias\n",
      "audio_model.blocks.27.cross_attn.k.weight\n",
      "audio_model.blocks.27.cross_attn.k.bias\n",
      "audio_model.blocks.27.cross_attn.v.weight\n",
      "audio_model.blocks.27.cross_attn.v.bias\n",
      "audio_model.blocks.27.cross_attn.o.weight\n",
      "audio_model.blocks.27.cross_attn.o.bias\n",
      "audio_model.blocks.27.cross_attn.norm_q.weight\n",
      "audio_model.blocks.27.cross_attn.norm_k.weight\n",
      "audio_model.blocks.27.cross_attn.k_fusion.weight\n",
      "audio_model.blocks.27.cross_attn.k_fusion.bias\n",
      "audio_model.blocks.27.cross_attn.v_fusion.weight\n",
      "audio_model.blocks.27.cross_attn.v_fusion.bias\n",
      "audio_model.blocks.27.cross_attn.pre_attn_norm_fusion.weight\n",
      "audio_model.blocks.27.cross_attn.pre_attn_norm_fusion.bias\n",
      "audio_model.blocks.27.cross_attn.norm_k_fusion.weight\n",
      "audio_model.blocks.27.ffn.0.weight\n",
      "audio_model.blocks.27.ffn.0.bias\n",
      "audio_model.blocks.27.ffn.2.weight\n",
      "audio_model.blocks.27.ffn.2.bias\n",
      "audio_model.blocks.27.modulation.modulation\n",
      "audio_model.blocks.28.self_attn.q.weight\n",
      "audio_model.blocks.28.self_attn.q.bias\n",
      "audio_model.blocks.28.self_attn.k.weight\n",
      "audio_model.blocks.28.self_attn.k.bias\n",
      "audio_model.blocks.28.self_attn.v.weight\n",
      "audio_model.blocks.28.self_attn.v.bias\n",
      "audio_model.blocks.28.self_attn.o.weight\n",
      "audio_model.blocks.28.self_attn.o.bias\n",
      "audio_model.blocks.28.self_attn.norm_q.weight\n",
      "audio_model.blocks.28.self_attn.norm_k.weight\n",
      "audio_model.blocks.28.norm3.weight\n",
      "audio_model.blocks.28.norm3.bias\n",
      "audio_model.blocks.28.cross_attn.q.weight\n",
      "audio_model.blocks.28.cross_attn.q.bias\n",
      "audio_model.blocks.28.cross_attn.k.weight\n",
      "audio_model.blocks.28.cross_attn.k.bias\n",
      "audio_model.blocks.28.cross_attn.v.weight\n",
      "audio_model.blocks.28.cross_attn.v.bias\n",
      "audio_model.blocks.28.cross_attn.o.weight\n",
      "audio_model.blocks.28.cross_attn.o.bias\n",
      "audio_model.blocks.28.cross_attn.norm_q.weight\n",
      "audio_model.blocks.28.cross_attn.norm_k.weight\n",
      "audio_model.blocks.28.cross_attn.k_fusion.weight\n",
      "audio_model.blocks.28.cross_attn.k_fusion.bias\n",
      "audio_model.blocks.28.cross_attn.v_fusion.weight\n",
      "audio_model.blocks.28.cross_attn.v_fusion.bias\n",
      "audio_model.blocks.28.cross_attn.pre_attn_norm_fusion.weight\n",
      "audio_model.blocks.28.cross_attn.pre_attn_norm_fusion.bias\n",
      "audio_model.blocks.28.cross_attn.norm_k_fusion.weight\n",
      "audio_model.blocks.28.ffn.0.weight\n",
      "audio_model.blocks.28.ffn.0.bias\n",
      "audio_model.blocks.28.ffn.2.weight\n",
      "audio_model.blocks.28.ffn.2.bias\n",
      "audio_model.blocks.28.modulation.modulation\n",
      "audio_model.blocks.29.self_attn.q.weight\n",
      "audio_model.blocks.29.self_attn.q.bias\n",
      "audio_model.blocks.29.self_attn.k.weight\n",
      "audio_model.blocks.29.self_attn.k.bias\n",
      "audio_model.blocks.29.self_attn.v.weight\n",
      "audio_model.blocks.29.self_attn.v.bias\n",
      "audio_model.blocks.29.self_attn.o.weight\n",
      "audio_model.blocks.29.self_attn.o.bias\n",
      "audio_model.blocks.29.self_attn.norm_q.weight\n",
      "audio_model.blocks.29.self_attn.norm_k.weight\n",
      "audio_model.blocks.29.norm3.weight\n",
      "audio_model.blocks.29.norm3.bias\n",
      "audio_model.blocks.29.cross_attn.q.weight\n",
      "audio_model.blocks.29.cross_attn.q.bias\n",
      "audio_model.blocks.29.cross_attn.k.weight\n",
      "audio_model.blocks.29.cross_attn.k.bias\n",
      "audio_model.blocks.29.cross_attn.v.weight\n",
      "audio_model.blocks.29.cross_attn.v.bias\n",
      "audio_model.blocks.29.cross_attn.o.weight\n",
      "audio_model.blocks.29.cross_attn.o.bias\n",
      "audio_model.blocks.29.cross_attn.norm_q.weight\n",
      "audio_model.blocks.29.cross_attn.norm_k.weight\n",
      "audio_model.blocks.29.cross_attn.k_fusion.weight\n",
      "audio_model.blocks.29.cross_attn.k_fusion.bias\n",
      "audio_model.blocks.29.cross_attn.v_fusion.weight\n",
      "audio_model.blocks.29.cross_attn.v_fusion.bias\n",
      "audio_model.blocks.29.cross_attn.pre_attn_norm_fusion.weight\n",
      "audio_model.blocks.29.cross_attn.pre_attn_norm_fusion.bias\n",
      "audio_model.blocks.29.cross_attn.norm_k_fusion.weight\n",
      "audio_model.blocks.29.ffn.0.weight\n",
      "audio_model.blocks.29.ffn.0.bias\n",
      "audio_model.blocks.29.ffn.2.weight\n",
      "audio_model.blocks.29.ffn.2.bias\n",
      "audio_model.blocks.29.modulation.modulation\n",
      "audio_model.head.modulation\n",
      "audio_model.head.head.weight\n",
      "audio_model.head.head.bias\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from utils.fm_solvers import FlowUniPCMultistepScheduler\n",
    "from utils.va_processing import snap_hw_to_multiple_of_32, save_video\n",
    "from utils.model_loading import (\n",
    "    init_fusion_score_model_ovi, \n",
    "    init_text_model, \n",
    "    init_mmaudio_vae, \n",
    "    init_wan_vae_2_2, \n",
    "    load_fusion_checkpoint\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "\n",
    "device = 5\n",
    "target_dtype = torch.bfloat16\n",
    "ckpt_dir = \"/home/chengxin/chengxin/Ovi/ckpts\"  # 请修改为你的检查点路径\n",
    "\n",
    "model, video_config, audio_config = init_fusion_score_model_ovi(rank=device, meta_init=True)\n",
    "if ckpt_dir is not None:\n",
    "    checkpoint_path = os.path.join(ckpt_dir, \"Ovi\", \"model.safetensors\")\n",
    "    load_fusion_checkpoint(model, checkpoint_path=checkpoint_path, from_meta=True)\n",
    "model = model.requires_grad_(False).eval().to(dtype=target_dtype).to(device=device)\n",
    "model.set_rope_params()\n",
    "model.train()\n",
    "model.gradient_checkpointing = True\n",
    "model.requires_grad_(False)\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    # if \"ffn\" in name:\n",
    "        print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31e1634c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chengxin/anaconda3/envs/vagen/lib/python3.10/site-packages/peft/mapping_func.py:73: UserWarning: You are trying to modify a model with PEFT for a second time. If you want to reload the model with a different config, make sure to call `.unload()` before.\n",
      "  warnings.warn(\n",
      "/home/chengxin/anaconda3/envs/vagen/lib/python3.10/site-packages/peft/tuners/tuners_utils.py:196: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 739,246,080 || all params: 13,139,245,268 || trainable%: 5.6262\n"
     ]
    }
   ],
   "source": [
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=128,\n",
    "    lora_alpha=64,\n",
    "    target_modules=['k_fusion', 'v_fusion', 'ffn.0', 'ffn.2',\n",
    "                    'self_attn.q', 'self_attn.k', 'self_attn.v', 'self_attn.o', \n",
    "                    'cross_attn.q', 'cross_attn.k', 'cross_attn.v', 'cross_attn.o'],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",)\n",
    "model.gradient_checkpointing = True\n",
    "model = get_peft_model(model, lora_config, adapter_name=\"ref\")\n",
    "model = get_peft_model(model, lora_config, adapter_name=\"learner\")\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "\n",
    "# model.save_pretrained(f\"log\")\n",
    "# lora_config = LoraConfig(\n",
    "#     r=128,\n",
    "#     lora_alpha=64,\n",
    "#     target_modules=['k_fusion'],\n",
    "#     lora_dropout=0.1,\n",
    "#     bias=\"none\",)\n",
    "# model = get_peft_model(model, lora_config, adapter_name=\"learner\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c062a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e70a92b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07558920",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "516bdbf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "videos = [\n",
    "    '_52ntwwQyv4_000070',\n",
    "    '0_1Jo7NAhl4_000149',\n",
    "    '2baznhAyEsg_000014',\n",
    "    '2OLXoKxJ1qg_000030',\n",
    "    '03fGTwkSBWs_000289',\n",
    "    '3yXCtpvjz6E_000017',\n",
    "    '4VBd7XmgsTc_000019',\n",
    "    '7HHCf9-HBiA_000000',\n",
    "    '7qx057kqx1E_000039',\n",
    "    '55',\n",
    "    '106',\n",
    "    '135',\n",
    "    '142',\n",
    "    '152',\n",
    "    '197',\n",
    "    'akjqOhO0IM0_000010',\n",
    "    'C7VWBi27oGc_000006',\n",
    "    'irN1qoE3-uQ_000004',\n",
    "    'IWvA2ZlAK2g_000028',\n",
    "    'J2L9eJ-h9DY_000030',\n",
    "    'JwJAh2X6btc_000010',\n",
    "    'KzK6d6Qpu_o_000010',\n",
    "    'lDTrnXAu4WY_000025',\n",
    "    'OJqJgotD8D4_000038',\n",
    "    'PnwTlG1aBdU_000086',\n",
    "    'QIT6l8y0_cE_000039',\n",
    "    'rsrWBquU8bc_000050',\n",
    "    'sxJjC9HC1Xs_000310',\n",
    "    'Vwz4vOzWnLE_000078',\n",
    "    'weSfQmkG35I_000002',\n",
    "    'WJV6Ey6hXEw_000005',\n",
    "    'Yi9Xdiq579o_000240',\n",
    "    'yUZMpGwS-OI_000230'\n",
    "]\n",
    "\n",
    "for video in videos:\n",
    "    shutil.copy(f'/home/chengxin/chengxin/vagen/log/ovi_fusion_dpo_15_11/logging/0/{video}.mp4', f'/home/chengxin/chengxin/vagen/log/compare/{video}_0.mp4')\n",
    "    shutil.copy(f'/home/chengxin/chengxin/vagen/log/ovi_fusion_dpo_15_11/logging/1400/{video}.mp4', f'/home/chengxin/chengxin/vagen/log/compare/{video}_dpo.mp4')\n",
    "    shutil.copy(f'/home/chengxin/chengxin/vagen/log/ovi_fusion_sft_15/logging/1400/{video}.mp4', f'/home/chengxin/chengxin/vagen/log/compare/{video}_sft.mp4')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423c563f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26caff30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "\n",
    "target_dir = '/home/chengxin/chengxin/dataset/VGGSound/avsync_vggss'\n",
    "\n",
    "with open('/home/chengxin/chengxin/vagen/data/ttva/vggss_train.json', 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "for path, info in tqdm(data.items()):\n",
    "    name = path.split('/')[-1]\n",
    "    if not os.path.exists( f\"{target_dir}/{name}\"):\n",
    "        shutil.copy(path, f\"{target_dir}/{name}\")\n",
    "    else:\n",
    "        print(path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "693d08af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1277"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(os.listdir(target_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8d4d08",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vagen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
