{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c37aafab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from model.wan.pipeline_wan_ttv import AutoencoderKLWan, WanPipeline\n",
    "from diffusers.utils import export_to_video\n",
    "from model.wan.wan_transformer_for_video import WanTransformer3DModel\n",
    "\n",
    "\n",
    "# Available models: Wan-AI/Wan2.1-T2V-14B-Diffusers, Wan-AI/Wan2.1-T2V-1.3B-Diffusers\n",
    "model_id = './assets/Wan2.1-T2V-1.3B-Diffusers'\n",
    "load_dtype = torch.float32\n",
    "transformer = WanTransformer3DModel.from_pretrained(\n",
    "        model_id, subfolder=\"transformer\", \n",
    "        torch_dtype=load_dtype, \n",
    "        local_files_only=True,\n",
    "        # low_cpu_mem_usage=False, \n",
    "        use_safetensors=True,\n",
    "        ignore_mismatched_sizes=True,      # Setting for model structure changes\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24e6617b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 5/5 [00:01<00:00,  3.48it/s]\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  8.21it/s]s/it]\n",
      "Loading pipeline components...: 100%|██████████| 5/5 [00:02<00:00,  2.30it/s]\n",
      "100%|██████████| 50/50 [02:30<00:00,  3.01s/it]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'output.mp4'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from diffusers.utils import export_to_video\n",
    "from diffusers import AutoencoderKLWan, WanPipeline\n",
    "from diffusers.schedulers.scheduling_unipc_multistep import UniPCMultistepScheduler\n",
    "\n",
    "# Available models: Wan-AI/Wan2.1-T2V-14B-Diffusers, Wan-AI/Wan2.1-T2V-1.3B-Diffusers\n",
    "model_id = \"./assets/Wan2.1-T2V-1.3B-Diffusers\"\n",
    "vae = AutoencoderKLWan.from_pretrained(model_id, subfolder=\"vae\", torch_dtype=torch.float16)\n",
    "flow_shift = 5.0 # 5.0 for 720P, 3.0 for 480P\n",
    "scheduler = UniPCMultistepScheduler(prediction_type='flow_prediction', use_flow_sigmas=True, num_train_timesteps=1000, flow_shift=flow_shift)\n",
    "pipe = WanPipeline.from_pretrained(model_id, vae=vae, torch_dtype=torch.float16)\n",
    "pipe.scheduler = scheduler\n",
    "pipe.to(\"cuda\")\n",
    "\n",
    "prompt = \"A cat and a dog baking a cake together in a kitchen. The cat is carefully measuring flour, while the dog is stirring the batter with a wooden spoon. The kitchen is cozy, with sunlight streaming through the window.\"\n",
    "negative_prompt = \"Bright tones, overexposed, static, blurred details, subtitles, style, works, paintings, images, static, overall gray, worst quality, low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn faces, deformed, disfigured, misshapen limbs, fused fingers, still picture, messy background, three legs, many people in the background, walking backwards\"\n",
    "\n",
    "output = pipe(\n",
    "     prompt=prompt,\n",
    "     negative_prompt=negative_prompt,\n",
    "     height=480,\n",
    "     width=832,\n",
    "     num_frames=81,\n",
    "     guidance_scale=5.0,\n",
    "    ).frames[0]\n",
    "export_to_video(output, \"output.mp4\", fps=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce27c133",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from diffusers import AutoModel\n",
    "from diffusers.training_utils import cast_training_params, free_memory\n",
    "import torch\n",
    "from torch.cuda.amp import autocast\n",
    "\n",
    "from diffusers.utils.torch_utils import randn_tensor\n",
    "from diffusers.video_processor import VideoProcessor\n",
    "from omegaconf import OmegaConf\n",
    "from model.wan.wan_transformer_for_video import WanTransformer3DModel\n",
    "from model.wan.pipeline_wan_ttv import FlowMatchEulerDiscreteScheduler, AutoencoderKLWan, WanPipeline\n",
    "from diffusers.schedulers.scheduling_unipc_multistep import UniPCMultistepScheduler\n",
    "from utils.text_encoding import get_t5_prompt_embeds, encode_prompt\n",
    "from diffusers.utils import check_min_version, convert_unet_state_dict_to_peft, export_to_video, is_wandb_available\n",
    "\n",
    "\n",
    "\n",
    "args = OmegaConf.load(\"/home/chengxin/chengxin/vagen/config/ttv_wan.yaml\")\n",
    "infer_dtype = torch.float16\n",
    "load_dtype = torch.float32\n",
    "device = \"cuda:1\"\n",
    "\n",
    "vae = AutoencoderKLWan.from_pretrained(\n",
    "    args.pretrained_model_name_or_path, subfolder=\"vae\", \n",
    "    torch_dtype=load_dtype\n",
    ").to(device)\n",
    "tokenizer = AutoModel.from_pretrained(\n",
    "    args.pretrained_model_name_or_path, subfolder=\"tokenizer\", \n",
    "    local_files_only=True,\n",
    "    use_safetensors=True,\n",
    ")\n",
    "transformer = WanTransformer3DModel.from_pretrained(\n",
    "    args.pretrained_model_name_or_path, subfolder=\"transformer\", \n",
    "    torch_dtype=load_dtype, \n",
    "    local_files_only=True,\n",
    "    low_cpu_mem_usage=False, \n",
    "    use_safetensors=True,\n",
    "    ignore_mismatched_sizes=True,      # Setting for model structure changes\n",
    ").to(device)\n",
    "text_encoder = AutoModel.from_pretrained(\n",
    "    args.pretrained_model_name_or_path, subfolder=\"text_encoder\", \n",
    "    torch_dtype=load_dtype, \n",
    "    local_files_only=True,\n",
    "    use_safetensors=True,\n",
    ").to(device)\n",
    "\n",
    "scheduler = UniPCMultistepScheduler(prediction_type='flow_prediction', use_flow_sigmas=True, num_train_timesteps=1000, flow_shift=args.validation.flow_shift)\n",
    "video_processor = VideoProcessor(vae_scale_factor=vae.config.scale_factor_spatial)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "config = args.validation\n",
    "with torch.no_grad():\n",
    "    print(\"Start log_validation\")\n",
    "    prompt_list = config.prompt.split(config.prompt_separator)\n",
    "    negative_prompt = config.negetive_prompt\n",
    "    output_dir = getattr(config, \"save_dir\", 5.0)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    for prompt_idx, prompt in enumerate(prompt_list):\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        free_memory()\n",
    "\n",
    "        prompt_embeds, negative_prompt_embeds = None, None\n",
    "        if config.negetive_prompt_embed is not None:\n",
    "            negative_prompt_embeds = torch.load(config.negetive_prompt_embed).to(device).unsqueeze(0)\n",
    "            \n",
    "        prompt_embeds, negative_prompt_embeds = encode_prompt(\n",
    "            prompt=prompt,\n",
    "            negative_prompt=negative_prompt,\n",
    "            tokenizer = tokenizer,\n",
    "            text_encoder = text_encoder,\n",
    "            do_classifier_free_guidance=config.guidance_scale > 1.0,\n",
    "            num_videos_per_prompt=config.num_videos_per_prompt,\n",
    "            prompt_embeds=prompt_embeds,\n",
    "            negative_prompt_embeds=negative_prompt_embeds,\n",
    "            max_sequence_length=512,\n",
    "            device=device,\n",
    "            dtype = load_dtype,\n",
    "        ) # type: ignore\n",
    "\n",
    "\n",
    "        with autocast(dtype=infer_dtype):\n",
    "            num_latent_frames = (config.num_frames - 1) // vae.config.scale_factor_temporal + 1\n",
    "            num_channels = transformer.module.config.in_channels if hasattr(transformer, \"module\") else transformer.config.in_channels\n",
    "            shape = (\n",
    "                    config.num_videos_per_prompt,\n",
    "                    num_channels,\n",
    "                    num_latent_frames,\n",
    "                    config.height // vae.config.scale_factor_spatial,\n",
    "                    config.width // vae.config.scale_factor_spatial,\n",
    "            )\n",
    "            latents = randn_tensor(shape, device=device, dtype=infer_dtype)\n",
    "            scheduler.set_timesteps(config.num_inference_steps, device=device)\n",
    "            timesteps = scheduler.timesteps\n",
    "\n",
    "            for i, t in tqdm(enumerate(timesteps)):\n",
    "                current_model = transformer.module if hasattr(transformer, \"module\") else transformer\n",
    "                current_guidance_scale = config.guidance_scale\n",
    "                latent_model_input = latents\n",
    "                timestep = t.expand(latents.shape[0])\n",
    "\n",
    "                with current_model.cache_context(\"cond\"):\n",
    "                    noise_pred = current_model(\n",
    "                        hidden_states=latent_model_input,\n",
    "                        timestep=timestep,\n",
    "                        encoder_hidden_states=prompt_embeds,\n",
    "                        attention_kwargs=None,\n",
    "                        return_dict=False,\n",
    "                    )[0]\n",
    "\n",
    "                if config.guidance_scale > 1.0:\n",
    "                    with current_model.cache_context(\"uncond\"):\n",
    "                        noise_uncond = current_model(\n",
    "                            hidden_states=latent_model_input,\n",
    "                            timestep=timestep,\n",
    "                            encoder_hidden_states=negative_prompt_embeds,\n",
    "                            attention_kwargs=None,\n",
    "                            return_dict=False,\n",
    "                        )[0]\n",
    "                    noise_pred = noise_uncond + current_guidance_scale * (noise_pred - noise_uncond)\n",
    "                latents = scheduler.step(noise_pred, t, latents, return_dict=False)[0]\n",
    "                                    \n",
    "\n",
    "            latents = latents.to(vae.dtype)\n",
    "            latents_mean = torch.tensor(vae.config.latents_mean).view(1, vae.config.z_dim, 1, 1, 1).to(vae.device, vae.dtype)\n",
    "            latents_std = 1 / torch.tensor(vae.config.latents_std).view(1, vae.config.z_dim, 1, 1, 1).to(vae.device, vae.dtype)\n",
    "            latents = latents / latents_std + latents_mean\n",
    "            video = vae.decode(latents, return_dict=False)[0]\n",
    "            video = video_processor.postprocess_video(video, output_type='np')\n",
    "            for i in range(config.num_videos_per_prompt):\n",
    "                export_to_video(video[i], f\"{output_dir}/output{prompt_idx}_{i}.mp4\", fps=config.fps if hasattr(config, \"fps\") else 16)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97e67628",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data-87-04/chengxin/envs/veo3/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  9.87it/s]\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from diffusers import AutoModel\n",
    "from diffusers.training_utils import cast_training_params, free_memory\n",
    "import torch\n",
    "from torch.cuda.amp import autocast\n",
    "\n",
    "from diffusers.utils.torch_utils import randn_tensor\n",
    "from diffusers.video_processor import VideoProcessor\n",
    "from omegaconf import OmegaConf\n",
    "from model.wan.wan_transformer_for_video import WanTransformer3DModel\n",
    "from model.wan.pipeline_wan_ttv import FlowMatchEulerDiscreteScheduler, AutoencoderKLWan, WanPipeline\n",
    "from diffusers.schedulers.scheduling_unipc_multistep import UniPCMultistepScheduler\n",
    "from utils.text_encoding import get_t5_prompt_embeds, encode_prompt\n",
    "from diffusers.utils import check_min_version, convert_unet_state_dict_to_peft, export_to_video, is_wandb_available\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "\n",
    "\n",
    "args = OmegaConf.load(\"/home/chengxin/chengxin/vagen/config/tta_tuning.yaml\")\n",
    "infer_dtype = torch.float16\n",
    "load_dtype = torch.float32\n",
    "device = \"cpu\"\n",
    "\n",
    "\n",
    "transformer = WanTransformer3DModel.from_pretrained(\n",
    "    args.pretrained_model_name_or_path, subfolder=\"transformer\", \n",
    "    torch_dtype=load_dtype, \n",
    "    local_files_only=True,\n",
    "    low_cpu_mem_usage=False, \n",
    "    use_safetensors=True,\n",
    "    ignore_mismatched_sizes=True,      # Setting for model structure changes\n",
    ").to(device)\n",
    "\n",
    "\n",
    "\n",
    "def set_requires_grad(transformer, target_params, print_param = False):\n",
    "    for name, param in transformer.named_parameters():\n",
    "        for target in target_params:\n",
    "            if target in name:\n",
    "                param.requires_grad = True  # 设置为需要梯度\n",
    "                if print_param:\n",
    "                    print(f\"{target}\", end = \" \")\n",
    "    print(\"\\n\")\n",
    "    return transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "349b6774",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data-87-04/chengxin/envs/veo3/lib/python3.10/site-packages/peft/mapping_func.py:73: UserWarning: You are trying to modify a model with PEFT for a second time. If you want to reload the model with a different config, make sure to call `.unload()` before.\n",
      "  warnings.warn(\n",
      "/data-87-04/chengxin/envs/veo3/lib/python3.10/site-packages/peft/tuners/tuners_utils.py:196: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scale_shift_table patch_embedding patch_embedding text_embedder text_embedder text_embedder text_embedder scale_shift_table scale_shift_table scale_shift_table scale_shift_table scale_shift_table scale_shift_table scale_shift_table scale_shift_table scale_shift_table scale_shift_table scale_shift_table scale_shift_table scale_shift_table scale_shift_table scale_shift_table scale_shift_table scale_shift_table scale_shift_table scale_shift_table scale_shift_table scale_shift_table scale_shift_table scale_shift_table scale_shift_table scale_shift_table scale_shift_table scale_shift_table scale_shift_table scale_shift_table scale_shift_table proj_out proj_out \n",
      "\n",
      "trainable params: 160,519,744 || all params: 1,570,384,960 || trainable%: 10.2217\n"
     ]
    }
   ],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=args.rank,  # LoRA 的秩（rank），通常设为 4、8、16 或 32\n",
    "    lora_alpha=args.lora_alpha,  # 缩放因子，控制 LoRA 更新的幅度\n",
    "    target_modules=[\"to_q\", \n",
    "                    \"to_k\", \n",
    "                    \"to_v\", \n",
    "                    \"ffn.net.0.proj\", \n",
    "                    \"ffn.net.2\"],  # 指定应用 LoRA 的模块（如 Transformer 的查询和值投影）\n",
    "    lora_dropout=0.1,  # Dropout 概率\n",
    "    bias=\"none\",  # 是否调整偏置\n",
    "    # task_type=\"CAUSAL_LM\"  # 任务类型，如 CAUSAL_LM 或 SEQ_CLS\n",
    ")\n",
    "\n",
    "\n",
    "transformer = get_peft_model(transformer, lora_config)\n",
    "transformer = set_requires_grad(transformer, ['patch_embedding', 'proj_out', 'scale_shift_table', 'text_embedder'], True)\n",
    "transformer.print_trainable_parameters()  # 查看可训练参数量\n",
    "\n",
    "\n",
    "# print(\"=== 可训练参数 ===\")\n",
    "# trainable_params = 0\n",
    "# total_params = 0\n",
    "# for name, param in transformer.named_parameters():\n",
    "#     if param.requires_grad:\n",
    "#         print(f\"Layer: {name}, Shape: {param.shape}\")\n",
    "#         trainable_params += param.numel()\n",
    "#     total_params += param.numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdfc376e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "324d32e9",
   "metadata": {},
   "source": [
    "# Stable Diffusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d84cd70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import AutoModel\n",
    "from diffusers.pipelines.stable_audio.modeling_stable_audio import StableAudioProjectionModel\n",
    "from model.stable_audio.stable_audio_transformer import StableAudioDiTModel\n",
    "import torch\n",
    "from diffusers.utils.torch_utils import randn_tensor\n",
    "from diffusers.models.embeddings import get_1d_rotary_pos_embed\n",
    "import inspect\n",
    "from tqdm import tqdm\n",
    "import soundfile as sf\n",
    "\n",
    "\n",
    "load_dtype = torch.float16\n",
    "device = \"cuda:0\"\n",
    "transformer = StableAudioDiTModel.from_pretrained(\n",
    "    '/home/chengxin/chengxin/vagen/assets/stable-audio-open-1.0', \n",
    "    subfolder=\"transformer\", \n",
    "    torch_dtype = load_dtype,\n",
    "    local_files_only=True,                  # From pretrained\n",
    "    low_cpu_mem_usage=False, \n",
    "    ignore_mismatched_sizes=True,      \n",
    "    # num_layers=10,                                 \n",
    "    use_safetensors=True,                          \n",
    ").to(device)\n",
    "\n",
    "\n",
    "projection_model = StableAudioProjectionModel.from_pretrained(\n",
    "    '/home/chengxin/chengxin/vagen/assets/stable-audio-open-1.0', \n",
    "    subfolder=\"projection_model\",\n",
    "    torch_dtype=load_dtype, \n",
    "    local_files_only=True,\n",
    "    use_safetensors=True,\n",
    ").to(device)\n",
    "\n",
    "vae = AutoModel.from_pretrained(\n",
    "    '/home/chengxin/chengxin/vagen/assets/stable-audio-open-1.0', \n",
    "    subfolder=\"vae\", \n",
    "    torch_dtype = load_dtype,\n",
    "    local_files_only=True,\n",
    "    use_safetensors=True,                      \n",
    ").to(device)\n",
    "\n",
    "\n",
    "text_encoder = AutoModel.from_pretrained(\n",
    "    '/home/chengxin/chengxin/vagen/assets/stable-audio-open-1.0', \n",
    "    subfolder=\"text_encoder\", \n",
    "    torch_dtype=load_dtype, \n",
    "    local_files_only=True,\n",
    "    use_safetensors=True,\n",
    ").to(device)\n",
    "\n",
    "tokenizer = AutoModel.from_pretrained(\n",
    "    '/home/chengxin/chengxin/vagen/assets/stable-audio-open-1.0', \n",
    "    subfolder=\"tokenizer\", \n",
    "    local_files_only=True,\n",
    "    use_safetensors=True,\n",
    ")\n",
    "\n",
    "scheduler = AutoModel.from_pretrained(\n",
    "    '/home/chengxin/chengxin/vagen/assets/stable-audio-open-1.0', \n",
    "    subfolder=\"scheduler\", \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb696101",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "200it [00:06, 28.58it/s]\n"
     ]
    }
   ],
   "source": [
    "from utils.text_encoding import encode_prompt_sd, encode_duration_sd, prepare_extra_step_kwargs\n",
    "\n",
    "\n",
    "prompt = [\"The sharp, resonant sound of a bowling ball striking the pins, followed by the clattering and scattering of the pins in a chaotic dance, fills the air with a mix of impact and rolling echoes.\"]\n",
    "negative_prompt = [\"Low quality.\"]\n",
    "batch_size = len(prompt)\n",
    "\n",
    "do_classifier_free_guidance = True\n",
    "num_waveforms_per_prompt = 1\n",
    "num_inference_steps = 200\n",
    "eta = 0.0\n",
    "guidance_scale = 7\n",
    "negative_prompt_embeds = None\n",
    "\n",
    "audio_start_in_s = 0.0\n",
    "audio_end_in_s   = 5.0\n",
    "\n",
    "with torch.no_grad():\n",
    "    prompt_embeds = encode_prompt_sd(\n",
    "        prompt,\n",
    "        tokenizer,\n",
    "        text_encoder,\n",
    "        projection_model,\n",
    "        device,\n",
    "        do_classifier_free_guidance,\n",
    "        negative_prompt,\n",
    "    )\n",
    "\n",
    "    # Encode duration\n",
    "    seconds_start_hidden_states, seconds_end_hidden_states = encode_duration_sd(\n",
    "        projection_model,\n",
    "        audio_start_in_s,\n",
    "        audio_end_in_s,\n",
    "        device,\n",
    "        do_classifier_free_guidance and (negative_prompt is not None or negative_prompt_embeds is not None),\n",
    "        batch_size,\n",
    "    )\n",
    "\n",
    "    # Create text_audio_duration_embeds and audio_duration_embeds\n",
    "    text_audio_duration_embeds = torch.cat([prompt_embeds, seconds_start_hidden_states, seconds_end_hidden_states], dim=1)\n",
    "    audio_duration_embeds = torch.cat([seconds_start_hidden_states, seconds_end_hidden_states], dim=2)\n",
    "\n",
    "    # In case of classifier free guidance without negative prompt, we need to create unconditional embeddings and\n",
    "    if do_classifier_free_guidance and negative_prompt_embeds is None and negative_prompt is None:\n",
    "        negative_text_audio_duration_embeds = torch.zeros_like(text_audio_duration_embeds, device=text_audio_duration_embeds.device)\n",
    "        text_audio_duration_embeds = torch.cat([negative_text_audio_duration_embeds, text_audio_duration_embeds], dim=0)\n",
    "        audio_duration_embeds = torch.cat([audio_duration_embeds, audio_duration_embeds], dim=0)\n",
    "\n",
    "    bs_embed, seq_len, hidden_size = text_audio_duration_embeds.shape\n",
    "    # duplicate audio_duration_embeds and text_audio_duration_embeds for each generation per prompt, using mps friendly method\n",
    "    text_audio_duration_embeds = text_audio_duration_embeds.repeat(1, num_waveforms_per_prompt, 1)\n",
    "    text_audio_duration_embeds = text_audio_duration_embeds.view(bs_embed * num_waveforms_per_prompt, seq_len, hidden_size)\n",
    "\n",
    "    # # print(audio_duration_embeds.shape)\n",
    "    audio_duration_embeds = audio_duration_embeds.repeat(1, num_waveforms_per_prompt, 1)\n",
    "    audio_duration_embeds = audio_duration_embeds.view(bs_embed * num_waveforms_per_prompt, -1, audio_duration_embeds.shape[-1])\n",
    "    # # print(audio_duration_embeds.shape)\n",
    "\n",
    "    # 4. Prepare timesteps\n",
    "    scheduler.set_timesteps(num_inference_steps, device=device)\n",
    "    timesteps = scheduler.timesteps\n",
    "\n",
    "    # 5. Prepare latent variables\n",
    "    num_channels_vae = transformer.config.in_channels\n",
    "    waveform_length = int(transformer.config.sample_size)\n",
    "    # waveform_length = int(audio_end_in_s * 22.5)\n",
    "    shape = (batch_size * num_waveforms_per_prompt, num_channels_vae, waveform_length)\n",
    "    generator = torch.Generator(\"cuda\").manual_seed(0)\n",
    "    # generator = None\n",
    "    latents = randn_tensor(shape, generator=generator, device=device, dtype=load_dtype)\n",
    "\n",
    "    # 6. Prepare extra step kwargs and rotary_embed_dim\n",
    "    extra_step_kwargs = prepare_extra_step_kwargs(generator, eta, scheduler)\n",
    "    rotary_embed_dim = transformer.config.attention_head_dim // 2\n",
    "    rotary_embedding = get_1d_rotary_pos_embed(\n",
    "        rotary_embed_dim,\n",
    "        latents.shape[2] + audio_duration_embeds.shape[1],\n",
    "        use_real=True,\n",
    "        repeat_interleave_real=False,\n",
    "    )\n",
    "\n",
    "    # 8. Denoising loop\n",
    "    for i, t in tqdm(enumerate(timesteps)):\n",
    "        # expand the latents if we are doing classifier free guidance\n",
    "        latent_model_input = torch.cat([latents] * 2) if do_classifier_free_guidance else latents\n",
    "        latent_model_input = scheduler.scale_model_input(latent_model_input, t)\n",
    "        t = torch.stack([t for _ in range(latent_model_input.shape[0])])\n",
    "\n",
    "        # predict the noise residual\n",
    "        noise_pred = transformer(\n",
    "            latent_model_input,\n",
    "            t,\n",
    "            encoder_hidden_states=text_audio_duration_embeds,\n",
    "            global_hidden_states=audio_duration_embeds,\n",
    "            rotary_embedding=rotary_embedding,\n",
    "            return_dict=False,\n",
    "        )[0]\n",
    "\n",
    "        # perform guidance\n",
    "        if do_classifier_free_guidance:\n",
    "            noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
    "            noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
    "        latents = scheduler.step(noise_pred, t, latents, **extra_step_kwargs).prev_sample\n",
    "\n",
    "    audio = vae.decode(latents).sample\n",
    "\n",
    "import torchaudio\n",
    "\n",
    "for i in range(len(audio)):\n",
    "    torchaudio.save(f'test{i}.wav', audio[i].to(torch.float32).cpu(), 44100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68454c7a",
   "metadata": {},
   "source": [
    "# Stable Diffusion FT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05ff6b0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data-87-04/chengxin/envs/veo3/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/data-87-04/chengxin/envs/veo3/lib/python3.10/site-packages/torch/nn/utils/weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    }
   ],
   "source": [
    "from diffusers import AutoModel\n",
    "from diffusers.pipelines.stable_audio.modeling_stable_audio import StableAudioProjectionModel\n",
    "from model.stable_audio.stable_audio_transformer import StableAudioDiTModel\n",
    "import torch\n",
    "from diffusers.utils.torch_utils import randn_tensor\n",
    "from diffusers.models.embeddings import get_1d_rotary_pos_embed\n",
    "import inspect\n",
    "from tqdm import tqdm\n",
    "import soundfile as sf\n",
    "from diffusers.schedulers.scheduling_unipc_multistep import UniPCMultistepScheduler\n",
    "\n",
    "\n",
    "load_dtype = torch.float16\n",
    "device = \"cuda:0\"\n",
    "transformer = StableAudioDiTModel.from_pretrained(\n",
    "    '/home/chengxin/chengxin/vagen/assets/stable-audio-open-1.0', \n",
    "    subfolder=\"transformer_ft\", \n",
    "    torch_dtype = load_dtype,\n",
    "    local_files_only=True,                  # From pretrained\n",
    "    low_cpu_mem_usage=False, \n",
    "    ignore_mismatched_sizes=True,      \n",
    "    # num_layers=10,                                 \n",
    "    use_safetensors=True,                          \n",
    ").to(device)\n",
    "\n",
    "# sd_tta_pt_16 ckpt1\n",
    "projection_model = StableAudioProjectionModel.from_pretrained(\n",
    "    '/home/chengxin/chengxin/vagen/assets/stable-audio-open-1.0', \n",
    "    subfolder=\"projection_model\",\n",
    "    torch_dtype=load_dtype, \n",
    "    local_files_only=True,\n",
    "    use_safetensors=True,\n",
    ").to(device)\n",
    "\n",
    "vae = AutoModel.from_pretrained(\n",
    "    '/home/chengxin/chengxin/vagen/assets/stable-audio-open-1.0', \n",
    "    subfolder=\"vae\", \n",
    "    torch_dtype = load_dtype,\n",
    "    local_files_only=True,\n",
    "    use_safetensors=True,                      \n",
    ").to(device)\n",
    "\n",
    "\n",
    "text_encoder = AutoModel.from_pretrained(\n",
    "    '/home/chengxin/chengxin/vagen/assets/stable-audio-open-1.0', \n",
    "    subfolder=\"text_encoder\", \n",
    "    torch_dtype=load_dtype, \n",
    "    local_files_only=True,\n",
    "    use_safetensors=True,\n",
    ").to(device)\n",
    "\n",
    "tokenizer = AutoModel.from_pretrained(\n",
    "    '/home/chengxin/chengxin/vagen/assets/stable-audio-open-1.0', \n",
    "    subfolder=\"tokenizer\", \n",
    "    local_files_only=True,\n",
    "    use_safetensors=True,\n",
    ")\n",
    "\n",
    "step_scheduler = UniPCMultistepScheduler(prediction_type='flow_prediction', use_flow_sigmas=True, num_train_timesteps=1000) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "46c45cf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "200it [00:55,  3.58it/s]\n"
     ]
    }
   ],
   "source": [
    "from utils.text_encoding import encode_prompt_sd, encode_duration_sd, prepare_extra_step_kwargs\n",
    "import json\n",
    "import torchaudio\n",
    "\n",
    "\n",
    "do_classifier_free_guidance = True\n",
    "num_waveforms_per_prompt = 1\n",
    "num_inference_steps = 200\n",
    "eta = 0.0\n",
    "guidance_scale = 7\n",
    "negative_prompt_embeds = None\n",
    "audio_start_in_s = 0.0\n",
    "audio_end_in_s   = 10.0\n",
    "\n",
    "\n",
    "with open('/home/chengxin/chengxin/vagen/data/tta/test_avsync_recap.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "for path, info in data.items():\n",
    "    prompt = [info['label']] \n",
    "    # Lions Roaring Deeply, Bowling striking\n",
    "    prompt = [ \"Sharpen knife\", \"chicken crowing\", \"Bowling rolling and striking\", \"hammering\", \"Lions Roaring Deeply\", 'Frog Croaking'] # ['Lions Roaring Deeply', 'Frog Croaking', 'Dog barking', 'Bowling striking']\n",
    "            #  [ \"Sharpen knife\", \"chicken crowing\", \"toilet flushing\", \"hammering\"]\n",
    "    negative_prompt = [\"\" for _ in range(len(prompt))]\n",
    "    batch_size = len(prompt)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        prompt_embeds = encode_prompt_sd(\n",
    "            prompt,\n",
    "            tokenizer,\n",
    "            text_encoder,\n",
    "            projection_model,\n",
    "            device,\n",
    "            do_classifier_free_guidance,\n",
    "            negative_prompt,\n",
    "        )\n",
    "\n",
    "        # 5. Prepare latent variables\n",
    "        num_channels_vae = transformer.config.in_channels\n",
    "        # waveform_length = int(transformer.config.sample_size)\n",
    "        waveform_length = int(audio_end_in_s * 22.5)\n",
    "        shape = (batch_size * num_waveforms_per_prompt, num_channels_vae, waveform_length)\n",
    "        # generator = torch.Generator(\"cuda\").manual_seed(0)\n",
    "        latents = randn_tensor(shape, device=device, dtype=load_dtype)\n",
    "\n",
    "        # 6. Prepare extra step kwargs and rotary_embed_dim\n",
    "        rotary_embed_dim = transformer.config.attention_head_dim // 2\n",
    "        rotary_embedding = get_1d_rotary_pos_embed(\n",
    "            rotary_embed_dim,\n",
    "            latents.shape[2] + 1,\n",
    "            use_real=True,\n",
    "            repeat_interleave_real=False,\n",
    "        )\n",
    "\n",
    "        # 8. Denoising loop\n",
    "        # TODO: Check scheduler, t = 1 for noise;  UniPCMultistepScheduler timesteps [1-1000]\n",
    "        step_scheduler.set_timesteps(num_inference_steps, device=device)\n",
    "        timesteps = step_scheduler.timesteps\n",
    "        for i, t in tqdm(enumerate(timesteps)):\n",
    "            latent_model_input = torch.cat([latents] * 2) if do_classifier_free_guidance else latents\n",
    "            latent_model_input = step_scheduler.scale_model_input(latent_model_input, t)\n",
    "            t = torch.stack([t for _ in range(latent_model_input.shape[0])])\n",
    "\n",
    "            # predict the noise residual\n",
    "            noise_pred = transformer(\n",
    "                latent_model_input,\n",
    "                t,\n",
    "                encoder_hidden_states=prompt_embeds, # text_audio_duration_embeds,\n",
    "                # global_hidden_states=audio_duration_embeds,\n",
    "                rotary_embedding=rotary_embedding,\n",
    "                return_dict=False,\n",
    "            )[0]\n",
    "\n",
    "            # perform guidance\n",
    "            if do_classifier_free_guidance:\n",
    "                t = t[:len(t)//2]\n",
    "                noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
    "                noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
    "            latents = step_scheduler.step(noise_pred, t[0], latents).prev_sample\n",
    "\n",
    "        audio = vae.decode(latents).sample\n",
    "\n",
    "\n",
    "    for i in range(len(audio)):\n",
    "        # torchaudio.save(f\"/home/chengxin/chengxin/vagen/log/predict/{path.split('/')[-1][:-4]}.wav\", audio[i].to(torch.float32).cpu(), 44100)\n",
    "        torchaudio.save(f\"./test{i}.wav\", audio[i].to(torch.float32).cpu(), 44100)\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25eb4b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, shutil\n",
    "import torchaudio\n",
    "\n",
    "i = 0\n",
    "gen_path = '/home/chengxin/chengxin/vagen/log/sd_tta_pt_16/logging/10000'    # /home/chengxin/chengxin/vagen/log/sd_tta_pt_16/checkpoints/checkpoint_1\n",
    "gen_path = '/home/chengxin/chengxin/vagen/log/sd_tta_ft_16/logging/6500'     # /home/chengxin/chengxin/vagen/log/sd_tta_ft_16/checkpoints/checkpoint_13   6500 7000 7500 \n",
    "# gen_path = '/home/chengxin/chengxin/vagen/log_backup/sd_tta_ft_recap_16/logging/5500'     # /home/chengxin/chengxin/vagen/log/sd_tta_ft_recap_16/checkpoints/   5500 4500   7000 8000\n",
    "# gen_path = '/home/chengxin/chengxin/vagen/log/wan_sd_ttva_55_bi_new/logging/1500'     \n",
    "\n",
    "target_path = '/home/chengxin/chengxin/Dataset_Sound/VGGSound/generated_audios/veo3/avsync'\n",
    "\n",
    "\n",
    "for file in os.listdir(target_path):\n",
    "    try:\n",
    "\n",
    "        input_file = f'{gen_path}/{file}'\n",
    "        output_file = f'{target_path}/{file}'\n",
    "        duration = 5.4  # 截取时长（秒）\n",
    "\n",
    "        waveform, sample_rate = torchaudio.load(input_file)\n",
    "        num_samples = int(duration * sample_rate)\n",
    "        waveform = waveform[:, :num_samples]\n",
    "        # waveform[:, num_samples:] = 0\n",
    "\n",
    "        torchaudio.save(output_file, waveform, sample_rate)\n",
    "        # os.remove(input_file)\n",
    "        i += 1\n",
    "\n",
    "    except:\n",
    "        pass\n",
    "        # print(file)\n",
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc14bdbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.va_processing import add_audio_to_video\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "input_dir = '/home/chengxin/chengxin/vagen/log/wan_sd_ttva_55_bi_new/logging/0'\n",
    "output_dir = '/home/chengxin/chengxin/Dataset_Sound/VGGSound/generated_audios/veo3/avsync_video'\n",
    "for path in tqdm(glob(f'{input_dir}/*.mp4')):\n",
    "    v_path =  f\"{input_dir}/{path.split('/')[-1][:-4]}.mp4\"\n",
    "    a_path =  f\"{input_dir}/{path.split('/')[-1][:-4]}.wav\"\n",
    "    o_path = f\"{output_dir}/{path.split('/')[-1][:-4]}.mp4\"\n",
    "    if os.path.exists(a_path):\n",
    "        add_audio_to_video(video_path = v_path, audio_path = a_path, output_path = o_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a842661a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded fusion checkpoint from /home/chengxin/chengxin/Ovi/ckpts/Ovi/model.safetensors\n",
      "trainable params: 739,246,080 || all params: 12,399,999,188 || trainable%: 5.9617\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from utils.fm_solvers import FlowUniPCMultistepScheduler\n",
    "from utils.va_processing import snap_hw_to_multiple_of_32, save_video\n",
    "from utils.model_loading import (\n",
    "    init_fusion_score_model_ovi, \n",
    "    init_text_model, \n",
    "    init_mmaudio_vae, \n",
    "    init_wan_vae_2_2, \n",
    "    load_fusion_checkpoint\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "\n",
    "device = 5\n",
    "target_dtype = torch.bfloat16\n",
    "ckpt_dir = \"/home/chengxin/chengxin/Ovi/ckpts\"  # 请修改为你的检查点路径\n",
    "\n",
    "model, video_config, audio_config = init_fusion_score_model_ovi(rank=device, meta_init=True)\n",
    "if ckpt_dir is not None:\n",
    "    checkpoint_path = os.path.join(ckpt_dir, \"Ovi\", \"model.safetensors\")\n",
    "    load_fusion_checkpoint(model, checkpoint_path=checkpoint_path, from_meta=True)\n",
    "model = model.requires_grad_(False).eval().to(dtype=target_dtype).to(device=device)\n",
    "model.set_rope_params()\n",
    "model.train()\n",
    "model.gradient_checkpointing = True\n",
    "model.requires_grad_(False)\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    # if \"ffn\" in name:\n",
    "        print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "31e1634c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chengxin/anaconda3/envs/vagen/lib/python3.10/site-packages/peft/mapping_func.py:73: UserWarning: You are trying to modify a model with PEFT for a second time. If you want to reload the model with a different config, make sure to call `.unload()` before.\n",
      "  warnings.warn(\n",
      "/home/chengxin/anaconda3/envs/vagen/lib/python3.10/site-packages/peft/tuners/tuners_utils.py:196: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 739,246,080 || all params: 13,139,245,268 || trainable%: 5.6262\n"
     ]
    }
   ],
   "source": [
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=128,\n",
    "    lora_alpha=64,\n",
    "    target_modules=['k_fusion', 'v_fusion', 'ffn.0', 'ffn.2',\n",
    "                    'self_attn.q', 'self_attn.k', 'self_attn.v', 'self_attn.o', \n",
    "                    'cross_attn.q', 'cross_attn.k', 'cross_attn.v', 'cross_attn.o'],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",)\n",
    "model.gradient_checkpointing = True\n",
    "model = get_peft_model(model, lora_config, adapter_name=\"ref\")\n",
    "model = get_peft_model(model, lora_config, adapter_name=\"learner\")\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "model.set_adapter('ref')\n",
    "model.save_pretrained(f\"log\")\n",
    "model.set_adapter('learner')\n",
    "model.save_pretrained(f\"log\")\n",
    "# lora_config = LoraConfig(\n",
    "#     r=128,\n",
    "#     lora_alpha=64,\n",
    "#     target_modules=['k_fusion'],\n",
    "#     lora_dropout=0.1,\n",
    "#     bias=\"none\",)\n",
    "# model = get_peft_model(model, lora_config, adapter_name=\"learner\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a94bc24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4717d555",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "tensor(393216)\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# import safetensors\n",
    "# import safetensors.torch\n",
    "\n",
    "# # 读取 safetensors 文件\n",
    "# learner1 = safetensors.torch.load_file(\"/home/chengxin/chengxin/vagen/log/learner/adapter_model.safetensors\")\n",
    "# ref1 = safetensors.torch.load_file(\"/home/chengxin/chengxin/vagen/log/ref/adapter_model.safetensors\")\n",
    "# learner2 = safetensors.torch.load_file(\"/home/chengxin/chengxin/vagen/log/ovi_fusion_debug/checkpoints/checkpoint_0/learner/adapter_model.safetensors\")\n",
    "# ref2 = safetensors.torch.load_file(\"/home/chengxin/chengxin/vagen/log/ovi_fusion_debug/checkpoints/checkpoint_0/ref/adapter_model.safetensors\")\n",
    "\n",
    "# print((learner1['base_model.model.base_model.model.audio_model.blocks.0.cross_attn.k.lora_A.weight'] != ref1['base_model.model.base_model.model.audio_model.blocks.0.cross_attn.k.lora_A.weight']).sum())\n",
    "# print((learner1['base_model.model.base_model.model.audio_model.blocks.0.cross_attn.k.lora_A.weight'] != ref2['base_model.model.base_model.model.audio_model.blocks.0.cross_attn.k.lora_A.weight']).sum())\n",
    "# print((ref1['base_model.model.base_model.model.audio_model.blocks.0.cross_attn.k.lora_A.weight'] != ref2['base_model.model.base_model.model.audio_model.blocks.0.cross_attn.k.lora_A.weight']).sum())\n",
    "# print((learner1['base_model.model.base_model.model.audio_model.blocks.0.cross_attn.k.lora_A.weight'] != learner2['base_model.model.base_model.model.audio_model.blocks.0.cross_attn.k.lora_A.weight']).sum())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vagen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
