tag: "wan_sd_train"     

# For hf setting
push_to_hub   : false
hub_token     : null
hub_model_id  : null
# For logging setting
tracker_name  : null
output_dir  : ./log/wan_sd_dpo
report_to   : wandb   
ckpt_subdir : checkpoints
logging_subdir: logging
wandb_init_args:
  mode      : "online" # online / offline  
  project   : "WanSD"
  name      : "wan_sd_dpo"
  tags      : null
  notes     : ""



# # # Model setting 
# # # From hf models and modification
# # audio_pretrained_model_name_or_path: ./assets/stable-audio-open-1.0 
# # audio_transformer_safetensors_path:  ./assets/sd_tta_ft_16/model.safetensors
# # video_pretrained_model_name_or_path: ./assets/Wan2.1-T2V-1.3B-Diffusers  
# # video_transformer_safetensors_path:  null
# # bridge_config:
# #   dual_dit_configs:
# #       num_dual_dit_blocks: 10
# #       num_input_channels_1: 1536
# #       num_input_channels_2: 1536
# #       num_qk_channels: 1536
# #       num_v_channels: 1536
# #       num_heads: 12
# #       t_emb_dim_1: 1536
# #       t_emb_dim_2: 1536
# #       video_bridge_points: [5, 7, 9, 11, 13, 15, 17, 19, 21, 23]    # 30 blocks in total
# #       audio_bridge_points: [3, 5, 7, 9,  11, 13, 15, 17, 19, 21]    # 24 blocks in total
# #       fusion_type: bicross
# # bridge_safetensors_path: null


# VA Brideg trainable setting
# Model setting 
# From hf models and modification
audio_pretrained_model_name_or_path: ./assets/stable-audio-open-1.0 
audio_transformer_safetensors_path:  ./assets/sd_tta_ft_16/model.safetensors
video_pretrained_model_name_or_path: ./assets/Wan2.1-T2V-1.3B-Diffusers  
video_transformer_safetensors_path:  null
bridge_config:
  dual_dit_configs:
      num_dual_dit_blocks: 10
      num_input_channels_1: 1536
      num_input_channels_2: 1536
      num_qk_channels: 1536
      num_v_channels: 1536
      num_heads: 12
      t_emb_dim_1: 1536
      t_emb_dim_2: 1536
      video_bridge_points: [5, 7, 9, 11, 13, 15, 17, 19, 21, 23]    # 30 blocks in total
      audio_bridge_points: [3, 5, 7, 9,  11, 13, 15, 17, 19, 21]    # 24 blocks in total
      fusion_type: bicross
bridge_safetensors_path: null
full_safetensors_path: /home/chengxin/chengxin/vagen/assets/wan_sd_ttva_55/checkpoint_10/model.safetensors

# VA Brideg trainable setting
lora_config:
  lora_load_path : null
  use_lora: true
  rank: 256  
  lora_alpha: 64
  lora_modules: ['q_proj_1', 'k_proj_1', 'v_proj_1', 'o_proj_1',
                 'q_proj_2', 'k_proj_2', 'v_proj_2', 'o_proj_2', 
                 'mlp_1.fc1', 'mlp_1.fc2', 'mlp_2.fc1', 'mlp_2.fc2',
                 'adaln_modulation_1.1', 'adaln_modulation_2.1']
block_config:
  train_va: false   
  audio_trainable_blocks: [19, 20, 21, 22, 23]  # 24 blocks in total
  video_trainable_blocks: [25, 26, 27, 28, 29]  # 30 blocks in total
optimize_params : null # ["dual_dit_blocks",
                      #  "video_transformer.blocks.5.attn", "video_transformer.blocks.6.attn", "video_transformer.blocks.7.attn", "video_transformer.blocks.8.attn", "video_transformer.blocks.9.attn",
                      #  "video_transformer.blocks.10.attn", "video_transformer.blocks.11.attn", "video_transformer.blocks.12.attn", "video_transformer.blocks.13.attn", "video_transformer.blocks.14.attn",
                      #  "video_transformer.blocks.15.attn", "video_transformer.blocks.16.attn", "video_transformer.blocks.17.attn", "video_transformer.blocks.18.attn", "video_transformer.blocks.19.attn",
                      #  "video_transformer.blocks.20.attn", "video_transformer.blocks.21.attn", "video_transformer.blocks.22.attn", "video_transformer.blocks.23.attn", "video_transformer.blocks.24.attn",
                      #  "audio_transformer.transformer_blocks.3.attn", "audio_transformer.transformer_blocks.4.attn", "audio_transformer.transformer_blocks.5.attn", "audio_transformer.transformer_blocks.6.attn",
                      #  "audio_transformer.transformer_blocks.7.attn", "audio_transformer.transformer_blocks.8.attn", "audio_transformer.transformer_blocks.9.attn", "audio_transformer.transformer_blocks.10.attn",
                      #  "audio_transformer.transformer_blocks.11.attn", "audio_transformer.transformer_blocks.12.attn", "audio_transformer.transformer_blocks.13.attn", "audio_transformer.transformer_blocks.14.attn",
                      #  "audio_transformer.transformer_blocks.15.attn", "audio_transformer.transformer_blocks.16.attn", "audio_transformer.transformer_blocks.17.attn", "audio_transformer.transformer_blocks.18.attn",
                      #  "audio_transformer.transformer_blocks.19.attn", "audio_transformer.transformer_blocks.20.attn", "audio_transformer.transformer_blocks.21.attn", "audio_transformer.transformer_blocks.22.attn"] 



# Training parameters
resume_from_checkpoint  : null # '/home/chengxin/chengxin/vagen/log/wan_sd_ttva_55/checkpoints/checkpoint_8'
# Seed and precision
seed            : 42
mixed_precision : bf16   # Options: ["null", "fp16", "bf16"]
allow_tf32      : false 
# matmul_precision  : medium 
# Steps and saving
num_train_epochs        : 10        
max_train_steps         : null
checkpointing_steps     : 100 #######################  
checkpoints_total_limit : 40 
# For batch and LR setting
num_workers             : 4
train_batch_size_local  : 1
gradient_accumulation_steps : 1
learning_rate   : 5e-5     #######################
dpo_beta : 0.1 
scale_lr        : false     
lr_scheduler    : "cosine_with_restarts"  # Options: ["linear", "cosine", "cosine_with_restarts", "polynomial", "constant", "constant_with_warmup"]
lr_warmup_steps : 1000
lr_num_cycles   : 1
lr_power        : 1.0
# For optimizing training
enable_slicing  : false 
enable_tiling   : false 
gradient_checkpointing  : true
find_unused_parameters  : true



# Optimizer
optimizer       : "adam"  # Options: ["adam", "adamw", "prodigy"]
use_8bit_adam   : false    
adam_beta1      : 0.9
adam_beta2      : 0.95
prodigy_beta3     : null   
prodigy_decouple  : false  
adam_weight_decay : 0.0001
adam_epsilon      : 0.00000001
max_grad_norm     : 1.0    # [1.0, 0.5] 
prodigy_use_bias_correction : false  
prodigy_safeguard_warmup    : false  



# Dataset information
# Video processing


hy_dataloader:
  video_index_file: "/home/chengxin/chengxin/vagen/data/ttva/avsync_train_nocap.json"  
  # dataset setting
  load_mode: "video_audio_dpo" # video_audio_dpo  video_audio
  video_size: [832, 480]
  fps : 16                  
  max_frames: 81            
  target_sr: 44100            
  target_channels: 2          
  target_duration: 5.1      
  uncond_prob: 0.0          #######################
  # # # # negetive_prompt: ""       
  # DataLoader parameters
  shuffle: true               
  batch_size : ${train_batch_size_local}  
  num_workers: ${num_workers}
  prefetch_factor: 2
  





# Validation
validation: 
  scheduler:
    flow_shift: 5.0          ##
    num_train_timesteps: 1000
    
  eval_steps: ${checkpointing_steps}
  prompt_index_file: /home/chengxin/chengxin/vagen/data/ttva/avsync_test_nocap.json          
  seed: ${seed}
  num_va_per_prompt: 1
  num_inference_steps: 40   

                      ## 
  v_negative_prompt : Bright tones, overexposed, static, blurred details, subtitles, style, works, paintings, images, static, overall gray, worst quality, low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn faces, deformed, disfigured, misshapen limbs, fused fingers, still picture, messy background, three legs, many people in the background, walking backwards
  v_num_frames: ${hy_dataloader.max_frames}
  v_height: 480
  v_width: 832
  v_guidance_scale: 6  ##    

  a_negative_prompt : ""
  a_duration: ${hy_dataloader.target_duration}  
  a_guidance_scale: 6  ##

  data_info:
    video_info:
      fps: ${hy_dataloader.fps}
    audio_info:
      sr: ${hy_dataloader.target_sr}
      channels: ${hy_dataloader.target_channels}