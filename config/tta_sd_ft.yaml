tag: "t2a_wan_tuning_hy1kw_wonorm"     #  wan_on_audio-beta 

# For hf setting
push_to_hub   : false
hub_token     : null
hub_model_id  : null
# For logging setting
tracker_name  : null
output_dir  : ./log/sd_tta_ft_16
report_to   : wandb   # Options: ["wandb", "null"]
ckpt_subdir : checkpoints
logging_subdir: logging
wandb_init_args:
  mode      : "online" # online / offline
  project   : "Veo3"
  name      : "sd_tta_ft_16"
  tags      : null
  notes     : ""



# Model setting 
# From hf models and modification
pretrained_model_name_or_path: ./assets/stable-audio-open-1.0  # Required: Path to pretrained model or model identifier  Wan2.1-T2V-1.3B-Diffusers
# pretrained_audio_model_name_or_path  : /data-04/xihua/data/ckpt/audioldm2/huggingface
# # Lora setting
# use_lora: false
# rank: 64  
# lora_alpha: 64
# lora_modules: ["to_q", "to_k", "to_v", "ffn.net.0.proj", "ffn.net.2", "text_embedder.linear"]
# grad_modules: ['patch_embedding', 'proj_out', 'scale_shift_table', 'text_embedder']




# Training parameters
resume_from_checkpoint  : /home/chengxin/chengxin/vagen/log/sd_tta_pt_16/checkpoints/checkpoint_1 # /home/chengxin/chengxin/vagen/log/sd_tta_ft_16/checkpoints/checkpoint_13  # /home/chengxin/chengxin/vagen/log/sd_tta_pt_16/checkpoints/checkpoint_1  
# Seed and precision
seed            : 42
mixed_precision : null  #### # Options: ["null", "fp16", "bf16"]
allow_tf32      : false 
matmul_precision  : medium
# Steps and saving
num_train_epochs        : 200        ####
max_train_steps         : null
checkpointing_steps     : 500       ####
checkpoints_total_limit : 20 
# For batch and LR setting
num_workers             : 24
train_batch_size_local  : 24
gradient_accumulation_steps : 1
learning_rate   : 1e-6     ####
scale_lr        : false     ####
lr_scheduler    : "cosine_with_restarts"  # Options: ["linear", "cosine", "cosine_with_restarts", "polynomial", "constant", "constant_with_warmup"]
lr_warmup_steps : 1000
lr_num_cycles   : 1
lr_power        : 1.0
# For optimizing training
enable_slicing  : false 
enable_tiling   : false 
gradient_checkpointing  : false
find_unused_parameters  : true  



# Optimizer
optimizer       : "adam"  # Options: ["adam", "adamw", "prodigy"]
use_8bit_adam   : false    #
adam_beta1      : 0.9
adam_beta2      : 0.95
prodigy_beta3     : null   #
prodigy_decouple  : false  #
adam_weight_decay : 0.0001
adam_epsilon      : 0.00000001
max_grad_norm     : 1.0    # [1.0, 0.5] 
prodigy_use_bias_correction : false  #
prodigy_safeguard_warmup    : false  #



# Dataset information
# Video processing



hy_dataloader:
  video_index_file: "/home/chengxin/chengxin/vagen/data/tta/train_vggss_avsync.json"   ####
  # dataset setting
  load_mode: "audio_waveform"
  target_sr: 44100            ####
  target_channels: 2          ####
  target_duration: 10.0       ####
  uncond_prob: 0.1
  negetive_prompt: ""
  # DataLoader parameters
  shuffle: true               
  batch_size: ${train_batch_size_local}
  num_workers: ${num_workers}
  prefetch_factor: 2




# Validation
validation: 
  flow_shift: 1
  eval_steps: ${checkpointing_steps}
  seed: ${seed}
  save_dir : /home/chengxin/chengxin/vagen/log
  audio_end_in_s   : ${hy_dataloader.target_duration}
  sr: ${hy_dataloader.target_sr}
  channels: ${hy_dataloader.target_channels}
  guidance_scale: 7    ####
  num_waveforms_per_prompt: 1
  num_inference_steps: 100 
  eta : 0.0
  prompt_separator: ":::"
  negetive_prompt : ${hy_dataloader.negetive_prompt}
  negetive_prompt_embed : NULL
  prompt_index_file: /home/chengxin/chengxin/vagen/data/tta/test_vggss_avsync.json
